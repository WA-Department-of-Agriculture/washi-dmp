[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "1 Overview\nThe Washington Soil Health Initiative (WaSHI) is a partnership between the Washington State Department of Agriculture (WSDA), Washington State University (WSU), and the State Conservation Commission. WaSHI establishes a coordinated approach to healthy soil in Washington.\nTo date, nearly 1,000 soil samples and management surveys across 50 different cropping systems have been collected as a part of the state of the soils assessment (SoSA). WSDA and WSU lead this project with support from staff, students, conservation districts, and agricultural professionals throughout Washington."
  },
  {
    "objectID": "index.html#chapter-outline",
    "href": "index.html#chapter-outline",
    "title": "Data Management Plan",
    "section": "1.1 Chapter outline",
    "text": "1.1 Chapter outline\nThis Data Management Plan (DMP) is a living document will be continually reviewed and improved based on lessons learned, new information, and collaborator feedback.\n\n\n\n\n\n\nLinks to shared drive folders and files\n\n\n\nThis DMP is best viewed in Google Chrome since the browser can open local file links using the Enable local file links extension that should automatically be enabled by our organization.\nWhen viewing this DMP in Microsoft Edge, hyperlinks to files and folders on our shared drive are not accessible in the browser. Nothing will happen when clicking on the links. To open the file or folder, right-click on the hyperlink &gt; copy the path &gt; paste it into the search bar of the file explorer &gt; press Enter or click the arrow."
  },
  {
    "objectID": "index.html#roles-and-responsibilities",
    "href": "index.html#roles-and-responsibilities",
    "title": "Data Management Plan",
    "section": "1.2 Roles and responsibilities",
    "text": "1.2 Roles and responsibilities\nAll WaSHI personnel who will be interacting with SoSA data must familiarize themselves with the contents of this document. Following chapters with technical details will be referenced when relevant. If all collaborators are not consistently implementing this DMP, then the benefits of effective data management are lost.\nThe WSDA Data Scientist, supported by the project Principal Investigators (PIs), is responsible for providing guidance to WaSHI staff working with SoSA data and ensuring the implementation of the DMP. The Data Scientist is also responsible for reviewing and updating this document annually, and as needed. Upon updates, the Data Scientist will distribute this document to WaSHI staff and commit the source code to the GitHub repository.\n\nCurrent roles as of November 2023\n\n\nRole\nAffiliation\nName\n\n\n\n\nData Scientist\nWSDA\nJadey Ryan\n\n\nCo-PI\nWSDA\nDani Gelardi\n\n\nCo-PI\nWSU\nDeirdre Griffin LaHue\n\n\nData Stewards\nWaSHI personnel"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Management Plan",
    "section": "1.3 Acknowledgements",
    "text": "1.3 Acknowledgements\nThis DMP was adapted from the R.J. Cook Agronomy Farm Long-term Agroecological Research Site DMP (Carlson 2021), U.S. Fish and Wildlife Service data management life cycle (U.S. Fish & Wildlife Service 2023), Harvard Medical School Longwood Research Data Management DMP guidelines (Harvard Medical School 2023), and the Data Management in Large-Scale Education Research book (Lewis 2023).\n\n\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the R.J. Cook Agronomy Farm Long-Term Agroecological Research Site.”\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle."
  },
  {
    "objectID": "data-management.html#data-life-cycle",
    "href": "data-management.html#data-life-cycle",
    "title": "2  What is data management?",
    "section": "2.1 Data life cycle",
    "text": "2.1 Data life cycle\n\n\nThe U.S. Fish and Wildlife Service developed a great graphic to explain the elements of the data life cycle and emphasize the importance of data quality at every step (U.S. Fish & Wildlife Service 2023). Each step within the data life cycle requires careful intention to ensure transparency, quality, and integrity.\n\n\n\n\n\n\n\n\n\n\nOur adaptation of this data life cycle is outlined below.\n\n\n\n\n\n\n\n\nPlan\nEach sampling year presents an opportunity to consider what worked and what could be improved from the previous year. Planning involves making decisions about data acquisition, management, and quality control. For example, each year we provide a spreadsheet template with our requested column headers to Soiltest lab to ensure the measurements are reported with correct units and in the format we use. Special projects that deviate from our standard operating procedures require additional planning.\n\n\n\nAcquire\nWe acquire data by collecting and analyzing new samples, deriving new insights from existing data, or accepting datasets from collaborators. |\n\n\n\nMaintain\nMaintenance involves processing data for aggregation, analyses, and reporting. We create metadata that facilitates interpretation of the data and ensure the data are in a non-proprietary format that is accessible to our collaborators and future selves.\n\n\n\nAccess\nAccess refers to data storage, publication, and security. Raw and processed data with accompanying metadata should be stored, backed up, and available for information sharing with our partners. With PI approval, anonymized and aggregated data that does not compromise growers’ personally identifiable information (PII) should be made publicly available in a data repository or data product/decision-support tool.\n\n\n\nEvaluate\nWe evaluate data while processing and analyzing it to maximize accuracy and productivity, while minimizing costs associated with errors or tedious data cleaning labor. Evaluation workflows should be efficient, well-documented, and reproducible. Our evaluated data help us better understand how factors and management decisions impact soil health.\n\n\n\nArchive\nProperly archiving our results supports the long-term storage and usefulness of our data. While similar to the Access element of the life cycle, archiving focuses on preserving data for long-term/historical retention that aren’t needed for immediate access. For example, we archive each year’s raw data for long-term storage and set those files to Read-Only.\n\n\n\nQuality Assurance / Quality Control (QA/QC)\nData quality management prevents data defects that hinder our ability to apply data towards our science-based conservation efforts. Defects include incorrectly entered data, invalid data, and missing or lost data. QA/QC processes should be incorporated in every element of the data life cycle.\n\n\n\nThe following chapters describe our internal processes and standards to follow throughout each step in the data life cycle.\n\n\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "formats-standards.html#data-formats",
    "href": "formats-standards.html#data-formats",
    "title": "3  Formats & standards",
    "section": "3.1 Data formats",
    "text": "3.1 Data formats\nData generated from or integrated into WaSHI can be non-digital or digital.\n\nNon-digital data\nNon-digital data, such as field forms, management surveys, and chain of custody forms, are manually recorded on paper forms. Paper forms must be transcribed or converted to digital file formats and then stored in the WaSHI filing cabinet in the Natural Resources Building in Olympia.\n\n\nDigital data\nDigital data include tabular, spatial, and binary data, such as lab results, sample locations, and field photos. Non-conventional data also include code, algorithms, tools, and workflows.\nTabular data include comma separated values (csv), tab separated values (tsv), Microsoft Excel open XML spreadsheet (xlsx), and portable document format (pdf).\nSpatial data include file geodatabases (gdb), vector shapefiles (zipped folder containing multiple file extensions), keyhole markup language (kml or kmz). Tabular data may also contain spatial data as longitude and latitude.\nBinary data include photos (jpeg, png, gif, tiff), videos (mp4), code (R, py, js), and object-oriented data files (RDS, Rdata, parquet, arrow).\nProprietary data formats include Microsoft Excel, Word, and Powerpoint files (xlsx, docx, pptx). RDS and Rdata files are an example of an application-specific data format that can only be opened using the R programming language or RStudio IDE. These types of files should be saved in conjunction with a copy of the data in a non-proprietary and open-standard format, such as csv, to maintain accessibility for those who do not have Microsoft Office or do not use R.\nWritten documents and presentations are in formats including Microsoft Word and Powerpoint (docx and pptx), hypertext markup language (HTML), and pdf.\nNotebooks combine text with executable code to generate written documents and presentations in docx, pptx, HTML, or pdf formats. These notebooks are stored in formats depending on the programming language: a few examples include R markdown (rmd), Quarto (qmd), and Jupyter notebook (ipynb).\nThe list below is not exhaustive and will continue to grow as additional useful data sources are discovered.\n\n\n\n\nType\nSource\nFormats\n\n\n\n\nLab results\nProvided by the lab analyzing the soil sample, principal investigator of a study, or grower\ncsv, xlsx, pdf, xml, json, RDS, RData\n\n\nManagement surveys\nCollected through interviews with grower\ncsv, xlsx, RDS, RData, paper form (to be digitized)\n\n\nField forms\nCompleted in the field during/immediately after sampling\npdf, paper form (to be digitized), csv, xlsx\n\n\nSample locations\nIdentified prior to sampling and may be edited during sampling using ArcGIS Online, Collector, Field Maps or Google Maps\nArcGIS feature layer, shp, kmz, csv, xlsx\n\n\nChain of custody forms\nCompleted prior to shipping or dropping off samples\npdf, paper form (to be digitized)\n\n\nClimate data\nOSU PRISM, NOAA, Esri Living Atlas\ncsv, shp, netCDF, tiff, gdb\n\n\nSoil data\nNRCS Web Soil Survey, NRCS WA gSSURGO\ngdb, accdb\n\n\nStrata classification\nProvided by Soil Health Institute in 2021 as a lyr file\nlyr\n\n\nImages\nLogos, icons, photos taken in the field\njpeg, png, gif, tiff, svg\n\n\nVideos\nRecordings of meetings, training videos\nmp4\n\n\nDocuments\nReports, manuscripts, SOP, QAPP, factsheets, brochures\ndocx, txt, html, pdf\n\n\nPresentations\nPowerpoints, slide decks\npptx, html, pdf\n\n\nCode\nScripts for wrangling, processing, analyzing data; markdown for producing documents and presentations; style sheets for html outputs\nR, py, ipynb, js, yml, rmd, qmd, css, scss"
  },
  {
    "objectID": "formats-standards.html#sec-data-standards",
    "href": "formats-standards.html#sec-data-standards",
    "title": "3  Formats & standards",
    "section": "3.2 Data standards",
    "text": "3.2 Data standards\nDate will be expressed as YYYY-MM-DD according to the ISO 8601 standard.\nDate with time will be expressed as YYYY-MM-DDTHH:MM:SSZ.\n\nT separates date from time. The Z indicates the date-time is using the Universal Time Coordinated (UTC) with no offset.\nPacific Standard Time (PST) has a UTC-8:00 offset and Pacific Daylight Time (PDT) has a UTC-7:00 offset and would be expressed as YYYY-MM-DDTHH:MM:SS-8:00. The Z has been replaced with the offset.\nExample: 2023-11-28T14:55:56-08:00.\n\n\n\n\n\"ISO 8601\" from Randall Munroe's xkcd\n\n\nGeospatial data will be accompanied by metadata that abides by the ISO 19115 standard by following Esri’s documentation when working in ArcGIS Pro. Metadata contains information about the identification, the extent, the quality, the spatial and temporal schema, spatial reference, and distribution of digital geographic data.\nCode will follow the style guide in Chapter 10."
  },
  {
    "objectID": "naming.html#footnotes",
    "href": "naming.html#footnotes",
    "title": "4  Naming conventions",
    "section": "",
    "text": "Code files include anything that gets pushed to GitHub where it becomes a URL. Google recommends kebab-case because hyphens are better than underscores for URLs.↩︎\nSee Chapter 10 for the code style guide.↩︎"
  },
  {
    "objectID": "organization.html",
    "href": "organization.html",
    "title": "5  Organization",
    "section": "",
    "text": "We organize our folders into a hierarchical structure to clearly delineate segments of our projects, improve searchability, and ensure reproducibility across years.\n\nFolder Structure\nWe strive for a balance between a deep and shallow structure. If too shallow, there are too many files in one folder and they are hard to sort through. If too deep, we have to click too many times to get to a file and specific files can be difficult to find.\nY:\\NRAS\\Soil_Health_Initiative is the parent folder for all WaSHI content.\nWithin the Sampling sub-folder, we use a combination of date- (each year has its own sub-folder) and categorical- based (dataset and documentation that span across years) folder structures.\nY:\\NRAS\\Soil_Health_Initiative\\Sampling\n├── _completeDataset\n├── 2019_SCBG\n├── 2021_Sampling\n├── 2022_PartnershipsInSoilHealth\n├── 2023_Sampling\n├── 2024_Sampling\n├── Maps\n├── Projects\n├── QAPP\n├── SOPs\n├── TrainingVideos\n├── ArchivedSampleInventory.xlsx\n├── EquipmentInventory.xlsx\n└── SOSImpacts.xlsx\nWithin the each year sub-folder, we have sub-sub-folders for planning, forms, data, and processes. This structure helps maintain a reproducible workflow year after year. See the 2023_Sampling for an example:\nY:\\NRAS\\Soil_Health_Initiative\\Sampling\\2023_Sampling\n├── Applications\n├── CoCs\n├── Equipment\n├── FieldForms\n├── Forms\n├── GIS\n├── LabData\n├── Labels\n├── ManagementSurveys\n├── PublicDocs\n├── Purchases\n├── Reports\n├── SampleIDAssignments\n├── Scripts\n├── 2023_DataTracking.xlsx\n└── PostSeasonWrapUp_2023.docx\nAs mentioned in Section 6.1, it’s good practice to maintain the raw data. We use additional sub-folders for the LabData folder. Everything in Raw has been set as Read-Only.\nY:\\NRAS\\Soil_Health_Initiative\\Sampling\\2023_Sampling\\LabData\n├── 2023_DataTemplateSoiltest.xlsx\n├── Clean\n├── QC\n├── Raw\n└── Working\nSoil_Health_Initative &gt; Sampling &gt; 2023_Sampling &gt; LabData &gt; Clean already has five levels of nesting. We wouldn’t want to add any many more levels or the hierarchy becomes difficult to manage.\n\n\nArchive folders\nWhen too many drafts or versions begin to clutter a sub-folder, create a new folder with the naming convention of Archive_FolderDescription. Place the old drafts there. Leave the most current, accurate file in the main folder.\nFor example, the most recent sample labels for each conservation district are listed in the top level CompletedLabels folder, and previous working drafts were moved to the Archive_Labels folder.\nY:\\NRAS\\Soil_Health_Initiative\\Sampling\\2023_Sampling\\Labels\\CompletedLabels\n├── Archive_Labels\n│   ├── CowlitzCounty_Labels.docx\n│   ├── FerryCD_Labels.docx\n│   ├── LewisCD_Labels.docx\n│   └── StevensCD_Labels.docx\n├── CowlitzCounty_Labels_V2.docx\n├── FerryCD_Labels_V2.docx\n├── KittitasCD_Labels.docx\n├── LewisCD_Labels_V2.docx\n├── ...\n└── WallaWallaCD_Labels.docx"
  },
  {
    "objectID": "storage.html#sec-raw-data",
    "href": "storage.html#sec-raw-data",
    "title": "6  Storage",
    "section": "6.1 Read-only raw data",
    "text": "6.1 Read-only raw data\nOn our shared drives, raw data such as lab results from Soiltest or exports from ArcGIS Online, should be immediately set to Read-only. Right click the file &gt; click on Properties &gt; check the Read-only attribute box.\n\nThe file should then be copied over to a Working folder for any processing or analyses. The final dataset should be saved in a separate descriptively titled Clean folder. Keeping a readme.txt` to document your processing and analysis steps is good practice, as discussed in Section 7.2.1."
  },
  {
    "objectID": "storage.html#version-with-github",
    "href": "storage.html#version-with-github",
    "title": "6  Storage",
    "section": "6.2 Version with GitHub",
    "text": "6.2 Version with GitHub\nCode should be versioned and archived using GitHub\nInclude helpful resources for how-to use"
  },
  {
    "objectID": "storage.html#staff-turnover",
    "href": "storage.html#staff-turnover",
    "title": "6  Storage",
    "section": "6.3 Staff turnover",
    "text": "6.3 Staff turnover\n\nWSDA GitHub Organization\nZenodo\nDatabase credentials"
  },
  {
    "objectID": "documentation.html#project-level",
    "href": "documentation.html#project-level",
    "title": "7  Documentation",
    "section": "7.1 Project-level",
    "text": "7.1 Project-level\nProject-level documentation includes all descriptive information about the SoSA dataset, as well as planning decisions and process documentation. Documentation includes quality assurance project plans (QAPP), standard operating procedures (SOP), and other high-level documents (i.e., request for proposals, applications, meeting agendas/notes, etc.).\n\nQuality assurance project plan (QAPP)\nThe QAPP is the highest level of project documentation and covers everything from the project description, personnel roles and responsibilities, project timelines, data and measurement quality objectives, study design, and overviews of field, laboratory, and quality control.\nOurs can be found in Y:\\NRAS\\Soil_Health_Initiative\\Sampling\\QAPP, though it needs to be updated.\n\n\nStandard operating procedures (SOP)\nSOPs provide detailed instructions for field, lab, or data processing procedures and decision making processes.\nAll sampling related SOPs can be found in Y:\\NRAS\\Soil_Health_Initiative\\Sampling\\SOPs.\n\nSoSA sampling\nThe purpose of this SOP is to specify the procedures for a typical site visit in which soil sampling is conducted to measure physical, chemical, and biological soil health indicators. Procedures include equipment preparation prior to sampling, best practices for filling out field forms, the selection of sampling locations, sampling protocols, sample handling and storage, and submitting samples to the lab. This SOP serves to ensure data quality by creating audit trails and enabling verification that data are present, complete, and accurate. Additionally, this SOP will be used to maintain consistent sample collection procedures throughout the state for WSDA employees and partners.\n\n\nQuality control / quality assurance (QA/QC)\nThis SOP outlines the process for screening sample metadata and lab results for completeness, consistency, and quality. Procedures involve subject matter expertise, investigation, communication with sampling teams and labs, algorithmic quality control, and tagging sample results with quality codes (as shown in the below table). Data are then integrated into the statewide database according to a SOP not yet authored.\n\n\nView the quality codes\n\n\n\n\n\n\n\n\n\n\n\nCode\nTag\nDescription\nInclusion in analyses\n\n\n\n\n0\nExcellent\nMet lab’s and WSDA’s QC criteria\nYes\n\n\n100\nEstimate\nInterpolated missing value\nYes\n\n\n110\nDerived\nDerived from an estimated value\nYes\n\n\n120\nSuspect\nZ-score is ≥ |3|\nYes\n\n\n130\nCalculated ND\nCalculated value using at least one ND\nYes\n\n\n140\nNon-detect\nBelow the method detection limit\nNo\n\n\n160\nPoor\nDid not meet lab’s QC criteria\nNo\n\n\n180\nOutlier\nOutlier, designated by soil scientist\nNo\n\n\n200\nUnknown\nExternal dataset\nYes\n\n\n\n\n\n\nSOPs we don’t have (yet)\n\nassigning sample, producer, field IDs\ndata cleaning\ndata integration into database\ndata storage\nexternal data integration"
  },
  {
    "objectID": "documentation.html#dataset-level",
    "href": "documentation.html#dataset-level",
    "title": "7  Documentation",
    "section": "7.2 Dataset-level",
    "text": "7.2 Dataset-level\nDataset-level documentation applies to lab results, sample locations, grower information, and management data. We use readmes and changelogs to document what each dataset contains, how they are related, potential issues to be aware of, and any alterations made to the data.\n\nReadme\nreadme files are plain text documents that contain information about the files in a folder, explanation of versioning, and instructions/metadata for data packages. These files are saved as .txt, not as MS Word documents that take longer to open and can only be opened on computers with Microsoft installed.\n\nDescribe contents of folder\nFor example, the _completeDataset folder contains a readme.txt that describes each of the files’ structure, contents, and other pertinent information, such as the data source (i.e. PRISM mean annual precipitation and temperature are 30-year normals from 1991-2020 at 800 m resolution).\n\n\nView the example\n\n2020-2023_labResults_wide has one sample per row with each measurement as a separate column.\n\n2020-2023_labResults_long has one result per row with columns for sample identification, measurement name, result value, quality code, and lab.\nThis spreadsheet also contains a second tab that describes the quality codes.\n\n2020-2023_sampleLocations has the coordinates, PRISM mean annual precipitation and mean annual temperature (800 m resolution, 30-year normals, 1991-2020), and SHI strata.\n\ndataDictionary lists each variable with its short name (variable name without units), description, indicator type (chemical, physical, biological), unit (if applicable), label, and data type.\nThis spreadsheet also contains a second tab that describes the variables in the sampleLocations spreadsheet.\n\nscbg_producerId_recode lists all SCBG fields with original SCBG assigned producer ID and new WSDA producer ID\n\nallFarmInfo lists all participating farm names, producer names, producer IDs, field names, field IDs, and county\n\nexampleData.csv contains 100 random samples that have been anonymized with fake sampleIDs, county, farm, producer, and field names. WSU SCBG samples are excluded, as are the 0-6in to 6-12in WSDA samples.\n\n\n\nExplain versions\nAnother example is the readme.txt in the 2023_Sampling &gt; LabData &gt; Raw folder, which explains why there are two different versions of the lab results and where to find additional information.\n\n\nView the example\n\n2023-08-21\n2023_WSDASoilHealth_v1.xlsx has errors. See email for details.\n'          'v2.xlsx still has some errors, that are cleaned up in the R scripts.\n\n\n\nProvide instructions\nAn example of a readme.txt that provides instructions on how to use the folder contents can be found in the ArcGIS soil sample points box.com folder that is shared with outside partners.\n\n\nView the example\n\nTemplate for Soil Sample Points ArcGIS\n\n2023-06-01\n\nJadey Ryan | Washington State Department of Agriculture (WSDA)\njryan@agr.wa.gov\n\nPurpose:\nTo provide a template for ESRI ArcGIS data entry and management for soil sampling projects.\n\nFolder contents:\n- README.txt describes the folder contents and provides general instructions.\n- Template_SoilSamplePoints.aprx is an ArcGIS Project File should allow you to open the project in ArcGIS Pro.\n- Template_SoilSamplePoints.gdb is a file geodatabase, which you can open in ArcGIS Pro or ArcGIS Online.\n- crop_domain.csv provides WSDA approved crop types to use in ArcGIS 'Table to Domain' geoprocessing tool. We highly recommend using attribute domains, which are rules to enforce data integrity by limiting the field type and choices of an attribute field.\n- 923_NRAS_SoilHealthSOP_WEB.pdf is WSDA's Standard Operating Procedure for soil sampling. The appendices contain instructions for the GIS workflow of soil sampling.\n- /Screenshots/Feature_Layer_Offline_Editing.png shows the checkboxes required for editing.\n- /Screenshots/Configure_Forms.png shows where to click within a Web Map to open the Field Maps form editor.\n- /Screenshots/Table_to_Domain.png is an example of how to use the 'Table to Domain' geoprocessing tool with the crop_domain.csv.\n- /Screenshots/FieldMapsForm_*.png are examples of the Field Maps form structure, which can be created in ArcGIS Online.\n- WaSHI_SoilSeries_REST_Service is an internet shortcut to the URL for the Washington clipped soil series, originating from NRCS gSSURGO. When compositing multiple samples together from one field, we recommend keeping all sample points within one soil series to reduce variability in the composite samples.\n- The other folders (Index, GpMessages, ImportLog, .backups) are part of the ArcGIS Pro project and can be ignored.\n\nInstructions:\n1. Open Template_SoilSamplePoints.aprx.\n2. Update attribute fields and domains to work with your project.\n3. Update symbology, labels, visibility scale, popups, etc. The symbology of the sample points currently defaults to red when the 'Show/Hide Field Form' attribute is still 'Hide' or 'NULL'. When the sampler collects the sample and changes this attribute to 'Show', the point will turn yellow to indicate the sample has been collected.\n4. Share as Feature Layer and create a Web Map to allow others access to this map.\n5. Open the map in ArcGIS Online and click on 'Forms' in the right toolbar to configure your Field Maps form.\n6. Use the /Screenshots/FieldMapsForm_*.pngs as a guide, but adapt the field form structure to suit your project.\n\nOffline Areas:\n- If you anticipate needing to sample without cellular service or wifi access: 1) Open the hosted feature layer, 2) Click 'Settings' 3) Confirm 'Enable Sync' is turned on.\n- If you need the Soil Series layer, you will need to configure two separate Web Maps: one with and one without. This is because the WaSHI_SoilSeries_REST_Service is not allowed in Web Maps with 'Enable Sync' turned on. \n- Alternatively, you can create your own Soil Series layer and host it as a feature layer in your own organization.\n\nResources:\n- WSDA Soil Health YouTube channel: https://www.youtube.com/playlist?list=PL0pB20prk7Ni1daEYiEEXSWy8CfwO34FC\n- WSDA WaSHI_SoilSeries MapServer: https://fortress.wa.gov/agr/gis/wsdagis/rest/services/NRAS/WaSHI_SoilSeries/MapServer\n- Introduction to attribute domains: https://pro.arcgis.com/en/pro-app/latest/help/data/geodatabases/overview/an-overview-of-attribute-domains.htm\n- Introducing smart forms in ArcGIS Field Maps: https://www.esri.com/arcgis-blog/products/field-maps/field-mobility/introducing-arcgis-smart-forms/\n\n\n\n\nChangelog\nChangelogs are also simple and concise plain text documents saved in a folder alongside data files to document any changes to the dataset.\nAt the bare minimum, the changelog.txt should contain:\n\ndate of modification\ninitials of who made the changes\ndescription of the changes\n\n\n\nView the example\n\nContents of changelog.txt in _completeDataset folder:\n\n2022-12-15 JR standardized texture classes (title case, no extra white space) and converted texture, county, and crop to factor types.\n2023-01-03 JR corrected error in 2022 WSDA bulk density measurements in 2022-11-01_soiltestData_manualCleanup.xlsx.\n2023-03-02 JR recoded SCBG producer IDs to match WSDA format and cleaned up farm/producer names and IDs. See new scbg_producerId_recode.csv for list.\n2023-03-07 JR corrected Okanogan producer and field names (Devany, Townsend).\n2023-03-21 JR corrected cropType \"Fallow, Idle\" to \"Fallow\" and 2021 SCBG \"Pea\" samples to \"Pea, Dry\", updated Crop Group column. Updated results and sample locations spreadsheets.\n2023-07-13 JR added labID to labResults datasets and updated dataDictionary accordingly. \n2023-08-21 JR added 2023 data and updated dataDictionary sample_locations tab.\n2023-08-27 JR corrected SCBG pulse samples crop from \"Pea\" to \"Pea, Dry\".\n2023-08-30 JR added an anonymized dataset (100 samples from 2022-2023).\n2023-09-05 JR added labID to 2023 results.\n2023-09-06 JR added sampling organization column to make impact tracking easier. Fixed merge issue that was causing the loss of some producer IDs. Add \"County\" to relevant CD names.\n2023-11-14 JR switched from .RData to .RDS file type so users can assign a new name when loading the data into R with `data_wide &lt;- readRDS(\"2020-2023_labResults_wide.RDS\")`.\n2023-11-14 JR added sampling dates and depths to all 877 samples. See addSampleDepthsDates for R script. \n2023-12-08 JR corrected CropGroup from Fallow to Cereal Grain for samples with CropType of Fallow, Wheat in 2020-2023_sampleLocations.csv."
  },
  {
    "objectID": "documentation.html#variable-level",
    "href": "documentation.html#variable-level",
    "title": "7  Documentation",
    "section": "7.3 Variable-level",
    "text": "7.3 Variable-level\nVariable-level documentation includes data dictionaries and code books, which are often talked about interchangeably. However, we’ll refer to the data dictionary as a tabular collection of names, definitions, and attributes about the variables in a dataset created (ideally) in the planning phase of the project before data are collected. In contrast, codebooks provide descriptive, variable-level information and univariate summary statistics to allow users to understand the contents of a dataset without opening it. The codebook is created or updated after data are collected, cleaned, and validated.\n\nData dictionary\nIn a data dictionary, each row is a different variable and each column is a different attribute of that variable. With a data dictionary, any user should be able to properly interpret each variable in our data.\nOur dataDictionary.xlsx in the _completeDataset folder contains two tabs (labResults and sampleLocations) that describe the attributes of each variable.\n\n\nView the sampleLocations dictionary\n\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\nunit\ndataType\n\n\n\n\nyear\nYear the sample was collected\n\nNumeric\n\n\ncounty\nCounty of the sampled field\n\nCharacter\n\n\nsampleId\nSample identification code\n\nCharacter\n\n\ncropGroup\nCrop group of the sampled field\n\nCharacter\n\n\ncropType\nCrop type of the sampled field\n\nCharacter\n\n\nstratum\nSoil Health Institute stratum\n\nCharacter\n\n\nMAP_mm.year\nOSU PRISM mean annual precipitation\n30-year normals (1991-2020) at 800 m resolution\nmm/year\nNumeric\n\n\nMAT_c\nOSU PRISM mean annual temperature\n30-year normals (1991-2020) at 800 m resolution\ndegrees C\nNumeric\n\n\nlongitude\nLongitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\nlatitude\nLatitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\n\n\n\n\nCodebook\nCodebooks provide more information (i.e., existing values/ranges and summary statistics) than the data dictionary and can be used to understand a very high-level summary of the processed data. There are many R packages that generate codebooks; however, we have not implemented this type of documentation for our project yet.\nCrystal Lewis gave the lightning talk A Comparison of Packages to Generate Codebooks. I’d like to generate codebooks for our datasets once they live in a database."
  },
  {
    "objectID": "documentation.html#sec-external-data",
    "href": "documentation.html#sec-external-data",
    "title": "7  Documentation",
    "section": "7.4 External data",
    "text": "7.4 External data\nExternal data refers to any data not directly collected by WSDA or trained partners (e.g., WSU or conservation districts) that follow our SOPs. These can include other studies pre-dating WaSHI, special soil health surveys, or publicly available datasets.\nThe Data Scientist and Senior Soil Scientist will decide whether to integrate an external dataset case by case by considering the below questions:\n\nHow does the study design fit into SoSA goals?\nWho collected the soil samples?\nWhat field procedures were used and how were they documented?\nWho analyzed the soil samples? With which methods and QA/QC procedures?\nWhich pieces of metadata and management data accompany the lab results?\n\nFarm, producer and field info1\nSampling date\nSampling depth\nLatitude and longitude\nProduction system (current crop, crop rotation, etc.)\nTillage, livestock grazing, irrigation, soil fertility and amendments, conservation practices, etc.\n\nIs there a data dictionary or codebook to describe the variables and measurements, units, missing values, etc.?\n\nGenerally, the external data should 1) be well documented, 2) be collected and analyzed by reputable scientists and labs; and 3) have adequate accompanying metadata and management data to facilitate interpretation of the results.\nSome publicly available datasets to consider are listed in Y:\\NRAS\\Soil_Health_Initiative\\DataSources."
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "7  Documentation",
    "section": "",
    "text": "Enough farm, producer and field info to distinguish unique farmers and fields and assign IDs to. This info doesn’t need to be personally identifiable information.↩︎"
  },
  {
    "objectID": "flow.html#project-structure",
    "href": "flow.html#project-structure",
    "title": "8  Data flows",
    "section": "8.1 Project structure",
    "text": "8.1 Project structure"
  },
  {
    "objectID": "flow.html#original-data",
    "href": "flow.html#original-data",
    "title": "8  Data flows",
    "section": "8.2 Original data",
    "text": "8.2 Original data"
  },
  {
    "objectID": "flow.html#working-data",
    "href": "flow.html#working-data",
    "title": "8  Data flows",
    "section": "8.3 Working data",
    "text": "8.3 Working data"
  },
  {
    "objectID": "flow.html#clean-data",
    "href": "flow.html#clean-data",
    "title": "8  Data flows",
    "section": "8.4 Clean data",
    "text": "8.4 Clean data"
  },
  {
    "objectID": "share.html#data-sharing-and-public-access",
    "href": "share.html#data-sharing-and-public-access",
    "title": "9  Data sharing",
    "section": "9.1 Data sharing and public access",
    "text": "9.1 Data sharing and public access\nSoSA relies on growers’ willingness to volunteer their fields for sampling and participate in the required management survey. Their willingness depends on their trust in WaSHI to protect their privacy. Only aggregated and anonymized results will be publicly available or shared. The below data privacy statement may be shared with potential participants:\n\nData privacy statement\n\nData will be aggregated and reported in a way which mitigates personal identification of growers. Information will be used to understand broad impacts and characterize trends in soil health and production practices across regions. Results will not be reported in a way that makes individuals identifiable. Information collected in this survey may be subject to release in accordance with RCW 42.56 (Public Records Act).\n\nProcedures for anonymizing data are detailed in Section 9.4.\n\n\nAcknowledging WaSHI data in publications\nAll research partially or completely funded by WaSHI must include acknowledgements to the State of Washington. The following text should be included in all publications resulting from this funding:\n\nData was in part provided by the Washington Soil Initiative, which is supported by the State of Washington and administered by the Washington State Department of Agriculture, Washington State Conservation Commission, and Washington State University.\n\nIf WaSHI staff make substantial scientific contributions to the manuscript, discuss the possibility of co-authorship credit."
  },
  {
    "objectID": "share.html#public-repositories",
    "href": "share.html#public-repositories",
    "title": "9  Data sharing",
    "section": "9.2 Public repositories",
    "text": "9.2 Public repositories\n\nGitHub\nZenodo\ndata.gov"
  },
  {
    "objectID": "share.html#understand-watech-data-categorization",
    "href": "share.html#understand-watech-data-categorization",
    "title": "9  Data sharing",
    "section": "9.3 Understand WaTech data categorization",
    "text": "9.3 Understand WaTech data categorization\nhttps://watech.wa.gov/Categorizing-Data-State-Agency"
  },
  {
    "objectID": "share.html#sec-maintain-confidentiality",
    "href": "share.html#sec-maintain-confidentiality",
    "title": "9  Data sharing",
    "section": "9.4 Maintain confidentiality",
    "text": "9.4 Maintain confidentiality\nAnonymize and aggregate"
  },
  {
    "objectID": "share.html#timeline",
    "href": "share.html#timeline",
    "title": "9  Data sharing",
    "section": "9.5 Timeline",
    "text": "9.5 Timeline\nWhen to share data (after publication?), how long to share data?"
  },
  {
    "objectID": "code-guide.html#section",
    "href": "code-guide.html#section",
    "title": "10  Code Style Guide",
    "section": "10.1 ",
    "text": "10.1 \nTemplate snippet\nSection break snippet\n.R and .qmd file names\nfunction names\nargument names\nvariable names\nhttps://indrajeetpatil.github.io/second-hardest-cs-thing/"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carlson, Bryan. 2021. “Data Management Plan for the\nR.J. Cook Agronomy Farm Long-Term Agroecological Research\nSite.”\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education\nResearch [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life\nCycle.” https://www.fws.gov/data/life-cycle.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific Data Management\nand Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "Overview\nThe Washington Soil Health Initiative (WaSHI) is a partnership between the Washington State Department of Agriculture (WSDA), Washington State University (WSU), and the State Conservation Commission. WaSHI establishes a coordinated approach to healthy soil in Washington.\nTo date, nearly 1,000 soil samples and management surveys across 50 different cropping systems have been collected as a part of the State of the Soils Assessment (SOS). WSDA and WSU lead this project with support from staff, students, conservation districts, and agricultural professionals throughout Washington."
  },
  {
    "objectID": "index.html#chapter-outline",
    "href": "index.html#chapter-outline",
    "title": "Data Management Plan",
    "section": "Chapter outline",
    "text": "Chapter outline\nThis Data Management Plan (DMP) is a living document to be continually reviewed and improved based on lessons learned, new information, and collaborator feedback.\nChapter 1 describes what data management is, how it is crucial to achieve our data-driven goals, and how our data move through the data life cycle.\nChapter 2 describes the various data formats we collect and manage. ISO standards are also described for date and geospatial data.\nChapter 3 describes naming conventions, best practices, and examples for how we name folders and files.\nChapter 4 describes how we organize our folders into a hierarchical structure.\nChapter 5 describes where we store data, what our backup policies are, how we protect our raw data, and how we use version control.\nChapter 6 describes how we record each element of the data life cycle with project-level, dataset-level, and variable-level documents such as standard operating procedures, readme files, data dictionaries, etc.\nChapter 7 describes how data are generated, processed, and moved from start to finish. Processes and tasks are grouped by pre, during, and post field season.\nChapter 8 describes how we protect producer privacy; how our data fits into WaTech data categories; requirements and processes for maintaining confidentiality; and our data share agreement, public access policies, and our preferred acknowledgements.\nChapter 9 describes our recommended project structures, code-specific naming conventions, script structures, and code style.\n\n\n\n\n\n\nLinks to shared drive folders and files\n\n\n\nThis DMP is best viewed in Google Chrome since the browser can open links to our local folders and files using the Enable local file links extension that should automatically be enabled by our organization.\nHowever, when viewing this DMP in Microsoft Edge, links to our shared drive are not accessible in the browser. Nothing happens if you click on these links. To open the file or folder, right-click on the hyperlink &gt; copy the path &gt; paste it into the search bar of the file explorer &gt; press Enter or click the arrow."
  },
  {
    "objectID": "index.html#roles-and-responsibilities",
    "href": "index.html#roles-and-responsibilities",
    "title": "Data Management Plan",
    "section": "Roles and responsibilities",
    "text": "Roles and responsibilities\nAll WaSHI personnel who will be interacting with SOS data must familiarize themselves with the contents of this document. If all collaborators are not consistently implementing this DMP, then the benefits of effective data management are lost.\nThe WSDA Data Scientist, supported by the Co- Principal Investigators (CoPIs), is responsible for providing guidance to WaSHI staff working with SOS data and ensuring the implementation of this DMP. The Data Scientist is also responsible for reviewing and updating this document annually, and as needed. Upon updates, the Data Scientist will distribute this document to WaSHI staff and commit the source code to the GitHub repository.\n\nCurrent roles as of January 2024\n\n\n\n\n\n\n\n\nRole\nAffiliation\nName\nTitle\n\n\n\n\nCoPI\nWSDA\nDani Gelardi\nSenior Soil Scientist\n\n\nCoPI\nWSU\nDeirdre Griffin LaHue\nAssistant Professor\n\n\nData Manager\nWSDA\nJadey Ryan\nData Scientist\n\n\nData Stewards\nWaSHI personnel"
  },
  {
    "objectID": "index.html#staff-turnover",
    "href": "index.html#staff-turnover",
    "title": "Data Management Plan",
    "section": "Staff turnover",
    "text": "Staff turnover\nWhen staff leave our group, they take their skills, institutional knowledge, and personal understanding of their file management with them. Proper offboarding is essential to ensure knowledge isn’t lost, time isn’t wasted trying to recreate workflows, and projects keep moving.\nBefore the employee leaves, the Senior Soil Scientist and Data Scientist ensure that:\n\nFolders and files are moved from the employee’s personal drive to the shared drive. They are named and organized according to Chapter 3.\nWorkflows and specific processes the employee was responsible for are well documented.\nPermissions and ownerships are transferred to the appropriate remaining staff.\n\nGitHub WSDA organization\nArcGIS data products and online groups\nDatabase credentials\nBox.com folders\n\n\nMore resources and offboarding checklists from Harvard Research Data Management can be found in our data-management shared drive."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Management Plan",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis DMP was adapted from the R.J. Cook Agronomy Farm Long-Term Agroecological Research Site DMP (Carlson 2021), U.S. Fish and Wildlife Service data management life cycle (U.S. Fish & Wildlife Service 2023), Harvard Medical School Longwood Research Data Management DMP guidelines (Harvard Medical School 2023), and the Data Management in Large-Scale Education Research book (Lewis 2023).\n\n\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the R.J. Cook Agronomy Farm Long-Term Agroecological Research Site.”\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle."
  },
  {
    "objectID": "data-management.html#data-life-cycle",
    "href": "data-management.html#data-life-cycle",
    "title": "1  What is data management?",
    "section": "1.1 Data life cycle",
    "text": "1.1 Data life cycle\n\n\nThe U.S. Fish and Wildlife Service developed a great graphic to explain the elements of the data life cycle and emphasize the importance of data quality at every step (U.S. Fish & Wildlife Service 2023). Each step within the data life cycle requires careful intention to ensure transparency, quality, and integrity.\n\n\n\n\n\n\n\n\n\n\nOur adaptation of this data life cycle is outlined below.\n\n\n\n\n\n\n\n\n\n\nPlan\nEach sampling year presents an opportunity to consider what worked and what could be improved from the previous year. Planning involves making decisions about data acquisition, management, and quality control. For example, each year we provide a spreadsheet template with our requested column headers to Soiltest lab to ensure the measurements are reported with correct units and in the format we use. Special projects that deviate from our standard operating procedures require additional planning.\n\n\n\n\n\nAcquire\nWe acquire data by collecting and analyzing new samples, deriving new insights from existing data, or accepting datasets from collaborators.\n\n\n\n\n\nMaintain\nMaintenance involves processing data for aggregation, analyses, and reporting. We create metadata that facilitates interpretation of the data. We also store at least one copy of our data in a non-proprietary format that is accessible to our collaborators and future selves.\n\n\n\n\n\nAccess\nAccess refers to data storage, publication, and security. Raw and processed data with accompanying metadata should be stored, backed up, and available for information sharing with our partners. With PI approval, anonymized and aggregated data that does not compromise growers’ personally identifiable information (PII) should be made publicly available in a data repository or data product/decision-support tool.\n\n\n\n\n\nEvaluate\nWe evaluate data while processing and analyzing it to maximize accuracy and productivity, while minimizing costs associated with errors or tedious data cleaning labor. Evaluation workflows should be efficient, well-documented, and reproducible. Our evaluated data help us better understand how environmental factors and management decisions impact soil health.\n\n\n\n\n\nArchive\nProperly archiving our results supports the long-term storage and usefulness of our data. While similar to the Access element of the life cycle, archiving focuses on preserving data for long-term/historical retention that aren’t needed for immediate access. For example, we archive each year’s raw data for long-term storage and set those files to Read-Only.\n\n\n\n\n\nQuality Assurance / Quality Control (QA/QC)\nData quality management prevents data defects that hinder our ability to apply data towards our science-based conservation efforts. Defects include incorrectly entered data, invalid data, and missing or lost data. QA/QC processes should be incorporated in every element of the data life cycle.\n\n\n\n\n\nThe following chapters describe our internal processes and standards to follow throughout each step in the data life cycle.\n\n\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "formats-standards.html#data-formats",
    "href": "formats-standards.html#data-formats",
    "title": "2  Formats & standards",
    "section": "2.1 Data formats",
    "text": "2.1 Data formats\nData generated from or integrated into WaSHI can be non-digital or digital.\n\nNon-digital data\nNon-digital data, such as field forms, management surveys, and chain of custody forms, are manually recorded on paper forms. Paper forms must be transcribed or converted to digital file formats and then stored in the WaSHI filing cabinet in the Natural Resources Building in Olympia.\n\n\nDigital data\nDigital data include tabular, spatial, and binary data, such as lab results, sample locations, and field photos. Non-conventional data also include code, algorithms, tools, and workflows.\nTabular data include comma separated values (csv), tab separated values (tsv), Microsoft Excel open XML spreadsheet (xlsx), and portable document format (pdf).\nSpatial data include file geodatabases (gdb), vector shapefiles (zipped folder containing multiple file extensions), keyhole markup language (kml or kmz). Tabular data may also contain spatial data such as longitude and latitude.\nBinary data include photos (jpeg, png, gif, tiff), videos (mp4), code (R, py, js), and object-oriented data files (RDS, Rdata, parquet, arrow).\nProprietary data formats include Microsoft Excel, Word, and Powerpoint files (xlsx, docx, pptx). RDS and RData files are examples of application-specific data formats that can only be opened using the R programming language or RStudio IDE. These types of files should be saved in conjunction with a copy of the data in a non-proprietary and open-standard format, such as csv, to maintain accessibility for those who do not have Microsoft Office or do not use R.\nWritten documents and presentations are in formats including Microsoft Word and Powerpoint (docx and pptx), hypertext markup language (HTML), and pdf.\nNotebooks combine text with executable code to generate written documents and presentations in docx, pptx, html, or pdf formats. These notebooks are stored in formats depending on the programming language: a few examples include R markdown (rmd), Quarto (qmd), and Jupyter notebook (ipynb).\nThe list below is not exhaustive and will continue to grow as additional useful data sources are discovered.\n\n\n\n\nType\nSource\nFormats\n\n\n\n\nLab results\nProvided by the lab analyzing the soil sample, principal investigator of a study, or grower\ncsv, xlsx, pdf, xml, json, RDS, RData\n\n\nManagement surveys\nCollected through interviews with grower\ncsv, xlsx, RDS, RData, paper form (to be digitized)\n\n\nField forms\nCompleted in the field during/immediately after sampling\npdf, paper form (to be digitized), csv, xlsx\n\n\nSample locations\nIdentified prior to sampling and may be edited during sampling using ArcGIS Online, Collector, Field Maps or Google Maps\nArcGIS feature layer, shp, kmz, csv, xlsx\n\n\nChain of custody forms\nCompleted prior to shipping or dropping off samples\npdf, paper form (to be digitized)\n\n\nClimate data\nOSU PRISM, NOAA, Esri Living Atlas\ncsv, shp, netCDF, tiff, gdb\n\n\nSoil data\nNRCS Web Soil Survey, NRCS WA gSSURGO\ngdb, accdb\n\n\nStrata classification\nProvided by Soil Health Institute in 2021 as a lyr file\nlyr\n\n\nImages\nLogos, icons, photos taken in the field\njpeg, png, gif, tiff, svg\n\n\nVideos\nRecordings of meetings, training videos\nmp4\n\n\nDocuments\nReports, manuscripts, SOP, QAPP, factsheets, brochures\ndocx, txt, html, pdf\n\n\nPresentations\nPowerpoints, slide decks\npptx, html, pdf\n\n\nCode\nScripts for wrangling, processing, analyzing data; markdown for producing documents and presentations; style sheets for html outputs\nR, py, ipynb, js, yml, rmd, qmd, css, scss"
  },
  {
    "objectID": "formats-standards.html#sec-data-standards",
    "href": "formats-standards.html#sec-data-standards",
    "title": "2  Formats & standards",
    "section": "2.2 Data standards",
    "text": "2.2 Data standards\nDate will be expressed as YYYY-MM-DD according to the ISO 8601 standard.\nDate with time will be expressed as YYYY-MM-DDTHH:MM:SSZ.\n\nT separates date from time. The Z indicates the date-time is using the Universal Time Coordinated (UTC) with no offset.\nPacific Standard Time (PST) has a UTC-8:00 offset and Pacific Daylight Time (PDT) has a UTC-7:00 offset and would be expressed as YYYY-MM-DDTHH:MM:SS-8:00. The Z has been replaced with the offset.\nExample: 2023-11-28T14:55:56-08:00.\n\n\n\n\n“ISO 8601” from Randall Munroe’s xkcd\n\n\nGeospatial data will be accompanied by metadata that abides by the ISO 19115 standard by following Esri’s documentation when working in ArcGIS Pro. Metadata contains information about the identification, the extent, the quality, the spatial and temporal schema, spatial reference, and distribution of digital geographic data.\nCode will follow the style guide in Chapter 9."
  },
  {
    "objectID": "naming.html#why-are-conventions-important",
    "href": "naming.html#why-are-conventions-important",
    "title": "3  Naming conventions",
    "section": "3.1 Why are conventions important?",
    "text": "3.1 Why are conventions important?\n\nImproves consistency and predictability, making it easier to browse folders and know what the file/folder contains.\nEasily sort and retrieve files by date, conservation district, or another theme.\nFacilitates collaboration so all team members can find the information they need.\nStandardizes file paths and URLs for efficient programming and website hosting.\n\nURLs and programming languages are case-sensitive. WaSHI-data.csv and washi-data.csv are completely different files.\nURLs cannot have spaces in them. They must be escaped with this character entity %20. For example, wasoilhealth.org/producer spotlights would need to be wasoilhealth.org/producer%20spotlights.\n\n\nFor more web-specific naming conventions, see this Learn the Web webpage."
  },
  {
    "objectID": "naming.html#sec-naming-best-practices",
    "href": "naming.html#sec-naming-best-practices",
    "title": "3  Naming conventions",
    "section": "3.2 Best practices",
    "text": "3.2 Best practices\nSome files and folders in our shared drive do not follow these best practices or naming conventions. We are learning and improving as we go.\nThese are guidelines and naming things is hard, so try your best. If you’re not sure how to name some files or you’re adding a bunch of files that came from somewhere else, Jadey can help you organize and/or bulk rename them.\nSee Section 3.3 for a table of examples of folder and file names following these best practices.\n\nMeaningful name casing\nDifferent conventions work better for different purposes (folders and files versus programming objects).\n\nkebab-case: all lowercase with hyphens separating words. Use for all folders and files.\nsnake_case: all lowercase with underscores separating words. Only use for code, such as functions and variables in R. See Section 9.2.3.2 for example R errors when including hyphens in object names.\nUpperCamelCase: first letter of each word is capitalized. Only use for variables in spreadsheets and tables. If we used snake_case, the soil health indicators with units would be way too long. Using an _ to separate the measurement from the unit allows us to transform more easily our data with code. 96HrMinC_MgCKgDay instead of 96_hr_min_c_mg_kg_day. See Section 3.2.10 and Section 9.2.3.1 for more details.\n\n\n\n\nCartoon representations of common cases in coding. A snake screams “SCREAMING_SNAKE_CASE” into the face of a camel (wearing ear muffs) with “camelCase” written along its back. Vegetables on a skewer spell out “kebab-case” (words on a skewer). A mellow, happy looking snake has text “snake_case” along it. Artwork by @allison_horst.\n\n\n\n\nDelimiters convey meaning\nDeliberately use underscores and hyphens so we can easily understand the contents and programmatically parse file and folder names.\n\nUse underscores to delineate metadata elements (i.e. date from name from version date_name_version).\nUse hyphens to separate parts of one metadata element (i.e. date YYYY-MM-DD or name wsda-washi-presentation).\n\n\n\nNo spaces or special characters\nAvoid spaces and special characters (only use underscores and hyphens). Characters like / () ! ? % + \" ' have special meaning to computers and can break file paths and URLs.\n\n\nCharacter length matters\nComputers are unable to read file paths and file names that surpass a certain character length. Be concise AND descriptive. Omit prepositions and articles when possible. Abbreviate long words. The path limit on Windows is 260 characters.\n\n\n‘Back to front’ date\nRemember to express date ‘back to front’ like YYYY-MM-DD according to the ISO 8601 standard. Left pad single-digit months and days with a zero. This maintains the chronological order of records when they are sorted alphanumerically.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\n2020-05-28_agenda.pdf\n2-14-2023_Agenda.pdf\n\n\n2023-01-01_agenda.pdf\n2023-Jan-1_Agenda.pdf\n\n\n2023-02-14_agenda.pdf\nDec052020_Agenda.pdf\n\n\n2023-12-05_agenda.pdf\nMay_28_2020_Agenda.pdf\n\n\n\n\n\nGroup & sort files by name\nConsider how folders and files should be grouped and sorted. Put that piece of metadata in the beginning of the file name. By putting the conservation district name in the front, we can sort by district. Or, we can sort by date by putting the date before the district name.\n\n\n\n\n\n\n\nSort by district\nSort by date\n\n\n\n\ncowlitz_coc_2023-05-01.pdf\n2023-05-01_cowlitz_coc.pdf\n\n\ncowlitz_coc_2023-05-23.pdf\n2023-05-01_cowlitz_tracking.pdf\n\n\ncowlitz_tracking_2023-05-01.pdf\n2023-05-09_ferry-cd_tracking.pdf\n\n\nferry-cd_coc_2023-05-10.pdf\n2023-05-10_ferry-cd_coc.pdf\n\n\nferry-cd_coc_2023-05-17.pdf\n2023-05-17_ferry-cd_coc.pdf\n\n\nferry-cd_coc_2023-06-06.pdf\n2023-05-23_cowlitz_coc.pdf\n\n\nferry-cd_tracking_2023-05-09.pdf\n2023-06-06_ferry-cd_coc.pdf\n\n\n\n\n\nVersion numbers\nIncluding the date in the file name is one way to version a document. Alternatively, or in addition to, we can add a version number to the end of the file name. Consider how many possible versions there could be. If there may be more than 10, use leading zeros to left-pad single digit numbers so the file name always has the same length. v1 through v15 will not sort the same way as v01 through v15.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\nsop_v01.pdf\nSOP_v1.pdf\n\n\nsop_v02.pdf\nSOP_v10.pdf\n\n\n… [v03 - v09]\nSOP_v11.pdf\n\n\nsop_v10.pdf\nSOP_v2.pdf\n\n\nsop_v11.pdf\n… [v3 - v9]\n\n\n\n\n\nCollaboration\nAdd your initials to the end of the file name when “saving as” a file that multiple people are working on (i.e., 2023_sop-soil-health-monitoring_lm-jr.docx). This ensures a version is kept as a backup. Alternatively, use Track Changes if working in a MS Word document.\n\n\nLiterature and references\nWhen saving journal articles, user guides, and other reference materials, use the convention author_year_abbreviated-title where each authors’ last name is separated with a hyphen, the title is abbreviated, and hyphens separate each word of the title. Remember to use underscores to separate different metadata.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\nlal_2004_soil-c-to-mitigate-cc.pdf\nlal-2004-soil-c-to-mitigate-cc.pdf\n\n\nclark-et-al_2020_pmn-sampling\nclark-et-al_2020_pmn-sampling\n\n\n\n\n\nVariables and code\nNaming conventions for variables and code differ from folders and files. The hyphens in kebab-case cause errors in R and SQL code. Additionally, hyphens, spaces, and other special characters are invalid for ArcGIS table and field names.\n\nUse UpperCamelCase for variables (column names of spreadsheets and tables).\nUse snake_case for naming code objects, including R data structures (vectors, lists, dataframes) and functions.\n\nIn variable names, we include the measurement with the unit for clarity and self-documentation when sharing our data. This prevents unit confusion and reduces the risk of misinterpreting or inappropriately using the data. Using UpperCamelCase with an underscore to separate the measurement from the unit helps with readability and consistency.\nDo not use special characters. Instead of TOC_%, use TOC_Percent.\nSee Chapter 9 for more details in the code style guide."
  },
  {
    "objectID": "naming.html#sec-naming-examples",
    "href": "naming.html#sec-naming-examples",
    "title": "3  Naming conventions",
    "section": "3.3 Naming examples",
    "text": "3.3 Naming examples\n\n\n\n\n\n\n\n\n\n\nNaming convention\nExamples\n\n\n\n\nFolders\nkebab-case\n2024_sampling\ndata-management\n\n\nFiles\nkebab-case\n2023-11-15_survey-perennial.xlsx\nrecords-management_v01.docx\npend-oreille-cd_coc_2023-06-05.pdf\ngeisseler-et-al_2019_ace-protein.pdf\nwashi-logo-color.png\nwashi-dmp.Rproj\n01_load-metadata.R\n2024_producer-report.qmd\n\n\nVariables\n(column names)\nUpperCamelCase\nSampleId\nTOC_Percent\nPMN_MgKg\n\n\nCode\nsnake_case\nprimary_color\ncrop_summary\nassign_quality_codes()"
  },
  {
    "objectID": "organization.html#folder-structure",
    "href": "organization.html#folder-structure",
    "title": "4  Organization",
    "section": "4.1 Folder structure",
    "text": "4.1 Folder structure\nWe strive for a balance between a deep and shallow structure. If too shallow, there are too many files in one folder and they are hard to sort through. If too deep, we have to click too many times to get to a file and specific files can be difficult to find.\nY:/NRAS/soil-health-initiative is the parent folder for all WaSHI content.\nWithin the state-of-the-soils subfolder, we use a combination of date- (each year has its own subfolder) and categorical- based (dataset and documentation that span across years) folder structures.\nY:/NRAS/soil-health-initiative/state-of-the-soils/\n├── _complete-dataset\n├── 2019_scbg\n├── 2021_sampling\n├── 2022_partnerships-in-soil-health\n├── 2023_sampling\n├── 2024_sampling\n├── data-management\n├── data-sharing\n├── data-sources\n├── maps\n├── projects\n├── qapp\n├── sop\n├── training-videos\n├── equipment-inventory.xlsx\n├── archived-sample-inventory.xlsx\n└── sos-impacts.xlsx\nWithin each year subfolder, we have sub-subfolders for planning, forms, data, and processes. This structure helps maintain a reproducible workflow year after year. See the 2023_sampling folder tree for an example:\nY:/NRAS/soil-health-initiative/state-of-the-soils/2023_sampling/\n├── applications\n├── coc\n├── equipment\n├── field-forms\n├── forms\n├── gis\n├── lab-data\n├── labels\n├── management-surveys\n├── public-docs\n├── purchases\n├── reports\n├── sample-id-assignments\n├── scripts\n├── 2023-data-tracking.xlsx\n└── 2023_post-season-wrap-up.docx\nIt’s good practice to maintain the raw data, see how to set files to Read-Only in Section 5.2. We use additional subfolders for the lab-data folder. Everything in raw has been set as Read-Only.\nY:/NRAS/soil-health-initiative/state-of-the-soils/2023_sampling/lab-data\n├── 2023_data-template-soiltest.xlsx\n├── clean\n├── qc\n├── raw\n└── working\nsoil-health-initative &gt; state-of-the-soils &gt; 2023_sampling &gt; lab-data &gt; clean already has five levels of nesting. We wouldn’t want to add many more levels, or the hierarchy becomes difficult to manage."
  },
  {
    "objectID": "organization.html#archive-folders",
    "href": "organization.html#archive-folders",
    "title": "4  Organization",
    "section": "4.2 Archive folders",
    "text": "4.2 Archive folders\nWhen too many drafts or versions begin to clutter a subfolder, create a new folder with the naming convention of archive-folder-description. Place the old drafts there. Leave the most current, accurate file in the main folder.\nFor example, the most recent sample labels for each conservation district are listed in the top level completed-labels folder, and previous working drafts were moved to the archive-labels folder.\nY:/NRAS/soil-health-initiative/state-of-the-soils/2023_sampling/labels/completed-labels\n├── archive-labels\n│   ├── cowlitz-county_labels.docx\n│   ├── ferry-cd_labels.docx\n│   ├── lewis-cd_labels.docx\n│   └── stevens-cd_labels.docx\n├── cowlitz-county_labels_v2.docx\n├── ferry-cd_labels_v2.docx\n├── ...\n├── south-yakima-cd_labels.docx\n├── stevens-cd_labels_v2.docx\n└── walla-walla-cd_labels.docx"
  },
  {
    "objectID": "organization.html#code-based-project-organization",
    "href": "organization.html#code-based-project-organization",
    "title": "4  Organization",
    "section": "4.3 Code-based project organization",
    "text": "4.3 Code-based project organization\nCode-based projects should be organized according to Section 9.1.1 in the code style guide."
  },
  {
    "objectID": "storage.html#backup",
    "href": "storage.html#backup",
    "title": "5  Storage",
    "section": "5.1 Backup",
    "text": "5.1 Backup\nData must be stored in multiple locations. At a bare minimum, data on an individual computer must also be saved on the WSDA shared drive. Backing up data using version control (GitHub) or a cloud service (Microsoft OneDrive or Box.com) is strongly recommended."
  },
  {
    "objectID": "storage.html#sec-raw-data",
    "href": "storage.html#sec-raw-data",
    "title": "5  Storage",
    "section": "5.2 Read-only raw data",
    "text": "5.2 Read-only raw data\nOn our shared drives, raw data such as lab results from Soiltest or exports from ArcGIS Online, should immediately be set to Read-only. Right click the file &gt; click on Properties &gt; check the Read-only attribute box.\n\nThe file should then be copied over to a working folder for any processing or analyses. The final dataset should be saved in a separate clean folder, with a descriptive title. Keeping a readme.txt to document your processing steps is good practice, as discussed in Section 6.2.1."
  },
  {
    "objectID": "storage.html#sec-version-control",
    "href": "storage.html#sec-version-control",
    "title": "5  Storage",
    "section": "5.3 Version control with Git and GitHub",
    "text": "5.3 Version control with Git and GitHub\nA version control system records changes to a file or set of files over time. Git is a free and open-source distributed version control system and GitHub is the hosting site WSDA and WaSHI use to interface with this system. Git and GitHub are an important foundation of reproducible statistical and data scientific workflows (Bryan 2018).\nA major benefit of using version control is ensuring changes are well documented and previous versions are accessible if any changes must be recalled. Additionally, version control makes collaboration across projects much more robust.\nVersion control is not just for code either! It’s useful for scripts, documents, presentations, and books (like this DMP!). Instead of saving each version of a file with a different name (i.e., report_v01.docx and report_v02.docx; for a reminder on version naming, see Section 3.2.7), there’s only one file report.docx which automatically has its history and editors saved with Git and GitHub.\nIn the screenshot below, you can see who made a commit (which is basically a named version of changes), when that commit was made, and you can click on the commit message to view all of the files that were changed.\n\nAfter clicking on the first commit message, we see the documentation.qmd file was changed with additions highlighted in green and deletions highlighted in red.\n\n\n\n\n\n\nPrivacy considerations\nReview Chapter 8 to categorize the data included in the repository. If the data are not anonymized and aggregated, either 1) the repository must be set to private or 2) data files and any scripts containing Category 3 data as described in Section 8.1.0.2 must be added to the .gitignore file.\n\n\nGit and GitHub resources\nRead Jenny Bryan’s article Excuse Me, Do You Have a Moment to Talk About Version Control for a great background on Git and GitHub, why we should be using it, and a brief how to get started. For detailed instructions , please follow along with Jenny Bryan’s free online book Happy Git and GitHub for the useR. Another helpful book resource is GitHub: A Beginner’s Guide, which was created by Birds Canada (avian conservation NGO) for people without a lot of programming background.\nIf you prefer to look through slides, check out Byron C. Jaeger’s presentation Happier version control with Git and GitHub (and RStudio).\n\n\n\n\nBryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928."
  },
  {
    "objectID": "documentation.html#project-level",
    "href": "documentation.html#project-level",
    "title": "6  Documentation",
    "section": "6.1 Project-level",
    "text": "6.1 Project-level\nProject-level documentation includes all descriptive information about the SOS dataset, as well as planning decisions and process documentation. Documentation includes quality assurance project plans, standard operating procedures, and other high-level documents (i.e., request for proposals, applications, meeting agendas/notes, etc.).\n\nQuality assurance project plan (QAPP)\nThe QAPP is the highest level of project documentation and covers everything from the project description; personnel roles and responsibilities; project timelines; data and measurement quality objectives; study design; and overviews of field, laboratory, and quality control.\nOurs can be found in Y:/NRAS/soil-health-initiative/state-of-the-soils/qapp, though it needs to be updated.\n\n\nStandard operating procedures (SOP)\nSOPs provide detailed instructions for field, lab, or data processing procedures and decision making processes.\nOurs can be found in Y:/NRAS/soil-health-initiative/state-of-the-soils/sop.\n\nSOS sampling\nThe purpose of this SOP is to detail the procedures for a typical site visit in which soil samples are collected for physical, chemical, and biological soil health indicator analyses. Procedures include equipment preparation prior to sampling; best practices for filling out field forms; the selection of sampling locations; sampling protocols; sample handling and storage; and submitting samples to the lab. Following this SOP ensures data quality by creating audit trails and reminders to check that data are present, complete, and accurate. Additionally, this SOP will be used to maintain consistent sample collection procedures throughout the state for WSDA employees and partners.\n\n\nQuality control / quality assurance (QA/QC)\nThis SOP outlines the process for screening sample metadata and lab results for completeness, consistency, and quality. Procedures involve subject matter expertise, investigation, communication with sampling teams and labs, algorithmic quality control, and tagging sample results with quality codes (listed in the below table). Data are then integrated into the statewide database according to a SOP not yet authored.\n\n\n\n\n\n\n\n\n\n\nCode\nTag\nDescription\nInclusion in analyses\n\n\n\n\n0\nExcellent\nMet lab’s and WSDA’s QC criteria\nYes\n\n\n100\nEstimate\nInterpolated missing value\nYes\n\n\n110\nDerived\nDerived from an estimated value\nYes\n\n\n120\nSuspect\nZ-score is ≥ |3|\nYes\n\n\n130\nCalculated ND\nCalculated value using at least one ND\nYes\n\n\n140\nNon-detect\nBelow the method detection limit\nNo\n\n\n160\nPoor\nDid not meet lab’s QC criteria\nNo\n\n\n180\nOutlier\nOutlier, designated by soil scientist\nNo\n\n\n200\nUnknown\nExternal dataset\nYes\n\n\n\n\n\nSOPs we don’t have (yet)\n\nassigning producer, field, and sample IDs\ndata cleaning\ndata import into the database\ndata storage\nexternal data integration"
  },
  {
    "objectID": "documentation.html#dataset-level",
    "href": "documentation.html#dataset-level",
    "title": "6  Documentation",
    "section": "6.2 Dataset-level",
    "text": "6.2 Dataset-level\nDataset-level documentation applies to lab results, sample locations, grower information, and management data. We use readmes and changelogs to document what each dataset contains, how they are related, potential issues to be aware of, and any alterations made to the data.\n\nReadme\nreadme files are plain text documents that contain information about the files in a folder, explanation of versioning, and instructions/metadata for data packages. These files are saved as .txt, instead of MS Word documents that take longer to open and can only be opened on computers with Microsoft installed.\n\nDescribe contents of folder\nFor example, the _complete-dataset folder contains a readme.txt that describes each files’ structure, contents, and other pertinent information, such as the data source.\n\n\nView the example\n\n2020-2023_lab-results_wide has one sample per row with each measurement as a separate column.\n\n2020-2023_lab-results_long has one result per row with columns for sample identification, measurement name, result value, quality code, and lab.\nThis spreadsheet also contains a second tab that describes the quality codes.\n\n2020-2023_sample-locations has the coordinates, PRISM mean annual precipitation and mean annual temperature (800 m resolution, 30-year normals, 1991-2020), and SHI strata.\n\ndata-dictionary lists each variable with its short name (variable name without units), description, indicator type (chemical, physical, biological), unit (if applicable), label, and data type.\nThis spreadsheet also contains a second tab that describes the variables in the sampleLocations spreadsheet.\n\nscbg_producer-id_recode lists all SCBG fields with original SCBG assigned producer ID and new WSDA producer ID\n\nall-farm-info lists all participating farm names, producer names, producer IDs, field names, field IDs, and county\n\nexample-data.csv contains 100 random samples that have been anonymized with fake sampleIDs, county, farm, producer, and field names. WSU SCBG samples are excluded, as are the 0-6in to 6-12in WSDA samples.\n\n\n\nExplain versions\nAnother example is the readme.txt in the 2023_sampling &gt; lab-data &gt; raw folder, which explains why there are two different versions of the lab results and where to find additional information.\n\n\nView the example\n\n2023-08-21\n2023_wsda-soil-health_v1.xlsx has errors. See email for details.\n'          'v2.xlsx still has some errors, that are cleaned up in the R scripts.\n\n\n\nProvide instructions\nAn example of a readme.txt that provides instructions on how to use the folder contents can be found in the ArcGIS soil sample points box.com folder. When this folder is shared with our partners, the readme helps them orient to the contents of the folder and modify the files as needed for their own adaptation.\n\n\nView the example\n\nTemplate for Soil Sample Points ArcGIS\n\n2023-06-01\n\nJadey Ryan | Washington State Department of Agriculture (WSDA)\njryan@agr.wa.gov\n\nPurpose:\nTo provide a template for ESRI ArcGIS data entry and management for soil sampling projects.\n\nFolder contents:\n- readme.txt describes the folder contents and provides general instructions.\n- template-soil-sample-points.aprx is an ArcGIS Project File should allow you to open the project in ArcGIS Pro.\n- template-soil-sample-points.gdb is a file geodatabase, which you can open in ArcGIS Pro or ArcGIS Online.\n- crop-domain.csv provides WSDA approved crop types to use in ArcGIS 'Table to Domain' geoprocessing tool. We highly recommend using attribute domains, which are rules to enforce data integrity by limiting the field type and choices of an attribute field.\n- 923-nras-soil-health-sop-web.pdf is WSDA's Standard Operating Procedure for soil sampling. The appendices contain instructions for the GIS workflow of soil sampling.\n- /screenshots/feature-layer-offline-editing.png shows the checkboxes required for editing.\n- /screenshots/configure-forms.png shows where to click within a Web Map to open the Field Maps form editor.\n- /screenshots/table-to-domain.png is an example of how to use the 'Table to Domain' geoprocessing tool with the crop_domain.csv.\n- /screenshots/field-maps-form_*.png are examples of the Field Maps form structure, which can be created in ArcGIS Online.\n- washi_soil-series-rest-service is an internet shortcut to the URL for the Washington clipped soil series, originating from NRCS gSSURGO. When compositing multiple samples together from one field, we recommend keeping all sample points within one soil series to reduce variability in the composite samples.\n- The other folders (Index, GpMessages, ImportLog, .backups) are part of the ArcGIS Pro project and can be ignored.\n\nInstructions:\n1. Open template-soil-sample-points.aprx.\n2. Update attribute fields and domains to work with your project.\n3. Update symbology, labels, visibility scale, popups, etc. The symbology of the sample points currently defaults to red when the 'Show/Hide Field Form' attribute is still 'Hide' or 'NULL'. When the sampler collects the sample and changes this attribute to 'Show', the point will turn yellow to indicate the sample has been collected.\n4. Share as Feature Layer and create a Web Map to allow others access to this map.\n5. Open the map in ArcGIS Online and click on 'Forms' in the right toolbar to configure your Field Maps form.\n6. Use the /screenshots/field-maps-form_*.pngs as a guide, but adapt the field form structure to suit your project.\n\nOffline Areas:\n- If you anticipate needing to sample without cellular service or wifi access: 1) Open the hosted feature layer, 2) Click 'Settings' 3) Confirm 'Enable Sync' is turned on.\n- If you need the Soil Series layer, you will need to configure two separate Web Maps: one with and one without. This is because the washi_soil-series-rest-service is not allowed in Web Maps with 'Enable Sync' turned on. \n- Alternatively, you can create your own Soil Series layer and host it as a feature layer in your own organization.\n\nResources:\n- WSDA Soil Health YouTube channel: https://www.youtube.com/playlist?list=PL0pB20prk7Ni1daEYiEEXSWy8CfwO34FC\n- WSDA WaSHI_SoilSeries MapServer: https://fortress.wa.gov/agr/gis/wsdagis/rest/services/NRAS/WaSHI_SoilSeries/MapServer\n- Introduction to attribute domains: https://pro.arcgis.com/en/pro-app/latest/help/data/geodatabases/overview/an-overview-of-attribute-domains.htm\n- Introducing smart forms in ArcGIS Field Maps: https://www.esri.com/arcgis-blog/products/field-maps/field-mobility/introducing-arcgis-smart-forms/\n\n\n\n\nChangelog\nChangelogs are also simple and concise plain text documents saved in a folder alongside data files to document any changes to the dataset.\nAt the bare minimum, the changelog.txt should contain:\n\ndate of modification\ninitials of who made the changes\ndescription of the changes\n\nAn example changelog.txt can be found in the _complete-dataset folder.\n\n\nView the example\n\nContents of changelog.txt in _complete-dataset folder:\n\n2022-12-15 JR standardized texture classes (title case, no extra white space) and converted texture, county, and crop to factor types.\n2023-01-03 JR corrected error in 2022 WSDA bulk density measurements in 2022-11-01_soiltestData_manualCleanup.xlsx.\n2023-03-02 JR recoded SCBG producer IDs to match WSDA format and cleaned up farm/producer names and IDs. See new scbg_producerId_recode.csv for list.\n2023-03-07 JR corrected Okanogan producer and field names (Devany, Townsend).\n2023-03-21 JR corrected cropType \"Fallow, Idle\" to \"Fallow\" and 2021 SCBG \"Pea\" samples to \"Pea, Dry\", updated Crop Group column. Updated results and sample locations spreadsheets.\n2023-07-13 JR added labID to labResults datasets and updated dataDictionary accordingly. \n2023-08-21 JR added 2023 data and updated dataDictionary sample_locations tab.\n2023-08-27 JR corrected SCBG pulse samples crop from \"Pea\" to \"Pea, Dry\".\n2023-08-30 JR added an anonymized dataset (100 samples from 2022-2023).\n2023-09-05 JR added labID to 2023 results.\n2023-09-06 JR added sampling organization column to make impact tracking easier. Fixed merge issue that was causing the loss of some producer IDs. Add \"County\" to relevant CD names.\n2023-11-14 JR switched from .RData to .RDS file type so users can assign a new name when loading the data into R with `data_wide &lt;- readRDS(\"2020-2023_labResults_wide.RDS\")`.\n2023-11-14 JR added sampling dates and depths to all 877 samples. See addSampleDepthsDates for R script. \n2023-12-08 JR corrected CropGroup from Fallow to Cereal Grain for samples with CropType of Fallow, Wheat in 2020-2023_sampleLocations.csv."
  },
  {
    "objectID": "documentation.html#variable-level",
    "href": "documentation.html#variable-level",
    "title": "6  Documentation",
    "section": "6.3 Variable-level",
    "text": "6.3 Variable-level\nVariable-level documentation includes data dictionaries and codebooks, which are often talked about interchangeably. However, we’ll refer to the data dictionary as a tabular collection of names, definitions, and attributes about the variables in a dataset. Data dictionaries are ideally created in the planning phase of the project before data are collected. In contrast, codebooks provide descriptive, variable-level information and univariate summary statistics to allow users to understand the contents of a dataset without opening it. The codebook is created or updated after data are collected, cleaned, and validated.\n\nData dictionary\nIn a data dictionary, each row is a different variable, while each column is a different attribute of that variable. With a data dictionary, any user should be able to properly interpret each variable in our data.\nOur data-dictionary.xlsx in the _complete-dataset folder contains two tabs (lab-results and sample-locations) that describe the attributes of each variable.\nTODO: update variable names in R scripts, data dictionary, and Soiltest template.\n\n\nView the sample-locations dictionary\n\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\nunit\ndataType\n\n\n\n\nyear\nYear the sample was collected\n\nNumeric\n\n\ncounty\nCounty of the sampled field\n\nCharacter\n\n\nsampleId\nSample identification code\n\nCharacter\n\n\ncropGroup\nCrop group of the sampled field\n\nCharacter\n\n\ncropType\nCrop type of the sampled field\n\nCharacter\n\n\nstratum\nSoil Health Institute stratum\n\nCharacter\n\n\nMAP_mm.year\nOSU PRISM mean annual precipitation\n30-year normals (1991-2020) at 800 m resolution\nmm/year\nNumeric\n\n\nMAT_c\nOSU PRISM mean annual temperature\n30-year normals (1991-2020) at 800 m resolution\ndegrees C\nNumeric\n\n\nlongitude\nLongitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\nlatitude\nLatitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\n\n\n\n\nCodebook\nCodebooks provide more information (i.e., existing values/ranges and summary statistics) than the data dictionary and can be used to understand a very high-level summary of the processed data. There are many R packages that generate codebooks; however, we have not implemented this type of documentation for our project yet.\nCrystal Lewis gave the lightning talk A Comparison of Packages to Generate Codebooks. Once, our data live in a database, I’d like to generate codebooks."
  },
  {
    "objectID": "documentation.html#sec-external-data",
    "href": "documentation.html#sec-external-data",
    "title": "6  Documentation",
    "section": "6.4 External data",
    "text": "6.4 External data\nExternal data refers to any data not directly collected by WSDA or trained partners (e.g., WSU or conservation districts) that follow our SOPs. These can include other studies pre-dating WaSHI, special soil health surveys, or publicly available datasets.\nThe Data Scientist and Senior Soil Scientist will decide whether to integrate an external dataset case by case by considering the following questions:\n\nHow does the study design fit into SOS goals?\nWho collected the soil samples?\nWhat field procedures were used and how were they documented?\nWho analyzed the soil samples? With which methods and QA/QC procedures?\nWhich pieces of metadata and management data accompany the lab results?\n\nFarm, producer and field info1\nSampling date\nSampling depth\nLatitude and longitude\nProduction system (current crop, crop rotation, etc.)\nTillage, livestock grazing, irrigation, soil fertility and amendments, conservation practices, etc.\n\nIs there a data dictionary or codebook to describe the variables and measurements, units, missing values, etc.?\n\nGenerally, the external data should 1) be well documented, 2) be collected and analyzed by well-trained scientists and labs; and 3) have adequate accompanying metadata and management data to facilitate interpretation of the results.\nSome publicly available datasets to consider are listed in Y:/NRAS/soil-health-initiative/state-of-the-soils/data-sources."
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "6  Documentation",
    "section": "",
    "text": "Enough farm, producer and field info to distinguish unique farmers and fields for assigning unique IDs. This info doesn’t need to be personally identifiable information.↩︎"
  },
  {
    "objectID": "flow.html#pre-field-season",
    "href": "flow.html#pre-field-season",
    "title": "7  Data flow",
    "section": "7.1 Pre field season",
    "text": "7.1 Pre field season\n\nAssign unique identifiers\nBefore sample IDs can be assigned, collect the following information for each proposed sample:\n\nCounty\nOrganization of sampling team\nFarm name (optional)\nProducer name\nProducer contact information (optional)\nField name\nCrop\nGeneral management practice (i.e., conventional, cover crop, reduced tillage)\n\nView examples of the 2023 Sample Request Form sent to CDs and the Berries Sample Request Form for the WSDA/WSU special project.\nOnce producers and fields have been identified, we assign a unique ID for the producer, field, and sample with the following convention:\n\nProducer ID: first three letters of county + three-digit landowner number\n\nWHA001\n\nField ID: two-digit field number\n\n01 and 02\n\nPair ID (optional): letter extension added to paired fields\n\nA\n\nSample ID: last two digits of year + Producer ID + Field ID + Pair ID\n\n24-WHA001-01-A and 24-WHA001-02-A\n\n\nThe following counties have different abbreviations than their first three letters:\n\nClallam → CLL\nGrays Harbor → GRY\nKitsap → KIS\nSkamania → SKM\n\nProducer and field IDs should first be matched to previous participants. New producers and fields should continue the previous sequence. There should be no duplicate producer IDs or sample IDs.\nFor an example R script to automate this process, see assign-sample-ids.R.\n\n\nCreate sample labels\nWe automate sample label creation using R and Microsoft Word’s mail merge tool. labels.R generates a spreadsheet with all column names to be printed on the labels. Then labels-template-mail-merge.docx ingests this spreadsheet and the data scientist runs the mail merge to generate a word document with all of the labels to be printed, as shown in the completed-labels folder.\n\n\nCreate a data tracking sheet\nWe keep a spreadsheet to track which pieces of data we have for each sample ID, including:\n\nGPS points submitted through the ArcGIS Field Maps field form\nScanned paper field forms (for those without ArcGIS Field Maps)\nManagement surveys through ArcGIS Survey123\nScanned chain of custodies with shipping tracking numbers\nLocation of archival falcon tubes\nNotes for if a sample will no longer be sampled, a sample ID was changed, etc.\n\nSee the 2023 spreadsheet for an example.\n\n\nDevelop ArcGIS web tools\nWe use ArcGIS to build tools for managing spatial data and collecting management survey data. A sample selection feature layer is hosted and a web map with offline capabilities is created using ArcGIS Pro. Domains are used for point numbers, bulk density, and crop types. Then, a field form is created on ArcGIS Online using Field Maps. Management surveys are created and hosted with Survey123 and Experience Builder. We also use an ArcGIS Notebook with python to back up our feature layer and survey data.\nThis template ArcGIS Pro project includes a readme.txt that describes this process.\nThe ArcGIS Notebook is set to run as a task Monday, Wednesday, and Friday during the field season.\n\n\nView code from the ArcGIS Notebook\n\nimport arcgis\nfrom arcgis.gis import GIS\nimport datetime as dt\nfrom datetime import timezone, timedelta\ngis = GIS(\"home\")\n\nfolder_path = '/arcgis/home/backups/2023/points'\ntitle = \"2023*\"\nowner = \"jryan_NRAS\"\nitems = gis.content.search(query = \"title:\" + title + \" AND owner:\" + owner,\n                          item_type='Feature Layer')\nprint(str(len(items)) + \" items will be backed up to \" + folder_path +\". See the list below:\")\nitems\n\ndef download_as_fgdb(item_list, backup_location):\n    for item in item_list:\n        try:\n            if 'View Service' in item.typeKeywords:\n                print(item.title + \" is view, not downloading\")\n            else: \n                print(\"Downloading \" + item.title)\n                version = dt.datetime.now(timezone(timedelta(hours=-8))).strftime(\"%Y-%m-%d\")\n                result = item.export(item.title + \"_\" + version, \"File Geodatabase\")\n                result.download(backup_location)\n                result.delete()\n                print(\"Successfully downloaded \" + item.title)\n        except:\n            print(\"An error occurred downloading \" + item.title)\n    print(\"The function has completed\")\n\ndownload_as_fgdb(items, folder_path)\n\nfolder_path = '/arcgis/home/backups/2023/surveys'\ntitle = \"2023 * Survey* Production\"\nowner = \"dgelardi_NRAS\"\nitems = gis.content.search(query = \"title:\" + title + \" AND owner:\" + owner,\n                          item_type='Feature Layer')\nprint(str(len(items)) + \" items will be backed up to \" + folder_path +\". See the list below:\")\nitems\n\ndef download_as_fgdb(item_list, backup_location):\n    for item in item_list:\n        try:\n            if 'View Service' in item.typeKeywords:\n                print(item.title + \" is view, not downloading\")\n            else: \n                print(\"Downloading \" + item.title)\n                version = dt.datetime.now(timezone(timedelta(hours=-8))).strftime(\"%Y-%m-%d\")\n                result = item.export(item.title + \"_\" + version, \"CSV\")\n                result.download(backup_location)\n                result.delete()\n                print(\"Successfully downloaded \" + item.title)\n        except:\n            print(\"An error occurred downloading \" + item.title)\n    print(\"The function has completed\")\n\ndownload_as_fgdb(items, folder_path)"
  },
  {
    "objectID": "flow.html#during-field-season",
    "href": "flow.html#during-field-season",
    "title": "7  Data flow",
    "section": "7.2 During field season",
    "text": "7.2 During field season\nData collection in the field is detailed in the monitoring SOP. We’ll focus on the behind-the scenes tasks for managing data.\n\nUpdate data tracking spreadsheet\nThroughout the season, the data tracking spreadsheet must be updated as various forms and surveys, as described in create a data tracking sheet, are received.\n\n\nModify IDs when samples change\nSometimes a producer can no longer participate, or they want to change which field is sampled. The sample request form should be updated, versioned, and archived (sample-request-form-ferry.xlsx → sample-request-form-ferry_v2.xlsx).\nThe assign-sample-ids.R script should be run again to update the sample IDs. Lines 362 - 386 should be commented out as shown in the highlighted lines of the script on GitHub. Note: This link will take you to a 404 page if you are not logged into a GitHub account that is part of the WSDA organization.\nTake a look through the 01_returned-sample-requests and 02_completed-sample-ids folders for an example of this flow.\nA concise, explanatory note should be added to the data tracking spreadsheet."
  },
  {
    "objectID": "flow.html#post-field-season",
    "href": "flow.html#post-field-season",
    "title": "7  Data flow",
    "section": "7.3 Post field season",
    "text": "7.3 Post field season\n\nOrganize multiple sources of data\nWe have many sources of data such as the initial sample request forms, ArcGIS Field Maps field forms, and management surveys. To organize these multiple sources into a single source of truth, we cross-reference each source and reach out to the sampling teams to resolve conflicting information. This is especially important for verifying the crop that was planted at the time of sampling.\nSee how to mostly automate this process in these 01_load-metadata.R and 02_check-crops.R scripts.\n\n\nProcess lab data\nFollow the QA/QC SOP for processing the lab data.\nSee the 2023 processing scripts and QA/QC report:\n\n03_process-spatial-data.R\n04_load-lab-data.R\n05_calculate-z-scores.R\n2020-2023_qc-results-summary.qmd\n\n\n\nGenerate reports\nUse the {soils} package to create a new project for each year. To avoid email attachment size limitations, reports may be saved to Box.com for distribution to the sampling partners who will send the reports to the participants. Note: this folder is private unless invited by Jadey, Dani, or Perry.\n\n\nImport data to database\nTBD. Currently data are only in spreadsheets.\n\n\nSave data to shared drive and WSU Teams channel\nThe output data files and reports from process lab data and generate reports are copied to the state-of-the-soils folder in its respective year_sampling folder. Review Chapter 4 and look at previous years in the shared drive to follow the same folder structure and organization.\nThe final datasets in wide and long formats should also be saved to the WSU SCBG Soil Health Assessment Teams channel. If you don’t have access to the Data for stats folder, email it to Deirdre Griffin-LaHue.\n\n\nArchive jars and falcon tubes\nArchival subsamples in glass jars are stored in the Yakima WSDA storage room and the cryogenic archive subsamples in falcon tubes are stored in the -80 °C freezer at the WSU Mount Vernon Northwestern Washington Research & Extension Center.\nLabels on the falcon tubes must be taped with a generous amount of packing tape to avoid falling off as they stiffen up and flatten out when they freeze.\nThe archive spreadsheet must be updated."
  },
  {
    "objectID": "share.html#watech-data-categorization",
    "href": "share.html#watech-data-categorization",
    "title": "8  Data sharing",
    "section": "8.1 WaTech data categorization",
    "text": "8.1 WaTech data categorization\nUnder Washington State Policy 141.10 (Securing Information Technology Assets), state agencies must classify data into categories based on the sensitivity of the data. WaTech provides guidance on the four categories of data.\n\nCategory 4: “Confidential information requiring special handling”\nWaTech: Data requires strict handling requirements applied by statues (e.g. HIPAA) or regulations (e.g. rules on employee files).\nSOS: We don’t manage any data under this category.\n\n\nCategory 3: “Confidential information”\nWaTech: Data includes “personal information” as defined in RCW 42.56.590 (Security Breaches) and RCW 19.255.010 (Personal Information Disclosure). An individual’s first name or first initial and last name in combination with at least one of the following elements: social security number, driver’s license or Washington identification card number, or any account numbers that permit access to their financial account.\nSOS: While we do not collect any of the above elements in combination with grower names, we still protect individual and farm names, and latitude and longitude coordinates as confidential information.\n\n\nCategory 2: “Sensitive information”\nWaTech: Data are intended for official use only and withheld unless specifically requested.\nSOS: Lab results and management surveys fall under this category. Access to this data requires a data share agreement.\n\n\nCategory 1: “Public information”\nWaTech: Data is not covered in any of the above categories or is already released to the public.\nSOS: De-identified and aggregated data such as the number of soil samples and from which counties and crops they were collected fall under this category. For example, the SOS dashboard is publicly available and the map zoom is disabled at the 1:1,600,000 scale (counties level)."
  },
  {
    "objectID": "share.html#sec-maintain-confidentiality",
    "href": "share.html#sec-maintain-confidentiality",
    "title": "8  Data sharing",
    "section": "8.2 Maintain confidentiality",
    "text": "8.2 Maintain confidentiality\nOnly under special circumstances and with proper justification in the data share agreement should the following Category 3 data be released to external collaborators. Under no circumstances should these data be made publicly available.\n\nfarm name\ngrower first and last name\nfield names that contain street names or other identifying information\nlatitude and longitude coordinates or other geospatial identifiers\nany information that identifies the individual farm or grower\n\nCategory 2 data should be anonymized and aggregated to honor our data privacy statement by either removing or replacing Category 3 confidential information with dummy data. The {randomNames} R package can be used to replace real names with fake names. Latitude and longitude should be rounded to a precision that does not identify the farm or fields sampled.\nSee the example R script below, copied from the private soils-internal GitHub repo (if you aren’t part of the WSDA GitHub organization, this link will give you a 404 error). Note: this script does not follow the code style guide in Chapter 9 as it was written prior to this DMP.\n\n\nR code to anonymize data\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(randomNames)\n\nload(\"./data/_completeDataset/2020-2023_labResults_wide.Rdata\")\nload(\"./data/_completeDataset/2020-2023_labResults_long.Rdata\")\ncrops &lt;- read_excel(\"./data/reference/crops.xlsx\")\npoints &lt;- read.csv(\"./data/_completeDataset/2020-2023_sampleLocations.csv\") |&gt;\n  select(sampleId, longitude, latitude) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\n# Join lab data with gis data\ndata &lt;- left_join(allResults_wide, points) |&gt;\n  # Remove WSU SCBG samples and 0-6/6-12 in WSDA samples\n  subset(!year %in% c(2020, 2021) &\n           !grepl(\"[A-Z]\", fieldId)) |&gt;\n  janitor::remove_empty(\"cols\")\n\n# Join data with crop dictionary to get crop groups\ndata &lt;- left_join(data, crops, by = c(\"crop\" = \"Crop Type\")) |&gt;\n  select(-crop) |&gt;\n  relocate(\"crop\" = \"Crop Group\", .after = county)\n\n# Anonymize\nanonData &lt;- data |&gt;\n  # Farm name, producer name, sample ID\n  mutate(\n    farmName = paste0(\"Farm \", sprintf(\"%03d\", cur_group_id())),\n    producerName = randomNames(which.names = \"first\",\n                               sample.with.replacement = FALSE),\n    producerId = paste0(\n      paste0(sample(LETTERS, 3, replace = TRUE), collapse = \"\"),\n      \"0\",\n      paste0(sample(1:9, 1, replace = TRUE), collapse = \"\")\n    ),\n    fieldName = paste0(\"Field \", fieldId),\n    sampleId = paste0(\n      substr(year, 3, 4), \"-\", producerId, \"-\", fieldId\n    ),\n    .by = c(county, producerName, producerId)\n  ) |&gt;\n  # county\n  mutate(\n    county = paste0(\"County \", cur_group_id()),\n    .by = c(county)\n  )\n\n# Grab 100 random samples by county\nanon_subset &lt;- anonData |&gt;\n  slice_sample(n = 100)\n\nwrite.csv(anon_subset, file = \"./data/_completeDataset/exampleData.csv\",\n          na = \"\",\n          row.names = FALSE)"
  },
  {
    "objectID": "share.html#data-share-agreement",
    "href": "share.html#data-share-agreement",
    "title": "8  Data sharing",
    "section": "8.3 Data share agreement",
    "text": "8.3 Data share agreement\nSOS CoPIs created a Data Sharing and Scope of Work Agreement that details the type of data to be shared, the scope of work in which the data may be used, and terms for using SOS data.\nOnce the agreement has been signed by both CoPIs and the “Partnering Scientists”, the agreement should be saved in its own folder within Y:/NRAS/soil-health-initiative/state-of-the-soils/data-sharing. If this agreement is also part of a grant proposal, it should also be saved in its corresponding grant folder Y:/NRAS/soil-health-initiative/contracts-grants/grants.\nThe request correspondence, code to subset the requested data, and the final dataset sent should all be saved in this folder. Internally documenting this information allows us to track publications and attributions resulting from this data sharing. This documentation also helps us track the broader impact of how our collaborators’ use the SOS dataset."
  },
  {
    "objectID": "share.html#public-access",
    "href": "share.html#public-access",
    "title": "8  Data sharing",
    "section": "8.4 Public access",
    "text": "8.4 Public access\nCurrently the only SOS data publicly available are the counts of samples across the project, counties, and crop types displayed in the ArcGIS Online Dashboard.\nA small, anonymized subset is included as example data in the {washi} and {soils} R packages for demonstration purposes.\nTODO: discussion with team on the below ideas\nIn the future when the data are more mature and hosted in a proper database, we may publish an anonymized subset in a public repository such as:\n\nGitHub via an R package or Shiny app\nZenodo: integrates with GitHub and is citable with a DOI\nData.WA.gov: open data portal for the State of Washington\n\nMore inspiration and ideas for enhancing data discoverability and sharing across the agricultural and soil health communities include:\n\nUSDA LTAR data dashboards\nCAF LTAR metadata tool\n\nChapter 16 of Data Management in Large-Scale Education Research discusses more considerations for data sharing and choosing public repositories (Lewis 2023)."
  },
  {
    "objectID": "share.html#acknowledgments",
    "href": "share.html#acknowledgments",
    "title": "8  Data sharing",
    "section": "8.5 Acknowledgments",
    "text": "8.5 Acknowledgments\nAll research and data partially or completely funded by WaSHI must include acknowledgements to the State of Washington. The following text should be included in all publications resulting from this funding:\n\nData were in part provided by the Washington Soil Initiative, which is supported by the State of Washington and administered by the Washington State Department of Agriculture, Washington State Conservation Commission, and Washington State University.\n\nIf WaSHI staff make substantial scientific contributions to the manuscript, discuss the possibility of co-authorship credit.\n\n\n\n\nEuropean Commission. 2016. “H2020 Programme Guidelines on FAIR Data Management in Horizon 2020.” https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nWhyte, Angus, and Graham Pryor. 2011. “Open Science in Practice: Researcher Perspectives and Participation.” International Journal of Digital Curation 6 (March): 199–213. https://doi.org/10.2218/ijdc.v6i1.182."
  },
  {
    "objectID": "code-guide.html#projects",
    "href": "code-guide.html#projects",
    "title": "9  Code style guide",
    "section": "9.1 Projects",
    "text": "9.1 Projects\nAll files associated with a given project (input data, R scripts, analytical results, figures, reports) should be kept together in one directory. RStudio has built-in support for this through projects. R projects bundle all the work in a portable, self-contained folder that can be moved around on your computer or on to other collaborators’ computers and still work.\nLearn more about projects in the Workflow: scripts and projects chapter of R4DS, in Jenny Bryan’s article Project-oriented workflow, and Shannon Pileggi’s workshop slides.\nTo create a project in RStudio, click File &gt; New Project, then follow the steps in the below figure.\n\n\n\nFigure from Chapter 6 of R4DS\n\n\nThe project folder should be checked into GitHub for version control as discussed in Section 5.3.\nIf not checked into GitHub, at bare minimum, the folder should be copied onto the shared drive.\n\nProject folder structure\nHaving a consistent and logical folder structure will make it easier for you (especially future you) and collaborators to make sense of the files and work you’ve done. Well documented projects also make it easier to resume a project after some time away with minimal frustration of having to remember where everything is, what you did, and why you did it.\nThe below structure works most of the time and should be used as a starting point. However, different projects have different needs, so add and remove subfolders as needed.\n\nroot: top-level project folder containing the .Rproj file.\ndata: contains raw and processed data files in subfolders. Raw data should be made read-only and not changed in any way. See Section 5.2 for a reminder on how to make a file read-only.\noutput: outputs from R scripts such as figures or tables.\nR: all R scripts containing data processing or function definitions.\nreports: Quarto or RMarkdown files are saved here, as well as the resulting reports.\nREADME: a markdown file (can be generated from Quarto or RMarkdown) to explain the project.\n\n\n\nSee an example project folder structure\n\n├── project-demo.Rproj\n├── data\n│   ├── processed\n│   │   └── data-clean.csv\n│   └── raw\n│       └── data-raw.xlsx\n├── output\n│   ├── fig-01.png\n│   ├── fig-02.png\n│   ├── tbl-01.png\n│   └── tbl-02.png\n├── R\n│   ├── 01_import.R\n│   ├── 02_tidy.R\n│   ├── 03_transform.R\n│   ├── 04_visualize.R\n│   └── custom-functions.R\n└── reports\n│   ├── soil-health-report.pdf\n│   └── soil-health-report.qmd\n│   └── images\n│       └── logo.png\n├── README.md\n└── README.qmd\n\nR packages may contain these additional subfolders and files:\n\ninst: any arbitrary additional files to be included with package installation such as CITATION, fonts, and Quarto templates.\nman: all .Rd (“R documentation”) files for each function that are generated from {roxygen2}.\nvignettes: long-form guides that go beyond function documentation and can be used as tutorials to demonstrate a workflow using the package to solve a particular problem.\ntests: all test files, usually using {testthat}.\npkgdown and docs: if using {pkgdown} to build a website for the package, you may have a pkgdown folder containing the favicon and any additional css and a docs folder containing the website source files.\nDESCRIPTION: file containing metadata about the package (authors, current version, dependencies).\nLICENSE: file describing the package usage agreement.\nNAMESPACE: file generated by {roxygen2} listing functions imported from other packages.\nNEWS.md: file documenting user-facing changes to a package.\n\nLearn more about other R package components in R Packages (2e).\n\n\nAbsolute vs relative paths\nNote: directories are synonymous with folders.\n❌Absolute paths start with the root directory and provide the full path to a specific file or folder like C:\\\\Users\\\\jryan\\\\Documents\\\\R\\\\projects\\\\project-demo\\\\data\\\\processed.1 You can run getwd() to find out where the current working directory is and setwd() to set a specific folder as your working directory. However, please don’t do use setwd() because this absolute file path is going to break your code if you reorganize your folders and is not going to work on any collaborators’ computers as their directory configuration will be different.\n\n\n\nA cartoon of a cracked glass cube looking frustrated with casts on its arm and leg, with bandaids on it, containing “setwd”, looks on at a metal riveted cube labeled “R Proj” holding a skateboard looking sympathetic, and a smaller cube with a helmet on labeled “here” doing a trick on a skateboard. Artwork by @allison_horst.\n\n\n✅Instead, always use relative paths in your scripts. Relative paths are relative to the working directory (i.e. the project’s home) like data/processed/data-clean.csv. When working in a RStudio project, the default working directory is always the root project directory (where the .Rproj file is).\nFor example, say 01_import.R contains the code read.csv(\"data/processed/data-clean.csv\"). This will read the file from C:\\\\Users\\\\jryan\\\\Documents\\\\R\\\\projects\\\\project-demo\\\\data\\\\processed\\\\data-clean.csv\". The magic of relative paths means that if Dani were have this project on her desktop and run this code, it would read the file from C:\\\\Users\\\\dgelardi\\\\Desktop\\\\project-demo\\\\data\\\\processed\\\\data-clean.csv. This is why relative paths are so important – they work no matter where the project folder is!\n\n\n{here} package\nIn combination with R projects, use the {here} package to build relative file paths. This is especially important in Quarto files because when the .qmd file renders, its default current working directory is wherever the .qmd file lives. If we are using the above example project structure and wanted to read in our clean data to our soil-health-report.qmd file, we would get an error running read.csv(\"data/processed/data-clean.csv\") because it would be looking for a data subfolder in the reports folder. Instead, we can use the {here} package to build a relative path from our root with read.csv(here::here(\"data\", \"processed\", \"data-clean.csv\")). {here} takes care of the backslashes or forward slashes so the relative path will work no matter the operating system.\n\n\n\nA cartoon showing two paths side-by-side. On the left is a scary spooky forest, with spiderwebs and gnarled trees, with file paths written on the branches like “~/mmm/nope.csv” and “setwd(”/haha/good/luck/“), with a scared looking cute fuzzy monster running out of it. On the right is a bright, colorful path with flowers, rainbow and sunshine, with signs saying”here!” and “it’s all right here!” A monster facing away from us in a backpack and walking stick is looking toward the right path. Stylized text reads “here: find your path.” Learn more about here. Artwork by @allison_horst."
  },
  {
    "objectID": "code-guide.html#naming-conventions",
    "href": "code-guide.html#naming-conventions",
    "title": "9  Code style guide",
    "section": "9.2 Naming conventions",
    "text": "9.2 Naming conventions\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\n— Phil Karlton\n\nBased on this quote, Indrajeet Patil developed slides with a lot of detailed language-agnostic advice on naming things in computer science.\nR code specific naming conventions are listed below. They mostly follow the best practices from Section 3.2 in the naming conventions chapter, with one exception as described in variables in a dataset.\nTODO: add Python naming conventions. They differ from R as static variables (aka constants) are supposed to be SCREAMING_SNAKE_CASE.\n\nProject folder, .RProj and GitHub repository\nThe project folder, .RProj file, and GitHub repository name should be the same. Be concise and descriptive. Use kebab-case.\nExample: washi-dmp and washi-dmp.RProj.\n\n\nFiles\nBe concise and descriptive. Avoid using special characters. Use kebab-case with underscores to separate different metadata groups (date_good-name).\nExamples: 2024_producer-report.qmd, tables.R, create-soils.R.\nIf files should be run in a particular order, prefix them with numbers. Left pad with zero if there may be more than 10 files.\nExample:\n01_import.R\n02_tidy.R\n03_transform.R\n04_visualize.R\n\n\nVariables, objects, and functions\nLet’s first define these terms so we’re on the same page. In this style guide, variables are essentially column names, objects are all data structures in R and ArcGIS (vectors, lists, data frames, fields, tables), and functions are self-contained modules of code that accomplish a specific task.\n\nVariables in a dataset\nThis is where we differ from the tidyverse style guide. Hadley recommends using only lowercase letters, numbers, and _ for all variables (column names and R objects). However, we are defining variables as only column names. For column names, we use UpperCamelCase and _ to separate the measurement name from the unit. Do not use special characters (i.e., write out Percent instead of using %).\nThis helps us mutate and pivot our data using the _:\nresults_long &lt;- data |&gt;\n  dplyr::mutate(dplyr::across(dplyr::contains(\"_\"), as.numeric)) |&gt;\n  tidyr::pivot_longer(\n    cols = dplyr::matches(\"_|pH\"),\n    names_to = \"measurement\"\n  ) \nVariable examples:\n# Good\nClay_Percent\n96HrMinC_MgCKgDay\nPMN_MgKg\n\n# Bad\n\n# Uses special character\nClay_%\n\n# Less human readable, inconsistent with style guide\n96hrminc_mgkgday\n\n# Hyphen will need to be escaped in R code to avoid error\nPMN-MgKg\n\n\nObjects and functions\nObjects names should be nouns, while function names should be verbs (Wickham 2022). Again, use lowercase letters, numbers, and underscores. Do not put a number as the first character of the name. Do not use hyphens. Do not use names of common functions or variables.\nObject examples:\n# Good\nprimary_color\ndata_2023\n\n# Bad\n\n# Less human readable, inconsistent with style guide\nprimarycolor\n\n# Using a hyphen in an object name causes error\ndata-2023 &lt;- read.csv(\"2023_data-clean.csv\")\nError in data - 2023 &lt;- read.csv(\"2023_data-clean.csv\") : could not find function \"-&lt;-\"\n  \n# Starting an object name with a number also causes error\n2023_data &lt;- read.csv(\"2023_data-clean.csv\")\nError: unexpected input in \"2023_\"\n\n# Overwrites R shortcut for TRUE\nT &lt;- FALSE\n\n# Overwrites R function\nc &lt;- 10\nFunction examples:\n# Good\nadd_row()\nassign_quality_codes()\n\n# Bad\n\n# Uses noun instead of verb\nrow_adder() \n\n# Inconsistent with style guide\nassignQualityCodes()\n\n# Overwrites common base R function\nmean()"
  },
  {
    "objectID": "code-guide.html#r-scripts",
    "href": "code-guide.html#r-scripts",
    "title": "9  Code style guide",
    "section": "9.3 R scripts",
    "text": "9.3 R scripts\n\nHeader template\nIncluding a header in every R script helps standardize the metadata elements provided at the beginning of your code and documents its purpose. Dr. Timothy S Farewell wrote a great blog post for creating a template for the header of every R script. The following template and instructions are adapted from his post (Farewell 2018).\n\nScript name: meaningful and concise.\nPurpose: brief description of what the script aims to accomplish.\nAuthor(s) and email: it’s good to know where the script originated from if there are any questions, comments, or improvements.\nDate created: this is automatically filled in from the template.\nNotes: free-text space for any thoughts or todos.\n\n## Header ======================================================================\n## Script name: check-crops.R\n##\n## Purpose: Cross reference sample requests, Field Maps forms, and management \n## surveys to get the correct crop planted at the time of sampling.\n##\n## Author: Jadey Ryan \n##\n## Email: jryan@agr.wa.gov\n##\n## Date created: 2024-01-02\n##\n## Notes:\n##   \n    \n# Attach packages ==============================================================\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load data ====================================================================\nAdd this template to RStudio using snippets:\n\nModify the below code with your name and preferred packages.\nIn RStudio, go to Tools &gt; Edit Code Snippets.\nScroll to the bottom of the R code snippets, and paste your modified code (the indent and tabs are important!).\nClick Save and close the window.\nTry it out by opening a new blank .R script, typing “header”, and then pressing Shift + Tab.\n\nsnippet header\n    ## Header ======================================================================\n    ##\n    ## Script name: \n    ##\n    ## Purpose: \n    ##\n    ## Author: Jadey Ryan \n    ##\n    ## Email: jryan@agr.wa.gov\n    ##\n    ## Date created: `r paste(Sys.Date())`\n    ##\n    ## Notes:\n    ##   \n    \n    # Attach packages ==============================================================\n\n    library(readxl)\n    library(writexl)\n    library(janitor)\n    library(dplyr)\n    library(tidyr)\n    \n    # Load data ====================================================================\n\n\nSection template\nThe above header template also uses section breaks (commented lines with = that break up the script into easily readable chunks). Section breaks are fantastic tools in RStudio because they allow you to easily show or hide blocks of code, see an outline of your script, and navigate through the source file. Read more about code folding and sections in this Posit article.\nThe snippet to create this section template that fills in the rest of the line with = was adapted from this stack overflow answer.\nsnippet end\n    `r strrep(\"=\", 84 - rstudioapi::primary_selection(rstudioapi::getActiveDocumentContext())$range$start[2])`\nAfter adding the above code to your snippets, try creating a new section by typing “# Tidy data end” then pressing Shift + Tab.\n# Tidy data end&lt;Shift+Tab&gt; results in:\n# Tidy data ===================================================================="
  },
  {
    "objectID": "code-guide.html#code-styling",
    "href": "code-guide.html#code-styling",
    "title": "9  Code style guide",
    "section": "9.4 Code styling",
    "text": "9.4 Code styling\nReview the Syntax chapter of the tidyverse style guide for a lengthy section that covers spacing, function calls, long lines, semicolons, assignments, comments, etc. Also skim through Chapter 4 Workflow: code style in R4DS, which highlights the opinionated “most important parts of the tidyverse style guide”. Instead of rewriting all of these details and conventions into this style guide and making us all memorize the content, we should all just use the {styler} R Package (as advised in R4DS Chapter 4).\n{styler} is a package and RStudio Addin that formats code for you, so we can keep our coding style consistent across projects and better facilitate collaboration. We’ll deviate slightly from the tidyverse style and instead use {grkstyle}. {grkstyle} is an extension package for {styler} that Garrick Aden-Buie developed based on the tidyverse style guide. I prefer {grkstyle} over the tidyverse style that {styler} defaults to mainly because of how it handles line breaks in function calls.\nThe below example and installation instructions are pretty much copied directly from Garrick’s {grkstyle} README.\n\nExamples\n\ngrkstyle\ndo_something_very_complicated(\n    something = \"that\",\n    requires = many,\n    arguments = \"some of which may be long\"\n) \n\n\nstyler::tidyverse_style\ndo_something_very_complicated(\n  something = \"that\", requires = many,\n  arguments = \"some of which may be long\"\n) \n\n\n\nInstallation\nInstall {styler} and {grkstyle} with:\ninstall.packages(\"styler\")\n\noptions(repos = c(\n    gadenbuie = \"https://gadenbuie.r-universe.dev\",\n    getOption(\"repos\")\n))\n\n# Download and install grkstyle in R\ninstall.packages(\"grkstyle\")\nSet grkstyle as the default in {styler} functions and addins by running\n# Set default code style for {styler} functions\ngrkstyle::use_grk_style()\nor adding the following to your ~/.Rprofile\noptions(styler.addins_style_transformer = \"grkstyle::grk_style_transformer()\")\nTo edit your .Rprofile, you can use usethis::edit_r_profile() to open the file in your RStudio. You may need to install the {usethis} package.\n\n\nUsage\nOnce {styler} and {grkstyle} are installed, you can apply the style to your .R, .qmd, and .Rmd files using the command palette, keyboard shortcut, or addins menu.\n\nCommand palette\nUse RStudio’s command palette to quickly and easily access any RStudio command and see keyboard shortcuts. Open the command palette with Cmd/Ctrl + Shift + P, then type “styler” to see the shortcuts offered by {styler}.\n\n\nKeyboard shortcuts\nI often use Cmd/Ctrl + Shift + A to style the entire active file every time I finish a code block or section. To style just a selection, use Cmd/Ctrl + Alt + Shift + A.\n\n\nAddins menu\nYou can also use the Addins menu in RStudio to style your files by clicking on a button to run the command.\n\n\n\n\n\nFarewell, Dr Timothy S. 2018. “My Easy r Script Header Template  Tim Farewell.” https://timfarewell.co.uk/my-r-script-header-template/.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/index.html."
  },
  {
    "objectID": "code-guide.html#footnotes",
    "href": "code-guide.html#footnotes",
    "title": "9  Code style guide",
    "section": "",
    "text": "Note the two backslashes. Windows paths use backslashes, which mean something specific in R. to get a single backslash in the path, we need to type two backslashes (or use forward slashes).↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the\nR.J. Cook Agronomy Farm Long-Term Agroecological Research\nSite.”\n\n\nEuropean Commission. 2016. “H2020 Programme Guidelines on FAIR\nData Management in Horizon 2020.” https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf.\n\n\nFarewell, Dr Timothy S. 2018. “My Easy r Script Header Template\n Tim Farewell.” https://timfarewell.co.uk/my-r-script-header-template/.\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education\nResearch [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life\nCycle.” https://www.fws.gov/data/life-cycle.\n\n\nWhyte, Angus, and Graham Pryor. 2011. “Open Science in Practice:\nResearcher Perspectives and Participation.” International\nJournal of Digital Curation 6 (March): 199–213. https://doi.org/10.2218/ijdc.v6i1.182.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific Data Management\nand Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "1 Overview\nThe Washington Soil Health Initiative (WaSHI) is a partnership between the Washington State Department of Agriculture (WSDA), Washington State University (WSU), and the State Conservation Commission. WaSHI establishes a coordinated approach to healthy soil in Washington.\nTo date, nearly 1,000 soil samples and management surveys across 50 different cropping systems have been collected as a part of the state of the soils assessment (SoSA). WSDA and WSU lead this project with support from staff, students, conservation districts, and agricultural professionals throughout Washington."
  },
  {
    "objectID": "index.html#chapter-outline",
    "href": "index.html#chapter-outline",
    "title": "Data Management Plan",
    "section": "1.1 Chapter outline",
    "text": "1.1 Chapter outline\nThis Data Management Plan (DMP) is a living document will be continually reviewed and improved based on lessons learned, new information, and collaborator feedback.\n\n\n\n\n\n\nLinks to shared drive folders and files\n\n\n\nThis DMP is best viewed in Google Chrome since the browser can open local file links using the Enable local file links extension that should automatically be enabled by our organization.\nWhen viewing this DMP in Microsoft Edge, hyperlinks to files and folders on our shared drive are not accessible in the browser. Nothing will happen when clicking on the links. To open the file or folder, right-click on the hyperlink &gt; copy the path &gt; paste it into the search bar of the file explorer &gt; press Enter or click the arrow."
  },
  {
    "objectID": "index.html#roles-and-responsibilities",
    "href": "index.html#roles-and-responsibilities",
    "title": "Data Management Plan",
    "section": "1.2 Roles and responsibilities",
    "text": "1.2 Roles and responsibilities\nAll WaSHI personnel who will be interacting with SoSA data must familiarize themselves with the contents of this document. Following chapters with technical details will be referenced when relevant. If all collaborators are not consistently implementing this DMP, then the benefits of effective data management are lost.\nThe WSDA Data Scientist, supported by the project Principal Investigators (PIs), is responsible for providing guidance to WaSHI staff working with SoSA data and ensuring the implementation of the DMP. The Data Scientist is also responsible for reviewing and updating this document annually, and as needed. Upon updates, the Data Scientist will distribute this document to WaSHI staff and commit the source code to the GitHub repository.\n\nCurrent roles as of November 2023\n\n\nRole\nAffiliation\nName\n\n\n\n\nCoPI\nWSDA\nDani Gelardi\n\n\nCoPI\nWSU\nDeirdre Griffin LaHue\n\n\nData Scientist\nWSDA\nJadey Ryan\n\n\nData Stewards\nWaSHI personnel"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Management Plan",
    "section": "1.3 Acknowledgements",
    "text": "1.3 Acknowledgements\nThis DMP was adapted from the R.J. Cook Agronomy Farm Long-term Agroecological Research Site DMP (Carlson 2021), U.S. Fish and Wildlife Service data management life cycle (U.S. Fish & Wildlife Service 2023), Harvard Medical School Longwood Research Data Management DMP guidelines (Harvard Medical School 2023), and the Data Management in Large-Scale Education Research book (Lewis 2023).\n\n\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the R.J. Cook Agronomy Farm Long-Term Agroecological Research Site.”\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle."
  },
  {
    "objectID": "data-management.html#data-life-cycle",
    "href": "data-management.html#data-life-cycle",
    "title": "2  What is data management?",
    "section": "2.1 Data life cycle",
    "text": "2.1 Data life cycle\n\n\nThe U.S. Fish and Wildlife Service developed a great graphic to explain the elements of the data life cycle and emphasize the importance of data quality at every step (U.S. Fish & Wildlife Service 2023). Each step within the data life cycle requires careful intention to ensure transparency, quality, and integrity.\n\n\n\n\n\n\n\n\n\n\nOur adaptation of this data life cycle is outlined below.\n\n\n\n\n\n\n\n\nPlan\nEach sampling year presents an opportunity to consider what worked and what could be improved from the previous year. Planning involves making decisions about data acquisition, management, and quality control. For example, each year we provide a spreadsheet template with our requested column headers to Soiltest lab to ensure the measurements are reported with correct units and in the format we use. Special projects that deviate from our standard operating procedures require additional planning.\n\n\n\nAcquire\nWe acquire data by collecting and analyzing new samples, deriving new insights from existing data, or accepting datasets from collaborators. |\n\n\n\nMaintain\nMaintenance involves processing data for aggregation, analyses, and reporting. We create metadata that facilitates interpretation of the data and ensure the data are in a non-proprietary format that is accessible to our collaborators and future selves.\n\n\n\nAccess\nAccess refers to data storage, publication, and security. Raw and processed data with accompanying metadata should be stored, backed up, and available for information sharing with our partners. With PI approval, anonymized and aggregated data that does not compromise growers’ personally identifiable information (PII) should be made publicly available in a data repository or data product/decision-support tool.\n\n\n\nEvaluate\nWe evaluate data while processing and analyzing it to maximize accuracy and productivity, while minimizing costs associated with errors or tedious data cleaning labor. Evaluation workflows should be efficient, well-documented, and reproducible. Our evaluated data help us better understand how factors and management decisions impact soil health.\n\n\n\nArchive\nProperly archiving our results supports the long-term storage and usefulness of our data. While similar to the Access element of the life cycle, archiving focuses on preserving data for long-term/historical retention that aren’t needed for immediate access. For example, we archive each year’s raw data for long-term storage and set those files to Read-Only.\n\n\n\nQuality Assurance / Quality Control (QA/QC)\nData quality management prevents data defects that hinder our ability to apply data towards our science-based conservation efforts. Defects include incorrectly entered data, invalid data, and missing or lost data. QA/QC processes should be incorporated in every element of the data life cycle.\n\n\n\nThe following chapters describe our internal processes and standards to follow throughout each step in the data life cycle.\n\n\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "formats-standards.html#data-formats",
    "href": "formats-standards.html#data-formats",
    "title": "3  Formats & standards",
    "section": "3.1 Data formats",
    "text": "3.1 Data formats\nData generated from or integrated into WaSHI can be non-digital or digital.\n\nNon-digital data\nNon-digital data, such as field forms, management surveys, and chain of custody forms, are manually recorded on paper forms. Paper forms must be transcribed or converted to digital file formats and then stored in the WaSHI filing cabinet in the Natural Resources Building in Olympia.\n\n\nDigital data\nDigital data include tabular, spatial, and binary data, such as lab results, sample locations, and field photos. Non-conventional data also include code, algorithms, tools, and workflows.\nTabular data include comma separated values (csv), tab separated values (tsv), Microsoft Excel open XML spreadsheet (xlsx), and portable document format (pdf).\nSpatial data include file geodatabases (gdb), vector shapefiles (zipped folder containing multiple file extensions), keyhole markup language (kml or kmz). Tabular data may also contain spatial data as longitude and latitude.\nBinary data include photos (jpeg, png, gif, tiff), videos (mp4), code (R, py, js), and object-oriented data files (RDS, Rdata, parquet, arrow).\nProprietary data formats include Microsoft Excel, Word, and Powerpoint files (xlsx, docx, pptx). RDS and Rdata files are an example of an application-specific data format that can only be opened using the R programming language or RStudio IDE. These types of files should be saved in conjunction with a copy of the data in a non-proprietary and open-standard format, such as csv, to maintain accessibility for those who do not have Microsoft Office or do not use R.\nWritten documents and presentations are in formats including Microsoft Word and Powerpoint (docx and pptx), hypertext markup language (HTML), and pdf.\nNotebooks combine text with executable code to generate written documents and presentations in docx, pptx, HTML, or pdf formats. These notebooks are stored in formats depending on the programming language: a few examples include R markdown (rmd), Quarto (qmd), and Jupyter notebook (ipynb).\nThe list below is not exhaustive and will continue to grow as additional useful data sources are discovered.\n\n\n\n\nType\nSource\nFormats\n\n\n\n\nLab results\nProvided by the lab analyzing the soil sample, principal investigator of a study, or grower\ncsv, xlsx, pdf, xml, json, RDS, RData\n\n\nManagement surveys\nCollected through interviews with grower\ncsv, xlsx, RDS, RData, paper form (to be digitized)\n\n\nField forms\nCompleted in the field during/immediately after sampling\npdf, paper form (to be digitized), csv, xlsx\n\n\nSample locations\nIdentified prior to sampling and may be edited during sampling using ArcGIS Online, Collector, Field Maps or Google Maps\nArcGIS feature layer, shp, kmz, csv, xlsx\n\n\nChain of custody forms\nCompleted prior to shipping or dropping off samples\npdf, paper form (to be digitized)\n\n\nClimate data\nOSU PRISM, NOAA, Esri Living Atlas\ncsv, shp, netCDF, tiff, gdb\n\n\nSoil data\nNRCS Web Soil Survey, NRCS WA gSSURGO\ngdb, accdb\n\n\nStrata classification\nProvided by Soil Health Institute in 2021 as a lyr file\nlyr\n\n\nImages\nLogos, icons, photos taken in the field\njpeg, png, gif, tiff, svg\n\n\nVideos\nRecordings of meetings, training videos\nmp4\n\n\nDocuments\nReports, manuscripts, SOP, QAPP, factsheets, brochures\ndocx, txt, html, pdf\n\n\nPresentations\nPowerpoints, slide decks\npptx, html, pdf\n\n\nCode\nScripts for wrangling, processing, analyzing data; markdown for producing documents and presentations; style sheets for html outputs\nR, py, ipynb, js, yml, rmd, qmd, css, scss"
  },
  {
    "objectID": "formats-standards.html#sec-data-standards",
    "href": "formats-standards.html#sec-data-standards",
    "title": "3  Formats & standards",
    "section": "3.2 Data standards",
    "text": "3.2 Data standards\nDate will be expressed as YYYY-MM-DD according to the ISO 8601 standard.\nDate with time will be expressed as YYYY-MM-DDTHH:MM:SSZ.\n\nT separates date from time. The Z indicates the date-time is using the Universal Time Coordinated (UTC) with no offset.\nPacific Standard Time (PST) has a UTC-8:00 offset and Pacific Daylight Time (PDT) has a UTC-7:00 offset and would be expressed as YYYY-MM-DDTHH:MM:SS-8:00. The Z has been replaced with the offset.\nExample: 2023-11-28T14:55:56-08:00.\n\n\n\n\n\"ISO 8601\" from Randall Munroe's xkcd\n\n\nGeospatial data will be accompanied by metadata that abides by the ISO 19115 standard by following Esri’s documentation when working in ArcGIS Pro. Metadata contains information about the identification, the extent, the quality, the spatial and temporal schema, spatial reference, and distribution of digital geographic data.\nCode will follow the style guide in Chapter 10."
  },
  {
    "objectID": "naming.html#why-are-conventions-important",
    "href": "naming.html#why-are-conventions-important",
    "title": "4  Naming conventions",
    "section": "4.1 Why are conventions important?",
    "text": "4.1 Why are conventions important?\n\nImproves consistency and predictability, making it easier to browse folders and know what the file/folder contains.\nEasily sort and retrieve files by date, conservation district, or other theme.\nFacilitates collaboration so all team members can find the information they need.\nStandardizes file paths and URLs for efficient programming and website hosting.\n\nURLs and programming languages are case-sensitive. WaSHI-data.csv and washi-data.csv are completely different files.\nURLs cannot have spaces in them. They must be escaped with this character entity %20. wasoilhealth.org/producer spotlights would need to be wasoilhealth.org/producer%20spotlights.\n\n\nFor more web-specific naming conventions, see this Learn the Web webpage."
  },
  {
    "objectID": "naming.html#sec-naming-best-practices",
    "href": "naming.html#sec-naming-best-practices",
    "title": "4  Naming conventions",
    "section": "4.2 Best practices",
    "text": "4.2 Best practices\nMany files and folders in our shared drive do not follow the above best practices or below naming conventions. We are learning and improving as we go. Going forward, please follow the conventions listed below.\nThese are guidelines and naming things is hard, so try your best. If you’re not sure how to name a folder or file, talk with Jadey. If you’re adding a bunch of files that came from somewhere else, ask Jadey to help you organize and bulk rename them.\nSee Section 4.3 for a table of examples of folder and file names using these best practices.\n\nLowercase only\nDifferent conventions work better for different purposes (folders and file versus programming objects).\n\nkebab-case: all lowercase with hyphens separating words. Use for all folder and file names.\nsnake_case: all lowercase with underscores separating words. Only use for functions and variables in R. See Section 10.2.3.2 for example R errors when including hyphens in object names.\n\n\n\n\nCartoon representations of common cases in coding. A snake screams “SCREAMING_SNAKE_CASE” into the face of a camel (wearing ear muffs) with “camelCase” written along its back. Vegetables on a skewer spell out “kebab-case” (words on a skewer). A mellow, happy looking snake has text “snake_case” along it. Artwork by @allison_horst.\n\n\n\n\nDelimiters convey meaning\nDeliberately use underscores and hyphens so we can easily understand the contents and programmatically parse file and folder names.\n\nUse underscores to delineate metadata elements (i.e. date from name from version date_name_version).\nUse hyphens to separate parts of one metadata element (i.e. date YYYY-MM-DD or name wsda-washi-presentation).\n\n\n\nNo spaces or special characters\nAvoid spaces and special characters (only use underscores and hyphens). Characters like / () ! ? % + \" ' have special meaning to computers and can break file paths and URLs.\n\n\nCharacter length matters\nComputers are unable to read file paths and file names that surpass a certain character length. Be concise AND descriptive. Windows path limit is 260 characters.\n\n\n‘Back to front’ date\nRemember to express date ‘back to front’ of YYYY-MM-DD according to the ISO 8601 standard. This maintains the chronological order of records when they are sorted alphanumerically.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\n2020-05-28_agenda.pdf\n2-14-2023_Agenda.pdf\n\n\n2023-01-01_agenda.pdf\n2023-Jan-1_Agenda.pdf\n\n\n2023-02-14_agenda.pdf\nDec052020_Agenda.pdf\n\n\n2023-12-05_agenda.pdf\nMay_28_2020_Agenda.pdf\n\n\n\n\n\nGroup & sort files by name\nConsider how folders and files should be grouped and sorted. Put that piece of metadata in the beginning of the file name.\n\n\n\n\n\n\n\nSort by district\nSort by date\n\n\n\n\ncowlitz-coc_2023-05-01.pdf\n2023-05-01_cowlitz-coc.pdf\n\n\ncowlitz-coc_2023-05-23.pdf\n2023-05-01_cowlitz-tracking.pdf\n\n\ncowlitz-tracking_2023-05-01.pdf\n2023-05-09_ferry-cd-tracking.pdf\n\n\nferry-cd-coc_2023-05-10.pdf\n2023-05-10_ferry-cd-coc.pdf\n\n\nferry-cd-coc_2023-05-17.pdf\n2023-05-17_ferry-cd-coc.pdf\n\n\nferry-cd-coc_2023-06-06.pdf\n2023-05-23_cowlitz-coc.pdf\n\n\nferry-cd-tracking_2023-05-09.pdf\n2023-06-06_ferry-cd-coc.pdf\n\n\n\n\n\nVersion numbers\nIf not using the date to version, or to keep multiple drafts from the same date, add version information to the end of the file name. Think about how many possible versions there could be. If there may be more than 10, use leading zeros before single digit numbers so the file name always has the same length. V1 through V15 will not sort the same way as V01 through V15.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\nsop_v01.pdf\nSOP_V1.pdf\n\n\nsop_v02.pdf\nSOP_V10.pdf\n\n\n… [v03 - v09]\nSOP_V11.pdf\n\n\nsop_v10.pdf\nSOP_V2.pdf\n\n\nsop_v11.pdf\n… [V3 - V9]\n\n\n\n\n\nCollaboration\nAdd your initials at the end of the file name when “saving as” a file that multiple people are working on (i.e., 2023-11-14_sop-soil-health-monitoring_lm_jr.docx). This ensures a version is kept as a backup. Alternatively, use Track Changes if working in a MS Word document.\n\n\nCode and variables\nInstead of kebab-case, we use snake_case for all code objects (R vectors, lists, dataframes, functions, etc.) and variables (column names). Hyphens in object names can cause errors in R and SQL code.\nSee Chapter 10 for more details in the code style guide."
  },
  {
    "objectID": "naming.html#sec-naming-examples",
    "href": "naming.html#sec-naming-examples",
    "title": "4  Naming conventions",
    "section": "4.3 Naming examples",
    "text": "4.3 Naming examples\n\n\n\n\n\n\n\n\n\n\nNaming convention\nExamples\n\n\n\n\nFolders\nkebab-case\n2024_sampling\ndata-management\n\n\nFiles\nkebab-case\n2023-11-15_survey-perennial.xlsx\nrecords-management_v01.docx\npend-oreille-cd_coc_2023-06-05.pdf\nwashi-logo-color.png\nwashi-dmp.Rproj\n01_load-metadata.R\n2024_producer-report.qmd\n\n\nCode and variables\nsnake-case\nsample_id\ntoc_percent\npmn_mg_per_kg\ncrop_summary\nassign_quality_codes()"
  },
  {
    "objectID": "organization.html#folder-structure",
    "href": "organization.html#folder-structure",
    "title": "5  Organization",
    "section": "5.1 Folder Structure",
    "text": "5.1 Folder Structure\nWe strive for a balance between a deep and shallow structure. If too shallow, there are too many files in one folder and they are hard to sort through. If too deep, we have to click too many times to get to a file and specific files can be difficult to find.\nY:\\NRAS\\Soil_Health_Initiative is the parent folder for all WaSHI content.\nWithin the StateOfTheSoils sub-folder, we use a combination of date- (each year has its own sub-folder) and categorical- based (dataset and documentation that span across years) folder structures.\nY:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\n├── _completeDataset\n├── 2019_SCBG\n├── 2021_Sampling\n├── 2022_PartnershipsInSoilHealth\n├── 2023_Sampling\n├── 2024_Sampling\n├── DataManagement\n├── DataSharing\n├── DataSources\n├── Maps\n├── Projects\n├── QAPP\n├── SOPs\n├── TrainingVideos\n├── ArchivedSampleInventory.xlsx\n├── EquipmentInventory.xlsx\n└── SOSImpacts.xlsx\nWithin the each year sub-folder, we have sub-sub-folders for planning, forms, data, and processes. This structure helps maintain a reproducible workflow year after year. See the 2023_Sampling for an example:\nY:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\\2023_Sampling\n├── Applications\n├── CoCs\n├── Equipment\n├── FieldForms\n├── Forms\n├── GIS\n├── LabData\n├── Labels\n├── ManagementSurveys\n├── PublicDocs\n├── Purchases\n├── Reports\n├── SampleIDAssignments\n├── Scripts\n├── 2023_DataTracking.xlsx\n└── PostSeasonWrapUp_2023.docx\nAs mentioned in Section 6.2, it’s good practice to maintain the raw data. We use additional sub-folders for the LabData folder. Everything in Raw has been set as Read-Only.\nY:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\\2023_Sampling\\LabData\n├── 2023_DataTemplateSoiltest.xlsx\n├── Clean\n├── QC\n├── Raw\n└── Working\nSoil_Health_Initative &gt; StateOfTheSoils &gt; 2023_Sampling &gt; LabData &gt; Clean already has five levels of nesting. We wouldn’t want to add any many more levels or the hierarchy becomes difficult to manage."
  },
  {
    "objectID": "organization.html#archive-folders",
    "href": "organization.html#archive-folders",
    "title": "5  Organization",
    "section": "5.2 Archive folders",
    "text": "5.2 Archive folders\nWhen too many drafts or versions begin to clutter a sub-folder, create a new folder with the naming convention of Archive_FolderDescription. Place the old drafts there. Leave the most current, accurate file in the main folder.\nFor example, the most recent sample labels for each conservation district are listed in the top level CompletedLabels folder, and previous working drafts were moved to the Archive_Labels folder.\nY:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\\2023_Sampling\\Labels\\CompletedLabels\n├── Archive_Labels\n│   ├── CowlitzCounty_Labels.docx\n│   ├── FerryCD_Labels.docx\n│   ├── LewisCD_Labels.docx\n│   └── StevensCD_Labels.docx\n├── CowlitzCounty_Labels_V2.docx\n├── FerryCD_Labels_V2.docx\n├── KittitasCD_Labels.docx\n├── LewisCD_Labels_V2.docx\n├── ...\n└── WallaWallaCD_Labels.docx"
  },
  {
    "objectID": "organization.html#code-based-project-organization",
    "href": "organization.html#code-based-project-organization",
    "title": "5  Organization",
    "section": "5.3 Code-based project organization",
    "text": "5.3 Code-based project organization\nCode-based projects should be organized according to Section 10.1.1 in the code style guide."
  },
  {
    "objectID": "storage.html#backup",
    "href": "storage.html#backup",
    "title": "6  Storage",
    "section": "6.1 Backup",
    "text": "6.1 Backup\nData must be stored in multiple locations. At a bare minimum, data on an individual computer must also be saved on the WSDA shared drive. Backing up data using version control (GitHub) or a cloud service (Microsoft OneDrive or Box.com) is strongly recommended."
  },
  {
    "objectID": "storage.html#sec-raw-data",
    "href": "storage.html#sec-raw-data",
    "title": "6  Storage",
    "section": "6.2 Read-only raw data",
    "text": "6.2 Read-only raw data\nOn our shared drives, raw data such as lab results from Soiltest or exports from ArcGIS Online, should be immediately set to Read-only. Right click the file &gt; click on Properties &gt; check the Read-only attribute box.\n\nThe file should then be copied over to a Working folder for any processing or analyses. The final dataset should be saved in a separate descriptively titled Clean folder. Keeping a readme.txt to document your processing and analysis steps is good practice, as discussed in Section 7.2.1."
  },
  {
    "objectID": "storage.html#version-control-with-git-and-github",
    "href": "storage.html#version-control-with-git-and-github",
    "title": "6  Storage",
    "section": "6.3 Version control with Git and GitHub",
    "text": "6.3 Version control with Git and GitHub\nA version control system records changes to a file or set of files over time. Git is a free and open-source distributed version control system and GitHub is the hosting site WSDA and WaSHI use to interface with this system. Git and GitHub are an important foundation of reproducible statistical and data scientific workflows (Bryan 2018).\nA major benefit of using version control is ensuring changes are well documented and previous versions are accessible if any changes must be recalled. Additionally, version control makes collaboration across projects much more robust.\nVersion control is not just for code either! It’s useful for scripts, documents, presentations, and books (like this DMP!). Instead of saving each version of a file with a different name (i.e., Report_V01.docx and Report_V02.docx; for a reminder on version naming, see Section 4.2.7), there’s only one file Report.docx which automatically has its history and editors saved with Git and GitHub.\nIn the screenshot below, you can see who made a commit (which is basically a named version of changes), when that commit was made, and you can click on the commit message to view all of the files that were changed.\n\nAfter clicking on the first commit message, we see the documentation.qmd file was changed with additions highlighted in green and deletions highlighted in red.\n\n\n\n\n\n\nGit and GitHub resources\nRead Jenny Bryan’s article Excuse Me, Do You Have a Moment to Talk About Version Control for a great background on Git and GitHub, why we should be using it, and a brief how to get started. For detailed instructions , please follow along with Jenny Bryan’s free online book Happy Git and GitHub for the useR. Another helpful book resource is GitHub: A Beginner’s Guide, which was created by Birds Canada (avian conservation NGO) for people without a lot of programming background.\nIf you prefer to look through slides, check out Byron C. Jaeger’s presentation Happier version control with Git and GitHub (and RStudio)."
  },
  {
    "objectID": "storage.html#staff-turnover",
    "href": "storage.html#staff-turnover",
    "title": "6  Storage",
    "section": "6.4 Staff turnover",
    "text": "6.4 Staff turnover\nWhen staff leave our group, they take their skills, institutional knowledge, and personal understanding of their file management with them. Proper offboarding is essential to ensure knowledge isn’t lost, time isn’t wasted trying to recreate workflows, and projects keep moving.\nBefore the employee leaves, the Senior Soil Scientist and Data Scientist ensure that:\n\nFolders and files are moved from the employee’s personal drive to the shared drive. They are named and organized according to Chapter 4.\nWorkflows and specific processes the employee was responsible for are well documented.\nPermissions and ownerships are transferred to the appropriate remaining staff.\n\nGitHub WSDA organization\nArcGIS data products and online groups\nDatabase credentials\nBox.com folders\n\n\nMore resources and offboarding checklists from Harvard Research Data Management can be found in our DataManagement shared drive.\n\n\n\n\nBryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928."
  },
  {
    "objectID": "documentation.html#project-level",
    "href": "documentation.html#project-level",
    "title": "7  Documentation",
    "section": "7.1 Project-level",
    "text": "7.1 Project-level\nProject-level documentation includes all descriptive information about the SoSA dataset, as well as planning decisions and process documentation. Documentation includes quality assurance project plans (QAPP), standard operating procedures (SOP), and other high-level documents (i.e., request for proposals, applications, meeting agendas/notes, etc.).\n\nQuality assurance project plan (QAPP)\nThe QAPP is the highest level of project documentation and covers everything from the project description, personnel roles and responsibilities, project timelines, data and measurement quality objectives, study design, and overviews of field, laboratory, and quality control.\nOurs can be found in Y:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\\QAPP, though it needs to be updated.\n\n\nStandard operating procedures (SOP)\nSOPs provide detailed instructions for field, lab, or data processing procedures and decision making processes.\nAll sampling related SOPs can be found in Y:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\\SOPs.\n\nSoSA sampling\nThe purpose of this SOP is to specify the procedures for a typical site visit in which soil sampling is conducted to measure physical, chemical, and biological soil health indicators. Procedures include equipment preparation prior to sampling, best practices for filling out field forms, the selection of sampling locations, sampling protocols, sample handling and storage, and submitting samples to the lab. This SOP serves to ensure data quality by creating audit trails and enabling verification that data are present, complete, and accurate. Additionally, this SOP will be used to maintain consistent sample collection procedures throughout the state for WSDA employees and partners.\n\n\nQuality control / quality assurance (QA/QC)\nThis SOP outlines the process for screening sample metadata and lab results for completeness, consistency, and quality. Procedures involve subject matter expertise, investigation, communication with sampling teams and labs, algorithmic quality control, and tagging sample results with quality codes (as shown in the below table). Data are then integrated into the statewide database according to a SOP not yet authored.\n\n\nView the quality codes\n\n\n\n\n\n\n\n\n\n\n\nCode\nTag\nDescription\nInclusion in analyses\n\n\n\n\n0\nExcellent\nMet lab’s and WSDA’s QC criteria\nYes\n\n\n100\nEstimate\nInterpolated missing value\nYes\n\n\n110\nDerived\nDerived from an estimated value\nYes\n\n\n120\nSuspect\nZ-score is ≥ |3|\nYes\n\n\n130\nCalculated ND\nCalculated value using at least one ND\nYes\n\n\n140\nNon-detect\nBelow the method detection limit\nNo\n\n\n160\nPoor\nDid not meet lab’s QC criteria\nNo\n\n\n180\nOutlier\nOutlier, designated by soil scientist\nNo\n\n\n200\nUnknown\nExternal dataset\nYes\n\n\n\n\n\n\nSOPs we don’t have (yet)\n\nassigning sample, producer, field IDs\ndata cleaning\ndata integration into database\ndata storage\nexternal data integration"
  },
  {
    "objectID": "documentation.html#dataset-level",
    "href": "documentation.html#dataset-level",
    "title": "7  Documentation",
    "section": "7.2 Dataset-level",
    "text": "7.2 Dataset-level\nDataset-level documentation applies to lab results, sample locations, grower information, and management data. We use readmes and changelogs to document what each dataset contains, how they are related, potential issues to be aware of, and any alterations made to the data.\n\nReadme\nreadme files are plain text documents that contain information about the files in a folder, explanation of versioning, and instructions/metadata for data packages. These files are saved as .txt, not as MS Word documents that take longer to open and can only be opened on computers with Microsoft installed.\n\nDescribe contents of folder\nFor example, the _completeDataset folder contains a readme.txt that describes each of the files’ structure, contents, and other pertinent information, such as the data source (i.e. PRISM mean annual precipitation and temperature are 30-year normals from 1991-2020 at 800 m resolution).\n\n\nView the example\n\n2020-2023_labResults_wide has one sample per row with each measurement as a separate column.\n\n2020-2023_labResults_long has one result per row with columns for sample identification, measurement name, result value, quality code, and lab.\nThis spreadsheet also contains a second tab that describes the quality codes.\n\n2020-2023_sampleLocations has the coordinates, PRISM mean annual precipitation and mean annual temperature (800 m resolution, 30-year normals, 1991-2020), and SHI strata.\n\ndataDictionary lists each variable with its short name (variable name without units), description, indicator type (chemical, physical, biological), unit (if applicable), label, and data type.\nThis spreadsheet also contains a second tab that describes the variables in the sampleLocations spreadsheet.\n\nscbg_producerId_recode lists all SCBG fields with original SCBG assigned producer ID and new WSDA producer ID\n\nallFarmInfo lists all participating farm names, producer names, producer IDs, field names, field IDs, and county\n\nexampleData.csv contains 100 random samples that have been anonymized with fake sampleIDs, county, farm, producer, and field names. WSU SCBG samples are excluded, as are the 0-6in to 6-12in WSDA samples.\n\n\n\nExplain versions\nAnother example is the readme.txt in the 2023_Sampling &gt; LabData &gt; Raw folder, which explains why there are two different versions of the lab results and where to find additional information.\n\n\nView the example\n\n2023-08-21\n2023_WSDASoilHealth_v1.xlsx has errors. See email for details.\n'          'v2.xlsx still has some errors, that are cleaned up in the R scripts.\n\n\n\nProvide instructions\nAn example of a readme.txt that provides instructions on how to use the folder contents can be found in the ArcGIS soil sample points box.com folder that is shared with outside partners.\n\n\nView the example\n\nTemplate for Soil Sample Points ArcGIS\n\n2023-06-01\n\nJadey Ryan | Washington State Department of Agriculture (WSDA)\njryan@agr.wa.gov\n\nPurpose:\nTo provide a template for ESRI ArcGIS data entry and management for soil sampling projects.\n\nFolder contents:\n- README.txt describes the folder contents and provides general instructions.\n- Template_SoilSamplePoints.aprx is an ArcGIS Project File should allow you to open the project in ArcGIS Pro.\n- Template_SoilSamplePoints.gdb is a file geodatabase, which you can open in ArcGIS Pro or ArcGIS Online.\n- crop_domain.csv provides WSDA approved crop types to use in ArcGIS 'Table to Domain' geoprocessing tool. We highly recommend using attribute domains, which are rules to enforce data integrity by limiting the field type and choices of an attribute field.\n- 923_NRAS_SoilHealthSOP_WEB.pdf is WSDA's Standard Operating Procedure for soil sampling. The appendices contain instructions for the GIS workflow of soil sampling.\n- /Screenshots/Feature_Layer_Offline_Editing.png shows the checkboxes required for editing.\n- /Screenshots/Configure_Forms.png shows where to click within a Web Map to open the Field Maps form editor.\n- /Screenshots/Table_to_Domain.png is an example of how to use the 'Table to Domain' geoprocessing tool with the crop_domain.csv.\n- /Screenshots/FieldMapsForm_*.png are examples of the Field Maps form structure, which can be created in ArcGIS Online.\n- WaSHI_SoilSeries_REST_Service is an internet shortcut to the URL for the Washington clipped soil series, originating from NRCS gSSURGO. When compositing multiple samples together from one field, we recommend keeping all sample points within one soil series to reduce variability in the composite samples.\n- The other folders (Index, GpMessages, ImportLog, .backups) are part of the ArcGIS Pro project and can be ignored.\n\nInstructions:\n1. Open Template_SoilSamplePoints.aprx.\n2. Update attribute fields and domains to work with your project.\n3. Update symbology, labels, visibility scale, popups, etc. The symbology of the sample points currently defaults to red when the 'Show/Hide Field Form' attribute is still 'Hide' or 'NULL'. When the sampler collects the sample and changes this attribute to 'Show', the point will turn yellow to indicate the sample has been collected.\n4. Share as Feature Layer and create a Web Map to allow others access to this map.\n5. Open the map in ArcGIS Online and click on 'Forms' in the right toolbar to configure your Field Maps form.\n6. Use the /Screenshots/FieldMapsForm_*.pngs as a guide, but adapt the field form structure to suit your project.\n\nOffline Areas:\n- If you anticipate needing to sample without cellular service or wifi access: 1) Open the hosted feature layer, 2) Click 'Settings' 3) Confirm 'Enable Sync' is turned on.\n- If you need the Soil Series layer, you will need to configure two separate Web Maps: one with and one without. This is because the WaSHI_SoilSeries_REST_Service is not allowed in Web Maps with 'Enable Sync' turned on. \n- Alternatively, you can create your own Soil Series layer and host it as a feature layer in your own organization.\n\nResources:\n- WSDA Soil Health YouTube channel: https://www.youtube.com/playlist?list=PL0pB20prk7Ni1daEYiEEXSWy8CfwO34FC\n- WSDA WaSHI_SoilSeries MapServer: https://fortress.wa.gov/agr/gis/wsdagis/rest/services/NRAS/WaSHI_SoilSeries/MapServer\n- Introduction to attribute domains: https://pro.arcgis.com/en/pro-app/latest/help/data/geodatabases/overview/an-overview-of-attribute-domains.htm\n- Introducing smart forms in ArcGIS Field Maps: https://www.esri.com/arcgis-blog/products/field-maps/field-mobility/introducing-arcgis-smart-forms/\n\n\n\n\nChangelog\nChangelogs are also simple and concise plain text documents saved in a folder alongside data files to document any changes to the dataset.\nAt the bare minimum, the changelog.txt should contain:\n\ndate of modification\ninitials of who made the changes\ndescription of the changes\n\nAn example changelog.txt can be found in the _completeDataset folder.\n\n\nView the example\n\nContents of changelog.txt in _completeDataset folder:\n\n2022-12-15 JR standardized texture classes (title case, no extra white space) and converted texture, county, and crop to factor types.\n2023-01-03 JR corrected error in 2022 WSDA bulk density measurements in 2022-11-01_soiltestData_manualCleanup.xlsx.\n2023-03-02 JR recoded SCBG producer IDs to match WSDA format and cleaned up farm/producer names and IDs. See new scbg_producerId_recode.csv for list.\n2023-03-07 JR corrected Okanogan producer and field names (Devany, Townsend).\n2023-03-21 JR corrected cropType \"Fallow, Idle\" to \"Fallow\" and 2021 SCBG \"Pea\" samples to \"Pea, Dry\", updated Crop Group column. Updated results and sample locations spreadsheets.\n2023-07-13 JR added labID to labResults datasets and updated dataDictionary accordingly. \n2023-08-21 JR added 2023 data and updated dataDictionary sample_locations tab.\n2023-08-27 JR corrected SCBG pulse samples crop from \"Pea\" to \"Pea, Dry\".\n2023-08-30 JR added an anonymized dataset (100 samples from 2022-2023).\n2023-09-05 JR added labID to 2023 results.\n2023-09-06 JR added sampling organization column to make impact tracking easier. Fixed merge issue that was causing the loss of some producer IDs. Add \"County\" to relevant CD names.\n2023-11-14 JR switched from .RData to .RDS file type so users can assign a new name when loading the data into R with `data_wide &lt;- readRDS(\"2020-2023_labResults_wide.RDS\")`.\n2023-11-14 JR added sampling dates and depths to all 877 samples. See addSampleDepthsDates for R script. \n2023-12-08 JR corrected CropGroup from Fallow to Cereal Grain for samples with CropType of Fallow, Wheat in 2020-2023_sampleLocations.csv."
  },
  {
    "objectID": "documentation.html#variable-level",
    "href": "documentation.html#variable-level",
    "title": "7  Documentation",
    "section": "7.3 Variable-level",
    "text": "7.3 Variable-level\nVariable-level documentation includes data dictionaries and code books, which are often talked about interchangeably. However, we’ll refer to the data dictionary as a tabular collection of names, definitions, and attributes about the variables in a dataset created (ideally) in the planning phase of the project before data are collected. In contrast, codebooks provide descriptive, variable-level information and univariate summary statistics to allow users to understand the contents of a dataset without opening it. The codebook is created or updated after data are collected, cleaned, and validated.\n\nData dictionary\nIn a data dictionary, each row is a different variable and each column is a different attribute of that variable. With a data dictionary, any user should be able to properly interpret each variable in our data.\nOur dataDictionary.xlsx in the _completeDataset folder contains two tabs (labResults and sampleLocations) that describe the attributes of each variable.\n\n\nView the sampleLocations dictionary\n\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\nunit\ndataType\n\n\n\n\nyear\nYear the sample was collected\n\nNumeric\n\n\ncounty\nCounty of the sampled field\n\nCharacter\n\n\nsampleId\nSample identification code\n\nCharacter\n\n\ncropGroup\nCrop group of the sampled field\n\nCharacter\n\n\ncropType\nCrop type of the sampled field\n\nCharacter\n\n\nstratum\nSoil Health Institute stratum\n\nCharacter\n\n\nMAP_mm.year\nOSU PRISM mean annual precipitation\n30-year normals (1991-2020) at 800 m resolution\nmm/year\nNumeric\n\n\nMAT_c\nOSU PRISM mean annual temperature\n30-year normals (1991-2020) at 800 m resolution\ndegrees C\nNumeric\n\n\nlongitude\nLongitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\nlatitude\nLatitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\n\n\n\n\nCodebook\nCodebooks provide more information (i.e., existing values/ranges and summary statistics) than the data dictionary and can be used to understand a very high-level summary of the processed data. There are many R packages that generate codebooks; however, we have not implemented this type of documentation for our project yet.\nCrystal Lewis gave the lightning talk A Comparison of Packages to Generate Codebooks. I’d like to generate codebooks for our datasets once they live in a database."
  },
  {
    "objectID": "documentation.html#sec-external-data",
    "href": "documentation.html#sec-external-data",
    "title": "7  Documentation",
    "section": "7.4 External data",
    "text": "7.4 External data\nExternal data refers to any data not directly collected by WSDA or trained partners (e.g., WSU or conservation districts) that follow our SOPs. These can include other studies pre-dating WaSHI, special soil health surveys, or publicly available datasets.\nThe Data Scientist and Senior Soil Scientist will decide whether to integrate an external dataset case by case by considering the below questions:\n\nHow does the study design fit into SoSA goals?\nWho collected the soil samples?\nWhat field procedures were used and how were they documented?\nWho analyzed the soil samples? With which methods and QA/QC procedures?\nWhich pieces of metadata and management data accompany the lab results?\n\nFarm, producer and field info1\nSampling date\nSampling depth\nLatitude and longitude\nProduction system (current crop, crop rotation, etc.)\nTillage, livestock grazing, irrigation, soil fertility and amendments, conservation practices, etc.\n\nIs there a data dictionary or codebook to describe the variables and measurements, units, missing values, etc.?\n\nGenerally, the external data should 1) be well documented, 2) be collected and analyzed by well-trained scientists and labs; and 3) have adequate accompanying metadata and management data to facilitate interpretation of the results.\nSome publicly available datasets to consider are listed in Y:\\NRAS\\Soil_Health_Initiative\\StateOfTheSoils\\DataSources."
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "7  Documentation",
    "section": "",
    "text": "Enough farm, producer and field info to distinguish unique farmers and fields and assign IDs to. This info doesn’t need to be personally identifiable information.↩︎"
  },
  {
    "objectID": "flow.html",
    "href": "flow.html",
    "title": "8  Data flows",
    "section": "",
    "text": "assign unique identifiers\nlabels\ntracking sheet\narcgis (map, domains, field forms, surveys, script to back up)\ncross-reference different sources of crop data (field form, survey, initial form)\nlab data comes back - qa and cleaning\ngenerate reports (save to box.com)\nimport to database\nsave to year folder and _completeDataset, and WSU Teams channel\narchive jars and tubes, update archive spreadsheets"
  },
  {
    "objectID": "share.html#watech-data-categorization",
    "href": "share.html#watech-data-categorization",
    "title": "9  Data sharing",
    "section": "9.1 WaTech data categorization",
    "text": "9.1 WaTech data categorization\nUnder the Washington state policy 141.10 (Securing Information Technology Assets), state agencies must classify data into categories based on the sensitivity of the data. WaTech provides guidance on the four categories of data.\n\nCategory 4: “Confidential information requiring special handling”\nWaTech: Data requires strict handling requirements applied by statues (e.g. HIPAA) or regulations (e.g. rules on employee files).\nSoSA: We don’t manage any data falling under this category.\n\n\nCategory 3: “Confidential information”\nWaTech: Data includes “personal information” as defined in RCW 42.56.590 (security breaches) and RCW 19.255.010 (personal information disclosure). An individual’s first name or first initial and last name in combination with at least one of the following elements: social security number, driver’s license or Washington identification card number, or any account numbers that permit access to their financial account.\nSoSA: While we do not collect any of the above elements in combination with grower names, we still protect individual and farm names, and latitude and longitude coordinates as confidential information.\n\n\nCategory 2: “Sensitive information”\nWaTech: Data are intended for official use only and withheld unless specifically requested.\nSoSA: Lab results and management surveys fall under this category. Access to this data requires a data share agreement.\n\n\nCategory 1: “Public information”\nWaTech: Data is not covered in any of the above categories or is already released to the public.\nSoSA: De-identified and aggregated data such as the number of soil samples and from which counties and crops they were collected fall under this category. For example, the SoSA dashboard is publicly available and the map zoom is disabled at the 1:1,600,000 scale (counties level)."
  },
  {
    "objectID": "share.html#sec-maintain-confidentiality",
    "href": "share.html#sec-maintain-confidentiality",
    "title": "9  Data sharing",
    "section": "9.2 Maintain confidentiality",
    "text": "9.2 Maintain confidentiality\nUnder no circumstances should the following Category 3 data be released to external collaborators or made publicly available:\n\nfarm name\ngrower first and last name\nfield names that contain street names or other identifying information\nlatitude and longitude coordinates or other geospatial identifiers\nany information that identifies the individual farm or grower\n\nCategory 2 data should be anonymized and aggregated to honor our data privacy statement by either removing or replacing Category 3 confidential information with dummy data. The {randomNames} R package can be used to replace real names with fake names. Latitude and longitude should be rounded to a precision that does not identify the farm or fields sampled. See the example R script below, copied from the private soils-internal GitHub repo.\n\n\nR code to anonymize data\n\n```{r}\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(randomNames)\n\nload(\"./data/_completeDataset/2020-2023_labResults_wide.Rdata\")\nload(\"./data/_completeDataset/2020-2023_labResults_long.Rdata\")\ncrops &lt;- read_excel(\"./data/reference/crops.xlsx\")\npoints &lt;- read.csv(\"./data/_completeDataset/2020-2023_sampleLocations.csv\") |&gt;\n  select(sampleId, longitude, latitude) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\n# Join lab data with gis data\ndata &lt;- left_join(allResults_wide, points)\n\n# Join data with crop dictionary to get crop groups\ndata &lt;- left_join(data, crops, by = c(\"crop\" = \"Crop Type\")) |&gt;\n  select(-crop) |&gt;\n  relocate(\"crop\" = \"Crop Group\", .after = county)\n\n# Anonymize\nanonData &lt;- data |&gt;\n  # Farm name, producer name, sample ID\n  mutate(\n    farmName = paste0(\"Farm \", sprintf(\"%03d\", cur_group_id())),\n    producerName = randomNames(which.names = \"first\",\n                               sample.with.replacement = FALSE),\n    producerId = paste0(\n      paste0(sample(LETTERS, 3, replace = TRUE), collapse = \"\"),\n      \"0\",\n      paste0(sample(1:9, 1, replace = TRUE), collapse = \"\")\n    ),\n    fieldName = paste0(\"Field \", fieldId),\n    sampleId = paste0(\n      substr(year, 3, 4), \"-\", producerId, \"-\", fieldId\n    ),\n    .by = c(county, producerName, producerId)\n  ) |&gt;\n  # county\n  mutate(\n    county = paste0(\"County \", cur_group_id()),\n    .by = c(county)\n  )\n\n# Grab 100 random samples\nanon_subset &lt;- anonData |&gt;\n  slice_sample(n = 100)\n\nwrite.csv(anon_subset, file = \"./data/_completeDataset/exampleData.csv\",\n          na = \"\",\n          row.names = FALSE)\n```"
  },
  {
    "objectID": "share.html#data-share-agreement",
    "href": "share.html#data-share-agreement",
    "title": "9  Data sharing",
    "section": "9.3 Data share agreement",
    "text": "9.3 Data share agreement\nSoSA CoPIs created a Data Sharing and Scope of Work Agreement that details the type of data to be shared, the scope of work in which the data may be used, and terms for using SoSA data.\nOnce the agreement has been signed by both CoPIs and the “Partnering Scientists”, the agreement should be saved in its own folder within Y:/NRAS/Soil_Health_Initiative/StateOfTheSoils/DataSharing. If this agreement is also part of a grant proposal, it should also be saved in its corresponding grant folder Y:/NRAS/Soil_Health_Initiative/ContractsGrants/Grants.\nThe request correspondence, code to subset the requested data, and the final dataset sent should all be saved in this folder. Internally documenting this information allows us to track publications and attributions resulting from this data sharing. This documentation also helps us track the broader impact of how our collaborators’ use the SoSA dataset."
  },
  {
    "objectID": "share.html#public-access",
    "href": "share.html#public-access",
    "title": "9  Data sharing",
    "section": "9.4 Public access",
    "text": "9.4 Public access\nCurrently the only SoSA data publicly available are the counts of samples across the project, counties, and crop types displayed in the ArcGIS Online Dashboard.\nA small, anonymized subset is included as example data in the {washi} and {soils} R packages for demonstration purposes.\nTODO: discussion with team on the below ideas\nIn the future when the data are more mature and hosted in a proper database, we may publish an anonymized subset in a public repository such as:\n\nGitHub via an R package or Shiny app\nZenodo: integrates with GitHub and is citable with a DOI\nData.WA.gov: open data portal for the State of Washington\n\nMore inspiration and ideas for enhancing data discoverability and sharing across the agricultural and soil health communities include:\n\nUSDA LTAR data dashboards\nCAF LTAR metadata tool\n\nChapter 16 of Data Management in Large-Scale Education Research discusses more considerations for data sharing and choosing public repositories (Lewis 2023)."
  },
  {
    "objectID": "share.html#acknowledgments",
    "href": "share.html#acknowledgments",
    "title": "9  Data sharing",
    "section": "9.5 Acknowledgments",
    "text": "9.5 Acknowledgments\nAll research and data partially or completely funded by WaSHI must include acknowledgements to the State of Washington. The following text should be included in all publications resulting from this funding:\n\nData were in part provided by the Washington Soil Initiative, which is supported by the State of Washington and administered by the Washington State Department of Agriculture, Washington State Conservation Commission, and Washington State University.\n\nIf WaSHI staff make substantial scientific contributions to the manuscript, discuss the possibility of co-authorship credit.\n\n\n\n\nEuropean Commission. 2016. “H2020 Programme Guidelines on FAIR Data Management in Horizon 2020.” https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nWhyte, Angus, and Graham Pryor. 2011. “Open Science in Practice: Researcher Perspectives and Participation.” International Journal of Digital Curation 6 (March): 199–213. https://doi.org/10.2218/ijdc.v6i1.182."
  },
  {
    "objectID": "code-guide.html#projects",
    "href": "code-guide.html#projects",
    "title": "10  Code style guide",
    "section": "10.1 Projects",
    "text": "10.1 Projects\nAll files associated with a given project (input data, R scripts, analytical results, figures, reports) should be kept together in one directory. RStudio has built-in support for this through projects. R projects bundle all the work in a portable, self-contained folder that can be moved around on your computer or on to other collaborators’ computers and still work. Learn more about projects in the Workflow: scripts and projects chapter of R4DS and in Jenny Bryan’s article Project-oriented workflow.\nTo create a project in RStudio, click File &gt; New Project, then follow the steps in the below figure.\n\n\nProject folder structure\nHaving a consistent and logical folder structure will make it easier for you (and future you) and collaborators to make sense of the files and work you’ve done. Well documented projects also make it easier to resume a project after some time away with minimal frustration of having to remember where everything is, what you did, and why you did it.\nThe below structure works most of the time and should be used as a starting point. However, different projects have different needs, so add and remove subfolders as needed.\n\nroot: top-level project folder containing the .Rproj file.\ndata: contains raw and processed data files in subfolders. Raw data should be made read-only and not changed in any way.\noutput: outputs from R scripts such as figures or tables.\nR: all R scripts containing data processing or function definitions.\nreports: Quarto or RMarkdown files are saved here, as well as the resulting reports.\nREADME: a markdown file (can be generated from Quarto or RMarkdown) to explain the project.\n\n\n\nSee an example project folder structure\n\n├── project-demo.Rproj\n├── data\n│   ├── processed\n│   │   └── data-clean.csv\n│   └── raw\n│       └── data-raw.xlsx\n├── output\n│   ├── fig-01.png\n│   ├── fig-02.png\n│   ├── tbl-01.png\n│   └── tbl-02.png\n├── R\n│   ├── 01_import.R\n│   ├── 02_tidy.R\n│   ├── 03_transform.R\n│   ├── 04_visualize.R\n│   └── custom-functions.R\n└── reports\n│   ├── soil-health-report.pdf\n│   └── soil-health-report.qmd\n│   └── images\n│       └── logo.png\n├── README.md\n└── README.qmd\n\nR packages may contain these additional subfolders and files:\n\ninst: any arbitrary additional files to be included in package installation such as CITATION, fonts, and Quarto templates.\nman: all .Rd (“R documentation”) files for each function that are generated from {roxygen2}.\nvignettes: long-form guides that go beyond function documentation and can be used as tutorials to demonstrate a workflow using the package to solve a particular problem.\ntests: all tests, usually using {testthat}.\npkgdown and docs: if using {pkgdown} to build a website for the package, you may have a pkgdown folder containing the favicon and any additional css and a docs folder containing the website source files.\nDESCRIPTION: file containing metadata about the package (authors, current version, dependencies).\nLICENSE: file describing the package usage agreement.\nNAMESPACE: file generated by {roxygen2} listing functions imported from other packages.\nNEWS.md: file documenting user-facing changes to a package.\n\nLearn more about other R package components in R Packages (2e)(Wickham and Bryan 2023).\n\n\nAbsolute vs relative paths\nNote: directories are synonymous with folders.\n❌Absolute paths start with the root directory and provide the full path to a specific file or folder like C:\\\\Users\\\\jryan\\\\Documents\\\\R\\\\projects\\\\project-demo\\\\data\\\\processed.1 You can run getwd() to find out where the current working directory is and setwd() to set a specific folder as your working directory. However, please don’t do use setwd() because this absolute file path is going to break your code if you reorganize your folders and is not going to work on any collaborators’ computers as their directory configuration will be different.\n\n\n\nA cartoon of a cracked glass cube looking frustrated with casts on its arm and leg, with bandaids on it, containing \"setwd\", looks on at a metal riveted cube labeled \"R Proj\" holding a skateboard looking sympathetic, and a smaller cube with a helmet on labeled \"here\" doing a trick on a skateboard. Artwork by @allison_horst.\n\n\n✅Instead, always use relative paths in your scripts. Relative paths are relative to the working directory (i.e. the project’s home) like data/processed/data-clean.csv. When working in a RStudio project, the default working directory is always the root project directory (where the .Rproj file is).\nFor example, say 01-import.R contains the code read.csv(\"data/processed/data-clean.csv\"). This will read the file from C:\\\\Users\\\\jryan\\\\Documents\\\\R\\\\projects\\\\project-demo\\\\data\\\\processed\\\\data-clean.csv\". The magic of relative paths means that if Dani were have this project on her desktop and run this code, it would read the file from C:\\\\Users\\\\dgelardi\\\\Desktop\\\\project-demo\\\\data\\\\processed\\\\data-clean.csv\". This is why relative paths are so important – they work no matter where the project folder is!\n\n\n{here} package\nIn combination with R projects, use the {here} package to build relative file paths. This is especially important in Quarto files because when the .qmd file renders, its default current working directory is wherever the .qmd file lives. If we are using the above example project structure and wanted to read in our clean data to our soil-health-report.qmd file, we would get an error running read.csv(\"data/processed/data-clean.csv\") because it would be looking for a data subfolder in the reports folder. Instead, we can use the {here} package to build a relative path from our root with read.csv(here::here(\"data\", \"processed\", \"data-clean.csv\")). {here} takes care of the backslashes or forward slashes so the relative path will work no matter the operating system.\n\n\n\nA cartoon showing two paths side-by-side. On the left is a scary spooky forest, with spiderwebs and gnarled trees, with file paths written on the branches like \"~/mmm/nope.csv\" and \"setwd(\"/haha/good/luck/\"), with a scared looking cute fuzzy monster running out of it. On the right is a bright, colorful path with flowers, rainbow and sunshine, with signs saying \"here!\" and \"it's all right here!\" A monster facing away from us in a backpack and walking stick is looking toward the right path. Stylized text reads \"here: find your path.\" Learn more about here. Artwork by @allison_horst."
  },
  {
    "objectID": "code-guide.html#naming-conventions",
    "href": "code-guide.html#naming-conventions",
    "title": "10  Code style guide",
    "section": "10.2 Naming conventions",
    "text": "10.2 Naming conventions\n\n\"There are only two hard things in Computer Science: cache invalidation and naming things.\"\n— Phil Karlton\n\nBased on this quote, Indrajeet Patil developed slides with a lot of detailed advice on naming things in computer science (completely language-agnostic).\nR code specific naming conventions are listed below. They mostly follow the best practices from Section 4.2 in the naming conventions chapter still apply, with one exception as seen in Section 10.2.3.1.\nTODO: add Python naming conventions (they differ from R as static variables aka constants are supposed to be SCREAMING_SNAKE_CASE).\n\nProject folder and GitHub repository\nThe project folder and GitHub repository name should be the same. Be concise and descriptive. Use kebab-case.\nExample: washi-dmp, soils.\n\n\nFiles\nBe concise and descriptive. Avoid using special characters. Use kebab-case with underscores to separate different metadata groups (date_good-name).\nExamples: 2024_producer-report.qmd, tables.R, create-soils.R.\nIf files should be run in a particular order, prefix them with numbers. Left pad with zero if there may be more than 10 files.\nExample:\n01_import.R\n02_tidy.R\n03_transform.R\n04_visualize.R\n\n\nVariables, objects, and functions\nLet’s first define these terms so we’re on the same page. In this style guide, variables are essentially column names, objects are all data structures in R (vectors, lists, data frames), and functions are self-contained modules of code that accomplish a specific task.\n\nVariables in a dataset\nThis is where we differ from the tidyverse style guide. Hadley recommends using only lowercase letters, numbers, and _ for all variables (column names and R objects). However, we are defining variables as only column names and objects as only data structures in R. For column names, we use lowerCamelCase for most variables, except for measurement names, we use _ to separate the measurement name from the unit and . as a substitute for / (fraction bar).\nExample of general variable: sampleId\nExample of soil health indicator variable: 96-hr-min-c_mg-c-per-kg-per-day = 96 hour mineralizable carbon (mg C/kg/day).\n\n\nObjects and functions\nObjects names should be nouns and function names should be verbs (Wickham 2022). Again, use lowercase letters, numbers, and underscores. Do not put a number as the first character of the name. Do not use hyphens. Do not use names of common functions or variables.\nObject examples:\n# Good\nprimary_color\ndata_2023\n\n# Bad\n\n# Less human readable, inconsistent with style guide\nprimarycolor\n\n# Using a hyphen in an object name causes error\ndata-2023 &lt;- read.csv(\"2023_data-clean.csv\")\nError in data - 2023 &lt;- read.csv(\"2023_data-clean.csv\") : could not find function \"-&lt;-\"\n  \n# Starting an object name with a number also causes error\n2023_data &lt;- read.csv(\"2023_data-clean.csv\")\nError: unexpected input in \"2023_\"\n\n# Overwrites R shortcut for TRUE\nT &lt;- FALSE\n\n# Overwrites R function\nc &lt;- 10\nFunction examples:\n# Good\nadd_row()\nassign_quality_codes()\n\n# Bad\n\n# Uses noun instead of verb\nrow_adder() \n\n# Inconsistent with style guide\nassignQualityCodes()\n\n# Overwrites common base R function\nmean()"
  },
  {
    "objectID": "code-guide.html#r-scripts",
    "href": "code-guide.html#r-scripts",
    "title": "10  Code style guide",
    "section": "10.3 R Scripts",
    "text": "10.3 R Scripts\n\nHeader template\nIncluding a header in every R script helps standardize the metadata elements provided at the beginning of your code and documents its purpose. Dr. Timothy S Farewell wrote a great blog post for creating a template for the header of every R script. The following template and instructions are adapted from his post (Farewell 2018).\n\nScript name: meaningful and concise.\nPurpose: brief description of what the script aims to accomplish.\nAuthor(s) and email: it’s good to know where the script originated from if there are any questions, comments, or improvements.\nDate created: this is automatically filled in from the template.\nNotes: free-text space for any thoughts or todos.\n\n## ---------------------------\n## Script name: check-crops.R\n##\n## Purpose: Cross reference sample requests, Field Maps forms, and management \n## surveys to get the correct crop planted at the time of sampling.\n##\n## Author: Jadey Ryan \n##\n## Email: jryan@agr.wa.gov\n##\n## Date created: 2024-01-02\n##\n## Notes:\n##   \n## ---------------------------\n    \n# Attach packages ==============================================================\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load data ====================================================================\nAdd this template to RStudio using snippets:\n\nModify the below code with your name and preferred packages.\nIn RStudio, go to Tools &gt; Edit Code Snippets.\nScroll to the bottom of the R code snippets, and paste your modified code (the indent and tabs are important!).\nClick Save and close the window.\nTry it out by opening a new blank .R script, typing header, and then Shift + Tab.\n\nsnippet header\n    ## Header ======================================================================\n    ##\n    ## Script name: \n    ##\n    ## Purpose: \n    ##\n    ## Author: Jadey Ryan \n    ##\n    ## Email: jryan@agr.wa.gov\n    ##\n    ## Date created: `r paste(Sys.Date())`\n    ##\n    ## Notes:\n    ##   \n    \n    # Attach packages ==============================================================\n\n    library(readxl)\n    library(writexl)\n    library(janitor)\n    library(dplyr)\n    library(tidyr)\n    \n    # Load data ====================================================================\n\n\nSection template\nThe above header template also uses section breaks (commented lines with = that break up the script into easily readable chunks). Section breaks are fantastic tools in RStudio because they allow you to easily show or hide blocks of code, see an outline of your script, and navigate through the source file. Read more about code folding and sections in this Posit article.\nThe snippet to create this section template that fills in the rest of the line with = was adapted from this stack overflow answer.\nsnippet end\n    `r strrep(\"=\", 84 - rstudioapi::primary_selection(rstudioapi::getActiveDocumentContext())$range$start[2])`\nTo create a new section with the heading “Tidy data”, type in your script:\n# Tidy data end&lt;Shift+Tab&gt; which will result in:\n# Tidy data ===================================================================="
  },
  {
    "objectID": "code-guide.html#code-styling",
    "href": "code-guide.html#code-styling",
    "title": "10  Code style guide",
    "section": "10.4 Code styling",
    "text": "10.4 Code styling\nReview the Syntax chapter of the tidyverse style guide for a lengthy section that covers spacing, function calls, long lines, semicolons, assignments, comments, etc. Also skim through Chapter 4 Workflow: code style chapter of R4DS, which highlights the opinionated “most important parts of the tidyverse style guide”. Instead of rewriting all of these details and conventions into this style guide and making us all memorize the content, we should all just use the {styler} (as advised in R4DS Chapter 4).\n{styler} is a package and RStudio Addin that will format your code for you, so we can keep our coding style consistent across projects and better facilitate collaboration. We’ll deviate slightly from the tidyverse style and instead use {grkstyle}. {grkstyle} is an extension package for {styler} that Garrick Aden-Buie developed based on the tidyverse style guide. I prefer {grkstyle} over the tidyverse style that {styler} defaults to mainly because of how it handles line breaks in function calls.\nThe below example and installation instructions are pretty much copied directly from Garrick’s {grkstyle} README.\n\nExamples\n\ngrkstyle\ndo_something_very_complicated(\n    something = \"that\",\n    requires = many,\n    arguments = \"some of which may be long\"\n) \n\n\nstyler::tidyverse_style\ndo_something_very_complicated(\n  something = \"that\", requires = many,\n  arguments = \"some of which may be long\"\n) \n\n\n\nInstallation\nInstall {styler} and {grkstyle} with:\ninstall.packages(\"styler\")\n\noptions(repos = c(\n    gadenbuie = \"https://gadenbuie.r-universe.dev\",\n    getOption(\"repos\")\n))\n\n# Download and install grkstyle in R\ninstall.packages(\"grkstyle\")\nSet grkstyle as the default in {styler} functions and addins by running\n# Set default code style for {styler} functions\ngrkstyle::use_grk_style()\nor adding the following to your ~/.Rprofile\noptions(styler.addins_style_transformer = \"grkstyle::grk_style_transformer()\")\nTo edit your .Rprofile, you can use usethis::edit_r_profile() to open the file in your RStudio.\n\n\nUsage\nOnce {styler} and {grkstyle} are installed, you can apply the style to your .R, .qmd, and .Rmd files using the command palette, keyboard shortcut, or addins menu.\n\nCommand palette\nUse RStudio’s command palette to quickly and easily access any RStudio command and see keyboard shortcuts. Open the command palette with Cmd/Ctrl + Shift + P, then type “styler” to see the shortcuts offered by {styler}.\n\n\nKeyboard shortcuts\nI often use Cmd/Ctrl + Shift + A to style the entire active file every time I finish a code block or section. To style just a selection, use Cmd/Ctrl + Alt + Shift + A.\n\n\nAddins menu\nYou can also use the Addins menu in RStudio to style your files by clicking on a button to run the command.\n\n\n\n\n\nFarewell, Dr Timothy S. 2018. “My Easy r Script Header Template  Tim Farewell.” https://timfarewell.co.uk/my-r-script-header-template/.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages (2e). 2nd ed. https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Grolemund Garrett. 2023. R for Data Science (2e). 2nd ed. https://r4ds.hadley.nz/."
  },
  {
    "objectID": "code-guide.html#footnotes",
    "href": "code-guide.html#footnotes",
    "title": "10  Code style guide",
    "section": "",
    "text": "Note the two backslashes. Windows paths use backslashes, which mean something specific in R. to get a single backslash in the path, we need to type two backslashes (or use forward slashes).↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the\nR.J. Cook Agronomy Farm Long-Term Agroecological Research\nSite.”\n\n\nEuropean Commission. 2016. “H2020 Programme Guidelines on FAIR\nData Management in Horizon 2020.” https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf.\n\n\nFarewell, Dr Timothy S. 2018. “My Easy r Script Header Template\n Tim Farewell.” https://timfarewell.co.uk/my-r-script-header-template/.\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education\nResearch [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life\nCycle.” https://www.fws.gov/data/life-cycle.\n\n\nWhyte, Angus, and Graham Pryor. 2011. “Open Science in Practice:\nResearcher Perspectives and Participation.” International\nJournal of Digital Curation 6 (March): 199–213. https://doi.org/10.2218/ijdc.v6i1.182.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages (2e). 2nd\ned. https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Grolemund Garrett. 2023.\nR for Data Science (2e). 2nd ed. https://r4ds.hadley.nz/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific Data Management\nand Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18."
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management Plan",
    "section": "",
    "text": "Overview\nThe Washington Soil Health Initiative (WaSHI) is a partnership between the Washington State Department of Agriculture (WSDA), Washington State University (WSU), and the State Conservation Commission. WaSHI establishes a coordinated approach to healthy soil in Washington.\nTo date, nearly 1,000 soil samples and management surveys across 50 different cropping systems have been collected as a part of the State of the Soils Assessment (SOS). WSDA and WSU lead this project with support from staff, students, conservation districts, and agricultural professionals throughout Washington.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#chapter-outline",
    "href": "index.html#chapter-outline",
    "title": "Data Management Plan",
    "section": "Chapter outline",
    "text": "Chapter outline\nThis Data Management Plan (DMP) is a living document to be continually reviewed and improved based on lessons learned, new information, and collaborator feedback. The header on the first page displays the date of the last update.\n1  What is data management? describes what data management is, why it is crucial to achieve our data-driven goals, and how our data move through the data life cycle.\n2  Formats & standards describes the various data formats we collect and manage. ISO standards are also described for date and geospatial data.\n3  Naming conventions describes naming conventions, best practices, and examples for how we name folders and files.\n4  Organization describes how we organize our folders into a hierarchical structure.\n5  Storage & version control describes where we store data, what our backup policies are, how we protect our raw data, and how we use version control.\n6  Documentation describes how we record each element of the data life cycle with project-level, dataset-level, and variable-level documents such as standard operating procedures, readme files, data dictionaries, etc.\n7  Data flow describes how data are generated, processed, and moved from start to finish. Processes and tasks are grouped by pre, during, and post field season.\n8  Data sharing describes how we protect producer privacy; how our data fits into WaTech data categories; requirements and processes for maintaining confidentiality; and our data share agreement, public access policies, and our preferred acknowledgements.\n9  Code style guide describes our recommended project structures, code-specific naming conventions, script structures, and code style.\n\n\n\n\n\n\nLinks to shared drive folders and files\n\n\n\n\n\nThis DMP includes many links to folders and files on the shared drive, which are only accessible to WSDA staff on the state network or remotely connected using VPN (Virtual Private Networking).\nGoogle Chrome will allow you to open the links using the Enable local file links extension that should automatically be enabled by WSDA.\nHowever, shared drive links are not accessible when using Microsoft Edge. Nothing happens when clicking on these links. To open the file or folder, right-click on the hyperlink &gt; copy the path &gt; paste it into the search bar of the file explorer &gt; press Enter or click the arrow.\n\n\n\n\n\n\n\n\n\n\nGitHub links\n\n\n\nIf you aren’t logged into a GitHub account that is part of the WSDA organization or has access to the washi-sos repository, GitHub links to specific scripts will take you to a 404 not found error page.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#roles-and-responsibilities",
    "href": "index.html#roles-and-responsibilities",
    "title": "Data Management Plan",
    "section": "Roles and responsibilities",
    "text": "Roles and responsibilities\nTo maximize the benefits of effective data management, all WaSHI personnel who interact with SOS data must familiarize themselves with this DMP.\nThe WSDA Data Scientist, supported by the Co-Principal Investigators (CoPIs), is responsible for providing guidance to WaSHI staff working with SOS data and ensuring the implementation of this DMP. The Data Scientist is also responsible for reviewing and updating this document annually, and as needed. Upon updates, the Data Scientist will distribute this document to WaSHI staff and commit the source code to the GitHub repository.\n\nCurrent roles\n\n\n\n\n\n\n\n\nRole\nAffiliation\nName\nTitle\n\n\n\n\nCoPI\nWSDA\nDani Gelardi\nSenior Soil Scientist\n\n\nCoPI\nWSU\nDeirdre Griffin LaHue\nAssistant Professor\n\n\nData Manager\nWSDA\nJadey Ryan\nData Scientist\n\n\nData Stewards\nWaSHI staff",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#staff-turnover",
    "href": "index.html#staff-turnover",
    "title": "Data Management Plan",
    "section": "Staff turnover",
    "text": "Staff turnover\nWhen staff leave, they take their skills, institutional knowledge, and personal understanding of their file management with them. Proper offboarding is essential to ensure knowledge isn’t lost, time isn’t wasted trying to recreate workflows, and projects keep moving.\nBefore the employee leaves, the Senior Soil Scientist and Data Scientist ensure that:\n\nFolders and files are moved from the employee’s personal drive to the shared drive. They are named and organized according to 3  Naming conventions.\nWorkflows and specific processes the employee was responsible for are well documented.\nPermission and ownership for the following are transferred to the appropriate remaining staff:\n\nGitHub WSDA organization\nArcGIS data products and online groups\nDatabase credentials\nBox.com folders\n\n\nMore resources and offboarding checklists from Harvard Research Data Management can be found in our data-management shared drive.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data Management Plan",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis DMP was adapted from the R.J. Cook Agronomy Farm Long-Term Agroecological Research Site DMP (Carlson 2021), U.S. Fish and Wildlife Service data management life cycle (U.S. Fish & Wildlife Service 2023), Harvard Medical School Longwood Research Data Management DMP guidelines (Harvard Medical School 2023), and the Data Management in Large-Scale Education Research book (Lewis 2023).\n\n\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the R.J. Cook Agronomy Farm Long-Term Agroecological Research Site.”\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "data-management.html",
    "href": "data-management.html",
    "title": "1  What is data management?",
    "section": "",
    "text": "1.1 Data life cycle",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data management?</span>"
    ]
  },
  {
    "objectID": "data-management.html#data-life-cycle",
    "href": "data-management.html#data-life-cycle",
    "title": "1  What is data management?",
    "section": "",
    "text": "This graphic explains the data life cycle (U.S. Fish & Wildlife Service 2023), in which each step requires care to ensure transparency, quality, and integrity.\nOur adaptation is outlined below and the following chapters detail our internal processes and standards to follow throughout each step in the data life cycle.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlan\nPlanning includes decisions about data acquisition, management, and quality control, as well as regular examinations of ways to improve. For example, each year we provide an updated spreadsheet template to Soiltest lab to ensure that measurements are reported with correct units and in the correct format. Special projects that deviate from our standard operating procedures require additional planning.\n\n\n\n\n\n\n\nAcquire\nWe acquire data by collecting and analyzing new samples, deriving new insights from existing samples, or accepting datasets from collaborators.\n\n\n\n\n\n\n\nMaintain\nMaintenance involves processing data for aggregation, analyses, and reporting. We create metadata that facilitates interpretation of the data. We also store a copy of our data in a format that is accessible to our collaborators and future selves.\n\n\n\n\n\n\n\nAccess\nAccess refers to data storage, publication, and security. Raw and processed data with accompanying metadata should be stored, backed up, and available for information sharing with our partners. With PI approval, anonymized and aggregated data that does not compromise growers’ personally identifiable information can be made publicly available in a data repository or data product/decision-support tool.\n\n\n\n\n\n\n\nEvaluate\nWe evaluate data while processing and analyzing it to maximize accuracy and productivity, while minimizing costs associated with errors or tedious data cleaning labor. Evaluation workflows should be efficient, well-documented, and reproducible. Our evaluated data help us better understand how environmental factors and management decisions impact soil health.\n\n\n\n\n\n\n\nArchive\nProperly archiving our results supports the long-term storage and usefulness of our data. While similar to the Access element of the life cycle, archiving focuses on preserving data for historical and long-term access. For example, we archive each year’s raw data for long-term storage and set those files to Read-Only.\n\n\n\n\n\n\n\nQuality Assurance / Quality Control (QA/QC)\nData quality management prevents data defects that hinder our ability to apply data towards our science-based conservation efforts. Defects include incorrectly entered data, invalid data, and missing or lost data. QA/QC processes should be incorporated in every element of the data life cycle.\n\n\n\n\n\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life Cycle.” https://www.fws.gov/data/life-cycle.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is data management?</span>"
    ]
  },
  {
    "objectID": "formats-standards.html",
    "href": "formats-standards.html",
    "title": "2  Formats & standards",
    "section": "",
    "text": "2.1 Data formats\nData generated from or integrated into WaSHI can be non-digital or digital.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formats & standards</span>"
    ]
  },
  {
    "objectID": "formats-standards.html#data-formats",
    "href": "formats-standards.html#data-formats",
    "title": "2  Formats & standards",
    "section": "",
    "text": "Non-digital data\nNon-digital data, such as field forms, management surveys, and chain of custody forms, are manually recorded on paper forms. Paper forms must be transcribed or converted to digital file formats and then stored in the WaSHI filing cabinet in the Natural Resources Building in Olympia.\n\n\nDigital data\nDigital data include tabular, spatial, and binary data, such as lab results, sample locations, and field photos. Non-conventional data also include code, algorithms, tools, and workflows.\nTabular data include comma separated values (csv), tab separated values (tsv), Microsoft Excel open XML spreadsheet (xlsx), and portable document format (pdf).\nSpatial data include file geodatabases (gdb), vector shapefiles (zipped folder containing multiple file extensions), keyhole markup language (kml or kmz). Tabular data may also contain spatial data such as longitude and latitude.\nBinary data include photos (jpeg, png, gif, tiff), videos (mp4), code (R, py, js), and object-oriented data files (RDS, Rdata, parquet, arrow).\nProprietary data formats include Microsoft Excel, Word, and Powerpoint files (xlsx, docx, pptx). RDS and RData files are examples of application-specific data formats that can only be opened using the R programming language or RStudio IDE. These types of files should be saved in conjunction with a copy of the data in a non-proprietary and open-standard format, such as csv, to maintain accessibility for those who do not have Microsoft Office or do not use R.\nWritten documents and presentations are in formats including Microsoft Word and PowerPoint (docx and pptx), hypertext markup language (HTML), and pdf.\nNotebooks combine text with executable code to generate written documents and presentations in docx, pptx, html, or pdf formats. These notebooks are stored in formats depending on the programming language: a few examples include R markdown (rmd), Quarto (qmd), and Jupyter notebook (ipynb).\nThe list below is not exhaustive and will continue to grow as additional data sources are discovered.\n\n\n\nType\nSource\nFormats\n\n\n\n\nLab results\nProvided by an analytical lab, study PI, or grower\ncsv, xlsx, pdf, xml, json, RDS, RData\n\n\nManagement surveys\nCollected through interviews with grower\ncsv, xlsx, RDS, RData, scanned paper form\n\n\nField forms\nCompleted in the field during/immediately after sampling\npdf, scanned paper form, csv, xlsx\n\n\nSample locations\nIdentified prior to sampling using ArcGIS Online and updated while sampling using ArcGIS Field Maps\nArcGIS feature layer, shp, kmz, csv, xlsx\n\n\nChain of custody forms\nCompleted prior to shipping or dropping off samples\npdf, scanned paper form\n\n\nClimate data\nOSU PRISM, NOAA, Esri Living Atlas\ncsv, shp, netCDF, tiff, gdb\n\n\nSoil data\nNRCS Web Soil Survey, NRCS WA gSSURGO\ngdb, accdb\n\n\nImages\nLogos, icons, photos taken in the field\njpeg, png, gif, tiff, svg\n\n\nVideos\nRecordings of meetings, training videos\nmp4\n\n\nDocuments\nReports, manuscripts, SOP, QAPP, factsheets, brochures\ndocx, txt, html, pdf\n\n\nPresentations\nPowerPoints, slide decks\npptx, html, pdf\n\n\nCode\nScripts for wrangling and analyzing data; markdown for documents and presentations; style sheets for html\nR, py, ipynb, js, yml, rmd, qmd, css, scss",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formats & standards</span>"
    ]
  },
  {
    "objectID": "formats-standards.html#sec-data-standards",
    "href": "formats-standards.html#sec-data-standards",
    "title": "2  Formats & standards",
    "section": "2.2 Data standards",
    "text": "2.2 Data standards\n\n\nDate will be expressed as YYYY-MM-DD according to ISO 8601 standard.\nDate with time will be expressed as YYYY-MM-DDTHH:MM:SSZ.\n\nT separates date from time.\nZ designates the time zone (Z or -HH:MM).\n\nZ if using Universal Time Coordinated (UTC) with no offset.\nPacific Standard Time (PST) offset is -8:00.\nYYYY-MM-DDTHH:MM:SS-8:00\nPacific Daylight Time (PDT) offset is -7:00.\nYYYY-MM-DDTHH:MM:SS-7:00\n\n\n\n\n\n\n\n\nISO 8601, Randall Munroe’s xkcd\n\n\n\n\nGeospatial data will be accompanied by metadata that abides by the ISO 19115 standard and follows Esri’s documentation when using ArcGIS Pro. Metadata contains information about the identification, extent, quality, spatial and temporal schema, spatial reference, and distribution of digital geographic data.\nCode will follow the style guide in Chapter 9.\n\n\n\nISO 8601, Randall Munroe’s xkcd",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formats & standards</span>"
    ]
  },
  {
    "objectID": "naming.html",
    "href": "naming.html",
    "title": "3  Naming conventions",
    "section": "",
    "text": "3.1 Why are conventions important?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "naming.html#why-are-conventions-important",
    "href": "naming.html#why-are-conventions-important",
    "title": "3  Naming conventions",
    "section": "",
    "text": "Improves consistency and predictability, making it easier to browse folders and know what they contain.\nEnables sorting files by date, conservation district, or another theme.\nFacilitates collaboration so all team members can find the information they need.\nStandardizes file paths and URLs for efficient programming and website hosting.\n\nURLs and programming languages are case-sensitive. WaSHI-data.csv and washi-data.csv are completely different files.\nURLs cannot have spaces in them. They must be escaped with this character entity %20. For example, wasoilhealth.org/producer spotlights would need to be wasoilhealth.org/producer%20spotlights.\n\n\nFor more web-specific naming conventions, see this Learn the Web webpage.\n\n\n\n\n\n\nDocuments, Randall Munroe’s xkcd",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "naming.html#sec-naming-best-practices",
    "href": "naming.html#sec-naming-best-practices",
    "title": "3  Naming conventions",
    "section": "3.2 Best practices",
    "text": "3.2 Best practices\nSome files and folders in our shared drive do not follow these best practices or naming conventions. We are learning and improving as we go.\nThese are just guidelines. Because naming things is hard, we only ask that you try your best. If you’re unsure about names or adding externally named files, the Data Scientist can support you.\nSee Section 3.3 for a table of examples of folder and file names following these best practices.\n\nMeaningful name casing\nDifferent conventions work for different purposes (folders and files versus programming objects).\n\nkebab-case: all lowercase with hyphens separating words. Use for folders and files.\nsnake_case: all lowercase with underscores separating words. Only use for column names in spreadsheets and code, such as functions and variables in R. See Section 9.2.3.1 for example R errors when including hyphens in object names.\n\n\n\n\nArtwork by Allison Horst\n\n\n\n\nDelimiters convey meaning\nDeliberately use underscores and hyphens so we can easily understand the contents and programmatically parse file and folder names.\n\nUse underscores to delineate metadata elements (i.e. date from name from version date_name_version).\nUse hyphens to separate parts of one metadata element (i.e. date YYYY-MM-DD or name wsda-washi-presentation).\n\n\n\nNo spaces or special characters\nAvoid spaces and special characters (only use underscores and hyphens). Characters like / () ! ? % + \" ' have special meaning to computers and can break file paths and URLs.\n\n\nCharacter length matters\nComputers are unable to read file paths and file names that surpass a certain character length. Be concise AND descriptive. Omit prepositions and articles when possible. Abbreviate long words. The path limit on Windows is 260 characters.\n\n\n‘Back to front’ date\nExpress date ‘back to front’ like YYYY-MM-DD according to the ISO 8601 standard. Left pad single-digit months and days with zeros to maintain chronological order of records when sorting alphanumerically.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\n2020-05-28_agenda.pdf\n2-14-2023_Agenda.pdf\n\n\n2023-01-01_agenda.pdf\n2023-Jan-1_Agenda.pdf\n\n\n2023-02-14_agenda.pdf\nDec052020_Agenda.pdf\n\n\n2023-12-05_agenda.pdf\nMay_28_2020_Agenda.pdf\n\n\n\n\n\nGroup & sort files by name\nConsider how folders and files should be grouped and sorted, and include the appropriate metadata at the beginning of the file name. See examples below.\n\n\n\n\n\n\n\nSort by district\nSort by date\n\n\n\n\ncowlitz_coc_2023-05-01.pdf\n2023-05-01_cowlitz_coc.pdf\n\n\ncowlitz_coc_2023-05-23.pdf\n2023-05-01_cowlitz_tracking.pdf\n\n\ncowlitz_tracking_2023-05-01.pdf\n2023-05-09_ferry-cd_tracking.pdf\n\n\nferry-cd_coc_2023-05-10.pdf\n2023-05-10_ferry-cd_coc.pdf\n\n\nferry-cd_coc_2023-05-17.pdf\n2023-05-17_ferry-cd_coc.pdf\n\n\nferry-cd_coc_2023-06-06.pdf\n2023-05-23_cowlitz_coc.pdf\n\n\nferry-cd_tracking_2023-05-09.pdf\n2023-06-06_ferry-cd_coc.pdf\n\n\n\n\n\nVersion numbers\nIncluding the date in the file name is one way to version a file. Alternatively, or in addition to, append a number. Consider how many possible versions there could be. If more than 10, use leading zeros so the numbers have the same length. v1 through v15 will not sort the same way as v01 through v15.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\nsop_v01.pdf\nSOP_v1.pdf\n\n\nsop_v02.pdf\nSOP_v10.pdf\n\n\n… [v03 - v09]\nSOP_v11.pdf\n\n\nsop_v10.pdf\nSOP_v2.pdf\n\n\nsop_v11.pdf\n… [v3 - v9]\n\n\n\n\n\nCollaboration\nAdd your initials to the end of the file name when “saving as” a file that multiple people are working on (i.e., 2023_sop-soil-health-monitoring_lm-jr.docx). This ensures a version is kept as a backup. Alternatively, use Track Changes if working in a MS Word document.\n\n\nLiterature and references\nWhen saving journal articles, user guides, and other reference materials, use the convention author_year_abbreviated-title. Use underscores to separate different metadata.\n\n\n\n\n\n\n\n✅ Do this\n❌ Don’t do this\n\n\n\n\nlal_2004_soil-c-to-mitigate-cc.pdf\nlal-2004-soil-c-to-mitigate-cc.pdf\n\n\nclark-et-al_2020_pmn-sampling\nclark-et-al_2020_pmn-sampling\n\n\n\n\n\nColumn names and code\nNaming conventions for data column headers differ from folders and files. The hyphens in kebab-case cause errors in R and SQL code. Additionally, hyphens, spaces, and other special characters are invalid for ArcGIS table and field names.\nUse snake_case for column names in spreadsheets and code objects (R vectors, lists, dataframes, and functions).\nIn variable or parameter names, include the measurement with the unit. This prevents unit confusion and reduces the risk of misinterpreting or inappropriately using the data.\nDo not use special characters. Instead of toc_%, use toc_percent.\n\n\n\n\n\n\nNote\n\n\n\nThe code naming convention here applies primarily to R. Python and other programming languages have different conventions.\n\n\nSee Chapter 9 for more details in the code style guide.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "naming.html#sec-naming-examples",
    "href": "naming.html#sec-naming-examples",
    "title": "3  Naming conventions",
    "section": "3.3 Naming examples",
    "text": "3.3 Naming examples\n\n\n\n\n\n\n\n\n\nNaming convention\nExamples\n\n\n\n\nFolders\nkebab-case\n2024_sampling\ndata-management\n\n\nFiles\nkebab-case\n2023-11-15_survey-perennial.xlsx\n2024-03_washi-newsletter-wsda-sos.docx\ngeisseler-et-al_2019_ace-protein.pdf\nwashi-logo-color.png\nwashi-dmp.Rproj\n01_load-metadata.R\n2024_producer-report.qmd\n\n\nColumn Names & Code\nsnake_case\nsample_id\npmn_lb_ac\ncrop_summary\nassign_quality_codes()\n\n\n\n\n\n\nDocuments, Randall Munroe’s xkcd\nArtwork by Allison Horst",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Naming conventions</span>"
    ]
  },
  {
    "objectID": "organization.html",
    "href": "organization.html",
    "title": "4  Organization",
    "section": "",
    "text": "4.1 Folder structure\nThere is a delicate balance between deep and shallow folder structures. If too shallow, too many files in one folder are difficult to search. If too deep, too many clicks are required to find a specific file.\nY:/NRAS/soil-health-initiative is the parent folder for all WaSHI content.\nThe state-of-the-soils subfolder uses date- (each year has its own subfolder) and categorical- based (dataset and documentation that span across years) folder structures.\nWithin each year subfolder, use sub-subfolders for planning, forms, data, and processes to maintain a reproducible workflow each year. See the 2023_sampling folder tree for an example:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "organization.html#folder-structure",
    "href": "organization.html#folder-structure",
    "title": "4  Organization",
    "section": "",
    "text": "Y:/NRAS/soil-health-initiative/state-of-the-soils/\n├── _complete-dataset\n├── 2019_scbg\n├── 2021_sampling\n├── 2022_sampling\n├── 2023_sampling\n├── 2024_sampling\n├── data-management\n├── data-sharing\n├── data-sources\n├── maps\n├── projects\n├── qapp\n├── sop\n├── training-videos\n├── equipment-inventory.xlsx\n├── archived-sample-inventory.xlsx\n└── sos-impacts.xlsx\n\nY:/NRAS/soil-health-initiative/state-of-the-soils/2023_sampling/\n├── applications\n├── coc\n├── equipment\n├── field-forms\n├── forms\n├── gis\n├── lab-data\n├── labels\n├── management-surveys\n├── public-docs\n├── purchases\n├── reports\n├── sample-id-assignments\n├── scripts\n├── 2023-data-tracking.xlsx\n└── 2023_post-season-wrap-up.docx",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "organization.html#archive-folders",
    "href": "organization.html#archive-folders",
    "title": "4  Organization",
    "section": "4.2 Archive folders",
    "text": "4.2 Archive folders\nWhen too many drafts or versions clutter a subfolder, create a new folder with the naming convention of archive-folder-description. Place the old drafts there. Leave the most current, accurate file in the main folder.\nFor example, the most recent sample labels for each conservation district are listed in the top level completed-labels folder, and previous working drafts were moved to the archive-labels folder.\nY:/NRAS/soil-health-initiative/state-of-the-soils/2023_sampling/labels/completed-labels\n├── archive-labels\n│   ├── cowlitz-county_labels.docx\n│   ├── ferry-cd_labels.docx\n│   ├── lewis-cd_labels.docx\n│   └── stevens-cd_labels.docx\n├── cowlitz-county_labels_v2.docx\n├── ferry-cd_labels_v2.docx\n├── ...\n├── south-yakima-cd_labels.docx\n├── stevens-cd_labels_v2.docx\n└── walla-walla-cd_labels.docx",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "organization.html#code-based-project-organization",
    "href": "organization.html#code-based-project-organization",
    "title": "4  Organization",
    "section": "4.3 Code-based project organization",
    "text": "4.3 Code-based project organization\nCode-based projects should be organized according to Section 9.1.1 in the code style guide.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Organization</span>"
    ]
  },
  {
    "objectID": "storage-version-control.html",
    "href": "storage-version-control.html",
    "title": "5  Storage & version control",
    "section": "",
    "text": "5.1 Backup\nData must be stored in multiple locations. At minimum, data on an individual computer must also be saved on the WSDA shared drive. Backing up data using version control (GitHub) or a cloud service (Microsoft OneDrive or Box.com) is strongly recommended.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Storage & version control</span>"
    ]
  },
  {
    "objectID": "storage-version-control.html#sec-raw-data",
    "href": "storage-version-control.html#sec-raw-data",
    "title": "5  Storage & version control",
    "section": "5.2 Read-only raw data",
    "text": "5.2 Read-only raw data\nAlways set raw data files, such as lab results or ArcGIS Online exports, as Read-Only to avoid accidental corruption or overwriting. For example, in the lab-data folder, all original data files are set to Read-Only and saved in the raw folder.\nCopy the raw data file to the working folder for processing and analyses. Then save the final dataset in the separate clean folder with a descriptive title. Keeping a readme.txt to document processing steps is good practice, as discussed in Section 6.2.1.\nY:/NRAS/soil-health-initiative/state-of-the-soils/2023_sampling/lab-data\n├── 2023_data-template-soiltest.xlsx\n├── clean\n├── qc\n├── raw\n└── working\nTo set a file as Read-Only: right-click the file &gt; Properties &gt; check the Read-only attribute box &gt; OK.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Storage & version control</span>"
    ]
  },
  {
    "objectID": "storage-version-control.html#sec-version-control",
    "href": "storage-version-control.html#sec-version-control",
    "title": "5  Storage & version control",
    "section": "5.3 Version control with Git and GitHub",
    "text": "5.3 Version control with Git and GitHub\nA version control system records changes to files over time. Git is a free and open-source distributed version control system. GitHub is the hosting site we use to interface with Git. Git and GitHub are fundamental to reproducible statistical and data scientific workflows (Bryan 2018).\nVersion control ensures changes are documented and previous versions are accessible if changes must be recalled. Additionally, version control enables robust collaboration across projects.\nIt’s useful for not only code projects, but also for documents, presentations, and books (like this DMP!). Git and GitHub automatically save the revision history of each file, so there is only a single name for each file (e.g., report.docx) instead of report_v01.docx and report_v02.docx. For a reminder on version naming, see Section 3.2.7).\nThe screenshot below shows who made commits (i.e., named version histories) and when they were made. From this screen, a user can click on the commit message to view all files that were changed.\n\nAfter clicking the first commit message, a diff (i.e., a visual of what changed) displays the additions to documentation.qmd highlighted in green and deletions highlighted in red.\n\n\n\n\n\n\nPrivacy considerations\nReview Chapter 8 to categorize the data included in the repository to protect grower privacy. If the data are not anonymized and aggregated, either 1) the repository must be set to private or 2) data files and any scripts containing Category 3 data as described in Section 8.1.0.2 must be added to the .gitignore file.\n\n\nGit and GitHub resources\nRead Jenny Bryan’s article Excuse Me, Do You Have a Moment to Talk About Version Control (open-access pre-print; full article on WSDA shared drive) for a background on Git and GitHub, why we should use it, and how to get started. For detailed instructions, follow along with her free online book Happy Git and GitHub for the useR.\nGitHub: A Beginner’s Guide is a helpful resource created by Birds Canada for less advanced programmers. If you prefer to look through slides, see Byron C. Jaeger’s presentation Happier version control with Git and GitHub (and RStudio).\n\n\n\n\nBryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Storage & version control</span>"
    ]
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "6  Documentation",
    "section": "",
    "text": "6.1 Project-level\nProject-level documentation includes all descriptive information about the SOS dataset, as well as planning decisions and process documentation. Documentation includes quality assurance project plans, standard operating procedures, and other high-level documents (e.g., request for proposals, applications, meeting agendas/notes).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "documentation.html#project-level",
    "href": "documentation.html#project-level",
    "title": "6  Documentation",
    "section": "",
    "text": "Quality assurance project plan (QAPP)\nThe QAPP is the highest level of project documentation and covers everything from the project description; personnel roles and responsibilities; project timelines; data and measurement quality objectives; study design; and overviews of field, laboratory, and quality control.\nOurs can be found in Y:/NRAS/soil-health-initiative/state-of-the-soils/qapp.\n\n\nStandard operating procedures (SOP)\nSOPs provide detailed instructions for field, lab, or data processing procedures and decision-making processes.\nOurs can be found in Y:/NRAS/soil-health-initiative/state-of-the-soils/sop.\n\nSOS sampling\nThe purpose of this SOP is to detail the procedures for a typical site visit in which soil samples are collected for physical, chemical, and biological soil health indicator analyses. Procedures include equipment preparation prior to sampling; best practices for filling out field forms; the selection of sampling locations; sampling protocols; sample handling and storage; and submitting samples to the lab. Following this SOP ensures data quality by creating audit trails and reminders to check that data are present, complete, and accurate. Additionally, this SOP will be used to maintain consistent sample collection procedures throughout the state for WSDA employees and partners.\n\n\nQuality control / quality assurance (QA/QC)\nThis SOP outlines the process for screening sample metadata and lab results for completeness, consistency, and quality. Procedures involve subject matter expertise, investigation, communication with sampling teams and labs, algorithmic quality control, and tagging sample results with quality codes (listed in the below table). Data are then integrated into the statewide database.\n\n\n\n\n\n\n\n\n\nCode\nTag\nDescription\nInclusion in analyses\n\n\n\n\n0\nExcellent\nMet lab’s and WSDA’s QC criteria\nYes\n\n\n100\nEstimate\nInterpolated missing value\nYes\n\n\n110\nDerived\nDerived from an estimated value\nYes\n\n\n120\nSuspect\nZ-score is ≥ |3|\nYes\n\n\n130\nCalculated ND\nCalculated value using at least one ND\nYes\n\n\n140\nNon-detect\nBelow the method detection limit\nNo\n\n\n160\nPoor\nDid not meet lab’s QC criteria\nNo\n\n\n180\nOutlier\nOutlier, designated by soil scientist\nNo\n\n\n200\nUnknown\nExternal dataset\nYes\n\n\nND = non-detect",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "documentation.html#dataset-level",
    "href": "documentation.html#dataset-level",
    "title": "6  Documentation",
    "section": "6.2 Dataset-level",
    "text": "6.2 Dataset-level\nDataset-level documentation applies to lab results, sample locations, grower information, and management data. Readmes and changelogs document what each dataset contains, how they are related, potential issues to be aware of, and any alterations made to the data. See below for examples of what to include.\n\nReadme\nreadme files are plain text documents that contain information about the files in a folder, explanation of versioning, and instructions/metadata for data packages. These files are saved as .txt, instead of MS Word documents that take longer to open and can only be opened on computers with Microsoft installed.\n\nDescribe contents of folder\nThe readme.txt in the _complete-dataset folder describes each files’ structure, contents, and other pertinent information, such as data sources.\n\n\nView the example\n\nLast modified: 2024-03-18\n\nINSTRUCTIONS\nPlace the latest, complete SOS data in this folder as RDS and csv files. Update the readme and data dictionary as needed. Document the changes in the changelog.txt.\n\nROOT: complete dataset with documentation.\nsos_lab-results-wide has one sample per row with each measurement as a separate column.\nsos_lab-results-long has one result per row with columns for sample identification, measurement name, result value, quality code, and lab. See `~qc/qc_codes.csv` for quality code descriptions.\nsos_sample-locations has the coordinates, sample ID, crop info, and PRISM mean annual precipitation and mean annual temperature (800 m resolution, 30-year normals, 1991-2020).\nexample-data.csv contains 100 random samples that have been anonymized with fake sampleIDs, county, farm, producer, and field names. WSU SCBG samples are excluded, as are the 0-6in to 6-12in WSDA samples.\ndata-dictionary lists each variable with its short name (variable name without units), description, indicator type (chemical, physical, biological), unit (if applicable), label, and data type. This spreadsheet also contains tabs describing the variables in the sample-locations and qc-codes spreadsheets.\nqc_codes describes the quality codes.\nreadme.txt (this file) describes the contents of the root folder and subfolders.\nchangelog.txt documents changes made to the data and organization.\n\nMANAGEMENT SURVEYS\nSince management surveys aren't yet compiled, they will be placed in this folder.\nJR will work on a crosswalk between old sample IDs and new sample IDs to make joining with lab data easier.\n\nREFERENCE: dictionaries and data used in code.\ncolumn-names is a dictionary to convert the old naming convention (Ca_mg.kg) to the new convention (ca_mg_kg).\ncrop-group-type is a dictionary mapping WSDA crop group to crop type.\nformat-survey-crop-names.csv is a dictionary mapping crop_type (snake_case) to Crop Type (Title Case).\nlab-methods describes the lab, measurement name, method, and mdl.\nsoiltest-mdl contains Soiltest measurement names, methods, descriptions, units, and minimum detection limits.\nstaff-contacts lists year, sampling organization, and sampling contact(s).\nppt and temp are {sf} dataframes containing MAP (mm/yr) and MAT (deg C) downloaded from the {prism} package (800 m resolution, 30-year normals, 1991-2020). Created from download-spatial-data.R.\nwa is a {sf} dataframe containing the WA boundary downloaded from the {rgeoboundaries} package. Created from download-spatial-data.R.\n\n\n\nExplain versions\nThe readme.txt in the 2023_sampling &gt; lab-data &gt; raw folder explains why there are two different versions of the lab results and where to find additional information.\n\n\nView the example\n\n2023-08-21\n2023_wsda-soil-health_v1.xlsx has errors. See email for details.\n'          'v2.xlsx still has some errors, that are cleaned up in the R scripts.\n\n\n\nProvide instructions\nAnother readme.txt instructs how to use the files in the ArcGIS soil sample points box.com folder. When this folder is shared with partners, the readme helps orient them to the contents of the folder and modify the files as needed for their own project.\n\n\nView the example\n\nTemplate for Soil Sample Points ArcGIS\n\n2023-06-01\n\nJadey Ryan | Washington State Department of Agriculture (WSDA)\njryan@agr.wa.gov\n\nPurpose:\nTo provide a template for ESRI ArcGIS data entry and management for soil sampling projects.\n\nFolder contents:\n- readme.txt describes the folder contents and provides general instructions.\n- template-soil-sample-points.aprx is an ArcGIS Project File should allow you to open the project in ArcGIS Pro.\n- template-soil-sample-points.gdb is a file geodatabase, which you can open in ArcGIS Pro or ArcGIS Online.\n- crop-domain.csv provides WSDA approved crop types to use in ArcGIS 'Table to Domain' geoprocessing tool. We highly recommend using attribute domains, which are rules to enforce data integrity by limiting the field type and choices of an attribute field.\n- 923-nras-soil-health-sop-web.pdf is WSDA's Standard Operating Procedure for soil sampling. The appendices contain instructions for the GIS workflow of soil sampling.\n- /screenshots/feature-layer-offline-editing.png shows the checkboxes required for editing.\n- /screenshots/configure-forms.png shows where to click within a Web Map to open the Field Maps form editor.\n- /screenshots/table-to-domain.png is an example of how to use the 'Table to Domain' geoprocessing tool with the crop_domain.csv.\n- /screenshots/field-maps-form_*.png are examples of the Field Maps form structure, which can be created in ArcGIS Online.\n- washi_soil-series-rest-service is an internet shortcut to the URL for the Washington clipped soil series, originating from NRCS gSSURGO. When compositing multiple samples together from one field, we recommend keeping all sample points within one soil series to reduce variability in the composite samples.\n- The other folders (Index, GpMessages, ImportLog, .backups) are part of the ArcGIS Pro project and can be ignored.\n\nInstructions:\n1. Open template-soil-sample-points.aprx.\n2. Update attribute fields and domains to work with your project.\n3. Update symbology, labels, visibility scale, popups, etc. The symbology of the sample points currently defaults to red when the 'Show/Hide Field Form' attribute is still 'Hide' or 'NULL'. When the sampler collects the sample and changes this attribute to 'Show', the point will turn yellow to indicate the sample has been collected.\n4. Share as Feature Layer and create a Web Map to allow others access to this map.\n5. Open the map in ArcGIS Online and click on 'Forms' in the right toolbar to configure your Field Maps form.\n6. Use the /screenshots/field-maps-form_*.pngs as a guide, but adapt the field form structure to suit your project.\n\nOffline Areas:\n- If you anticipate needing to sample without cellular service or wifi access: 1) Open the hosted feature layer, 2) Click 'Settings' 3) Confirm 'Enable Sync' is turned on.\n- If you need the Soil Series layer, you will need to configure two separate Web Maps: one with and one without. This is because the washi_soil-series-rest-service is not allowed in Web Maps with 'Enable Sync' turned on. \n- Alternatively, you can create your own Soil Series layer and host it as a feature layer in your own organization.\n\nResources:\n- WSDA Soil Health YouTube channel: https://www.youtube.com/playlist?list=PL0pB20prk7Ni1daEYiEEXSWy8CfwO34FC\n- WSDA WaSHI_SoilSeries MapServer: https://fortress.wa.gov/agr/gis/wsdagis/rest/services/NRAS/WaSHI_SoilSeries/MapServer\n- Introduction to attribute domains: https://pro.arcgis.com/en/pro-app/latest/help/data/geodatabases/overview/an-overview-of-attribute-domains.htm\n- Introducing smart forms in ArcGIS Field Maps: https://www.esri.com/arcgis-blog/products/field-maps/field-mobility/introducing-arcgis-smart-forms/\n\n\n\n\nChangelog\nChangelogs are also simple and concise plain text documents saved in a folder alongside data files that document changes to the dataset. For more information, see keepachangelog.com/.\nAt the bare minimum, the changelog.txt contains:\n\ndate of modification\ninitials of who made the changes\ndescription of the changes\n\nSee the example changelog.txt in the _complete-dataset folder.\n\n\nView the example\n\nContents of changelog.txt in _complete-dataset folder:\n\n2022-12-15 JR standardized texture classes (title case, no extra white space) and converted texture, county, and crop to factor types.\n2023-01-03 JR corrected error in 2022 WSDA bulk density measurements in 2022-11-01_soiltestData_manualCleanup.xlsx.\n2023-03-02 JR recoded SCBG producer IDs to match WSDA format and cleaned up farm/producer names and IDs. See new scbg_producerId_recode.csv for list.\n2023-03-07 JR corrected Okanogan producer and field names (Devany, Townsend).\n2023-03-21 JR corrected cropType \"Fallow, Idle\" to \"Fallow\" and 2021 SCBG \"Pea\" samples to \"Pea, Dry\", updated Crop Group column. Updated results and sample locations spreadsheets.\n2023-07-13 JR added labID to labResults datasets and updated dataDictionary accordingly. \n2023-08-21 JR added 2023 data and updated dataDictionary sample_locations tab.\n2023-08-27 JR corrected SCBG pulse samples crop from \"Pea\" to \"Pea, Dry\".\n2023-08-30 JR added an anonymized dataset (100 samples from 2022-2023).\n2023-09-05 JR added labID to 2023 results.\n2023-09-06 JR added sampling organization column to make impact tracking easier. Fixed merge issue that was causing the loss of some producer IDs. Add \"County\" to relevant CD names.\n2023-11-14 JR switched from .RData to .RDS file type so users can assign a new name when loading the data into R with `data_wide &lt;- readRDS(\"2020-2023_labResults_wide.RDS\")`.\n2023-11-14 JR added sampling dates and depths to all 877 samples. See addSampleDepthsDates for R script. \n2023-12-08 JR corrected CropGroup from Fallow to Cereal Grain for samples with CropType of Fallow, Wheat in 2020-2023_sampleLocations.csv.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "documentation.html#variable-level",
    "href": "documentation.html#variable-level",
    "title": "6  Documentation",
    "section": "6.3 Variable-level",
    "text": "6.3 Variable-level\nVariable-level documentation includes data dictionaries, which are tabular collections of names, definitions, and attributes about the variables in a dataset. Data dictionaries are ideally created in the planning phase of the project before data are collected.\n\nData dictionary\nEach row is a different variable, and each column is a different attribute of that variable. With a data dictionary, a user should be able to properly interpret each variable in the data.\nOur data-dictionary.xlsx in the _complete-dataset folder contains three tabs (lab-results, sample-locations, and qc-codes) that describe the attributes of each variable.\n\n\nView the sample-locations dictionary\n\n\n\n\n\n\n\n\n\n\nvariable\ndescription\nunit\ndata_type\n\n\n\n\nyear\nYear the sample was collected\n\nNumeric\n\n\ncounty\nCounty of the sampled field\n\nCharacter\n\n\nsample_id\nSample identification code\n\nCharacter\n\n\ncrop_group\nCrop group of the sampled field\n\nCharacter\n\n\ncrop_type\nCrop type of the sampled field\n\nCharacter\n\n\nmap_mm_year\nOSU PRISM mean annual precipitation\n30-year normals (1991-2020) at 800 m resolution\nmm/year\nNumeric\n\n\nmat_c\nOSU PRISM mean annual temperature\n30-year normals (1991-2020) at 800 m resolution\ndegrees C\nNumeric\n\n\nlongitude\nLongitude of sample point, WGS84\ndecimal degrees\nNumeric\n\n\nlatitude\nLatitude of sample point, WGS84\ndecimal degrees\nNumeric",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "documentation.html#sec-external-data",
    "href": "documentation.html#sec-external-data",
    "title": "6  Documentation",
    "section": "6.4 External data",
    "text": "6.4 External data\nExternal data refers to any data not directly collected by WSDA or trained partners (e.g., WSU or conservation districts) that follow our SOPs. These can include other studies pre-dating WaSHI, special soil health surveys, and publicly available datasets.\nOn a case-by-case basis, the Senior Soil Scientist and Data Scientist consider the following questions when deciding whether to integrate an external dataset:\n\nHow does the study design fit into SOS goals?\nWhat field procedures were used and how were they documented?\nWho analyzed the soil samples? With which methods and QA/QC procedures?\nAre the following required metadata and management data available along with the lab results?\n\nFarm, producer, and field info1\nSampling date\nSampling depth\nLatitude and longitude\nProduction system (current crop, crop rotation, etc.)\nInformation concerning tillage, livestock grazing, irrigation, soil fertility and amendments, land use history, and/or conservation practices\n\nIs there a data dictionary or codebook describing the measurements, units, missing values, etc.?\n\nGenerally, external data should 1) be well documented, 2) be collected and analyzed by well-trained scientists and labs; and 3) have adequate accompanying metadata and management data to facilitate interpretation of the results.\nSome publicly available datasets to consider are in Y:/NRAS/soil-health-initiative/state-of-the-soils/external-data.\n\nIntake form\nExternal data may be provided in the External Data Intake spreadsheet, alongside related documents such as SOPs, management surveys, raw data files, etc.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "documentation.html#footnotes",
    "href": "documentation.html#footnotes",
    "title": "6  Documentation",
    "section": "",
    "text": "Enough farm, producer and field info to distinguish unique farmers and fields for assigning unique IDs. They don’t need to include personally identifiable information.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Documentation</span>"
    ]
  },
  {
    "objectID": "flow.html",
    "href": "flow.html",
    "title": "7  Data flow",
    "section": "",
    "text": "7.1 Pre field season",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data flow</span>"
    ]
  },
  {
    "objectID": "flow.html#pre-field-season",
    "href": "flow.html#pre-field-season",
    "title": "7  Data flow",
    "section": "",
    "text": "When preparing sample ID assignments, labels, chain of custodies, and other materials, use an accessible font to reduce transcription errors. Atkinson Hyperlegible has very distinct alphanumeric characters, which improves legibility. Download it from Google Fonts.\n\n\n\n\n\n\n“0O” and “11li” in Atkinson Hyperlegible\n\n\n\n\n\nAssign unique identifiers\nBefore sample IDs can be assigned, collect the following information for each proposed sample:\n\nCounty\nOrganization of sampling team\nFarm name (optional)\nProducer name\nProducer contact information (optional)\nField name\nCrop\nGeneral management practice (i.e., conventional, cover crop, reduced tillage)\n\nView examples of the 2024 Sample Request Form sent to conservation districts and the Berries Sample Request Form used for a WSDA/WSU special project.\nOnce producers and fields have been identified, assign a unique ID for the producer, field, and sample with the following convention:\n\nProducer ID: first three letters of county + three-digit landowner number\n\nWHA001\n\nField ID: two-digit field number\n\n01 and 02\n\nPair ID (optional): letter extension added to paired fields\n\nA\n\nSample ID: last two digits of year + Producer ID + Field ID + Pair ID\n\n24-WHA001-01-A and 24-WHA001-02-A\n\n\nThe following counties have different abbreviations than their first three letters:\n\nClallam → CLL\nGrays Harbor → GRY\nKitsap → KIS\nSkamania → SKM\n\nMatch producer and field IDs to previous participants. Continue the sequence for new producers and fields. Producer IDs and sample IDs must not be duplicated.\nFor an example R script to automate this process, see assign-sample-ids.R.\n\n\nCreate sample labels\nSample label creation is automated using R and Microsoft Word’s mail merge tool. labels.R generates a spreadsheet with the information to be printed on the labels. Then open labels-template-mail-merge.docx, select the spreadsheet as the recipient list, and run the mail merge to generate a word document with all labels to be printed (as shown in the completed-labels folder).\n\n\nCreate a data tracking sheet\nCreate a spreadsheet to track which data have been submitted for each sample, including:\n\nGPS points through the ArcGIS Field Maps field form\nScanned paper field forms (for those without ArcGIS Field Maps)\nManagement surveys through ArcGIS Survey123\nScanned chain of custodies with shipping tracking numbers\nLocation of archival falcon tubes (once retrieved by WSDA staff)\nNotes for if a sample will no longer be sampled, a sample ID was changed, etc.\n\nSee the 2023 spreadsheet for an example.\n\n\nDevelop ArcGIS web tools\nUse ArcGIS to build tools for managing spatial data and collecting management survey data. In ArcGIS Pro, create a sample selection feature layer with domains for point numbers, bulk density, and crop types. Publish this feature layer to ArcGIS Online as a web map with a soil series layer. Then publish a second copy without the soil series layer and enable offline use. On ArcGIS Online, use Field Maps to configure the field form for the feature layer. Management surveys are created and hosted with Survey123 and Experience Builder. Schedule the ArcGIS Notebook with Python that backs up all data to run as a task every Monday, Wednesday, and Friday during the field season.\nThis template ArcGIS Pro project includes a readme.txt that describes this process.\n\n\nView code from the ArcGIS Notebook\n\nimport arcgis\nfrom arcgis.gis import GIS\nimport datetime as dt\nfrom datetime import timezone, timedelta\ngis = GIS(\"home\")\n\nfolder_path = '/arcgis/home/backups/2023/points'\ntitle = \"2023*\"\nowner = \"jryan_NRAS\"\nitems = gis.content.search(query = \"title:\" + title + \" AND owner:\" + owner,\n                          item_type='Feature Layer')\nprint(str(len(items)) + \" items will be backed up to \" + folder_path +\". See the list below:\")\nitems\n\ndef download_as_fgdb(item_list, backup_location):\n    for item in item_list:\n        try:\n            if 'View Service' in item.typeKeywords:\n                print(item.title + \" is view, not downloading\")\n            else: \n                print(\"Downloading \" + item.title)\n                version = dt.datetime.now(timezone(timedelta(hours=-8))).strftime(\"%Y-%m-%d\")\n                result = item.export(item.title + \"_\" + version, \"File Geodatabase\")\n                result.download(backup_location)\n                result.delete()\n                print(\"Successfully downloaded \" + item.title)\n        except:\n            print(\"An error occurred downloading \" + item.title)\n    print(\"The function has completed\")\n\ndownload_as_fgdb(items, folder_path)\n\nfolder_path = '/arcgis/home/backups/2023/surveys'\ntitle = \"2023 * Survey* Production\"\nowner = \"dgelardi_NRAS\"\nitems = gis.content.search(query = \"title:\" + title + \" AND owner:\" + owner,\n                          item_type='Feature Layer')\nprint(str(len(items)) + \" items will be backed up to \" + folder_path +\". See the list below:\")\nitems\n\ndef download_as_fgdb(item_list, backup_location):\n    for item in item_list:\n        try:\n            if 'View Service' in item.typeKeywords:\n                print(item.title + \" is view, not downloading\")\n            else: \n                print(\"Downloading \" + item.title)\n                version = dt.datetime.now(timezone(timedelta(hours=-8))).strftime(\"%Y-%m-%d\")\n                result = item.export(item.title + \"_\" + version, \"CSV\")\n                result.download(backup_location)\n                result.delete()\n                print(\"Successfully downloaded \" + item.title)\n        except:\n            print(\"An error occurred downloading \" + item.title)\n    print(\"The function has completed\")\n\ndownload_as_fgdb(items, folder_path)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data flow</span>"
    ]
  },
  {
    "objectID": "flow.html#during-field-season",
    "href": "flow.html#during-field-season",
    "title": "7  Data flow",
    "section": "7.2 During field season",
    "text": "7.2 During field season\nData collection in the field is detailed in the sampling SOP. Here, we focus on the behind-the scenes tasks for managing data.\n\nUpdate data tracking spreadsheet\nThroughout the season, update the data tracking spreadsheet as various forms, surveys, and correspondence are received, as described in Create a data tracking sheet.\n\n\nModify IDs when samples change\nSometimes a producer can no longer participate, or they need to change which field is sampled. Update, version, and archive the sample request form (sample-request-form-ferry.xlsx → sample-request-form-ferry_v2.xlsx). Run the assign-sample-ids.R script again to update the sample IDs. Lines 362 - 386 should be commented out as shown in the highlighted lines of the script on GitHub.\nSee 01_returned-sample-requests and 02_completed-sample-ids for an example of this flow.\nAdd a concise, explanatory note to the data tracking spreadsheet.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data flow</span>"
    ]
  },
  {
    "objectID": "flow.html#post-field-season",
    "href": "flow.html#post-field-season",
    "title": "7  Data flow",
    "section": "7.3 Post field season",
    "text": "7.3 Post field season\n\nOrganize multiple sources of data\nTo unify the information from multiple data sources (e.g., sample request forms, ArcGIS Field Maps forms, and management surveys), cross-reference each source and reach out to the sampling teams to resolve conflicting information as needed. This is especially important for verifying the crop planted at the time of sampling.\nSee how to mostly automate this in: 01_load-metadata.R and 02_check-crops.R.\n\n\nProcess lab data\nFollow the QA/QC SOP for processing lab data.\nSee the 2023 processing scripts and QA/QC report on GitHub:\n\n03_process-spatial-data.R\n04_load-lab-data.R\n05_calculate-z-scores.R\n2020-2023_qc-results-summary.qmd\n\n\n\nGenerate reports\nUse the {soils} package to create a new project for each year. To avoid email attachment size limitations, save reports to Box.com for distribution to the sampling partners who send the reports to the participants. Access to this folder requires a share link provided by WSDA staff.\n\n\nSave data to shared drive and WSU Teams channel\nCopy the output data files and reports from Process lab data and Generate reports to the state-of-the-soils folder in its respective year_sampling folder. See Chapter 4 to review folder structure and organization.\nSave the final datasets (in wide and long formats) and documentation (data dictionary, changelog, readme) to the WSU SCBG Soil Health Assessment Teams channel.\n\n\nArchive jars and falcon tubes\nStore the archival subsamples in glass jars in the Yakima WSDA storage room and the cryogenic archive subsamples in falcon tubes in the -80 °C freezer at the WSU Mount Vernon Northwestern Washington Research & Extension Center.\nTape the labels on the falcon tubes with a generous amount of packing tape to avoid falling off when they freeze.\nUpdate the archive spreadsheet with the additional sample IDs, number of falcon tubes, and box number of the glass jar.\n\n\n\n“0O” and “11li” in Atkinson Hyperlegible",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data flow</span>"
    ]
  },
  {
    "objectID": "share.html",
    "href": "share.html",
    "title": "8  Data sharing",
    "section": "",
    "text": "Data privacy statement\nProcedures for anonymizing data are detailed in Section 8.2.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data sharing</span>"
    ]
  },
  {
    "objectID": "share.html#watech-data-categorization",
    "href": "share.html#watech-data-categorization",
    "title": "8  Data sharing",
    "section": "8.1 WaTech data categorization",
    "text": "8.1 WaTech data categorization\nUnder Washington State Policy 141.10 (Securing Information Technology Assets), state agencies must classify data into categories based on the sensitivity of the data. WaTech provides guidance on the four categories of data.\n\nCategory 4: “Confidential information requiring special handling”\nWaTech: Data requires strict handling requirements applied by statues or regulations.\nSOS: Not applicable.\n\n\nCategory 3: “Confidential information”\nWaTech: Data includes “personal information” as defined in RCW 42.56.590 (Security Breaches) and RCW 19.255.010 (Personal Information Disclosure). An individual’s first name or first initial and last name in combination with at least one of the following elements: social security number, driver’s license or Washington identification card number, or any account numbers that permit access to their financial account.\nSOS: We do not collect the above elements in combination with grower names. However, we treat individual names, farm names, and latitude and longitude coordinates as confidential information.\n\n\nCategory 2: “Sensitive information”\nWaTech: Data are intended for official use only and withheld unless specifically requested.\nSOS: This category includes lab results and management surveys. Access to this data requires a data share agreement.\n\n\nCategory 1: “Public information”\nWaTech: Data is not covered in any of the above categories or is already released to the public.\nSOS: De-identified and aggregated data, such as the number of soil samples and from which counties and crops they were collected, fall under this category. For example, the SOS dashboard is publicly available and the map zoom is disabled at the 1:1,600,000 scale (counties level).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data sharing</span>"
    ]
  },
  {
    "objectID": "share.html#sec-maintain-confidentiality",
    "href": "share.html#sec-maintain-confidentiality",
    "title": "8  Data sharing",
    "section": "8.2 Maintain confidentiality",
    "text": "8.2 Maintain confidentiality\nOnly under special circumstances and with proper justification in the data share agreement will the following Category 3 data be released to external collaborators. Under no circumstances will these data be made publicly available.\n\nFarm name\nGrower first and last name\nField names that contain street names or other identifying information\nLatitude and longitude coordinates or other geospatial identifiers\nAny information identifying the individual farm or grower\n\nAnonymize and aggregate Category 2 data to honor our data privacy statement by either removing or replacing Category 3 confidential information with dummy data. The {randomNames} R package can be used to replace real names with fake names. Round latitude and longitude to a precision that does not identify the farm or fields sampled.\nSee the example R script below, copied from the private washi-sos GitHub repository.\n\n\nR code to anonymize data\n\n# Attach packages ==============================================================\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(randomNames)\n\n# Load data ====================================================================\n\nfarm &lt;- readRDS(\n  \"data/sos-sample-metadata.RDS\"\n) |&gt;\n  select(\n    c(sample_id, farm_name, producer_name, producer_id, field_name, field_id)\n  )\n\nresults_wide &lt;- readRDS(\n  \"data/sos-lab-results-wide.RDS\"\n) |&gt;\n  select(-c(\n    project_id,\n    lab_id:lab_analyzed_date,\n    crop_type,\n    crop_variety,\n    texture_class\n  ))\n\npoints &lt;- readRDS(\n  \"data/sos-sample-locations.RDS\"\n) |&gt;\n  select(sample_id, longitude, latitude) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 0)))\n\n# Join farm info with lab data\nresults_wide &lt;- left_join(farm, results_wide) |&gt;\n  relocate(year, .before = sample_id) |&gt;\n  rename(crop = crop_group)\n\n# Join lab data with gis data\ndata &lt;- left_join(results_wide, points) |&gt;\n  remove_empty(\"cols\") |&gt;\n  relocate(latitude, longitude, .after = county)\n\n# Remove WSU SCBG samples and 0-6/6-12 in WSDA samples\ndata &lt;- data |&gt;\n  subset(!year %in% c(2020, 2021) &\n    !grepl(\"[A-Z]\", field_id)) |&gt;\n  remove_empty(\"cols\") |&gt;\n  select(-c(pmn_nitrate_n_mg_kg:min_c_24hr_mg_c_kg_day))\n\n# Anonymize\nset.seed(123)\nanon_data &lt;- data |&gt;\n  # Farm name, producer name, sample ID\n  mutate(\n    farm_name = paste0(\"Farm \", sprintf(\"%03d\", cur_group_id())),\n    producer_name = randomNames(\n      which.names = \"first\",\n      sample.with.replacement = FALSE\n    ),\n    producer_id = paste0(\n      paste0(sample(LETTERS, 3, replace = TRUE), collapse = \"\"),\n      \"0\",\n      paste0(sample(1:9, 1, replace = TRUE), collapse = \"\")\n    ),\n    field_name = paste0(\"Field \", field_id),\n    sample_id = paste0(\n      substr(year, 3, 4),\n      \"-\",\n      producer_id,\n      \"-\",\n      field_id\n    ),\n    .by = c(county, producer_name, producer_id)\n  ) |&gt;\n  # county\n  mutate(\n    county = paste0(\"County \", cur_group_id()),\n    .by = c(county)\n  )\n\n# Grab 100 random samples\nanon_subset &lt;- anon_data |&gt;\n  slice_sample(n = 100) |&gt;\n  arrange(sample_id)\n\nwrite.csv(\n  anon_subset,\n  file = \"data/example-data.csv\",\n  na = \"\",\n  row.names = FALSE\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data sharing</span>"
    ]
  },
  {
    "objectID": "share.html#data-share-agreement",
    "href": "share.html#data-share-agreement",
    "title": "8  Data sharing",
    "section": "8.3 Data share agreement",
    "text": "8.3 Data share agreement\nSOS CoPIs created a Data Sharing and Scope of Work Agreement detailing the type of data to be shared, the scope of work in which the data may be used, and terms for using SOS data.\nOnce both CoPIs and the “Partnering Scientists” sign the agreement, save it in its own subfolder within Y:/NRAS/soil-health-initiative/state-of-the-soils/data-sharing. If this agreement is part of a grant, place a copy in its corresponding grant subfolder within Y:/NRAS/soil-health-initiative/contracts-grants/grants. Include relevant correspondence, code to subset the requested data, and final dataset in the agreement’s subfolder in the data-sharing folder. This documentation allows us to track publications, attributions, and the broader impact of the SOS dataset.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data sharing</span>"
    ]
  },
  {
    "objectID": "share.html#public-access",
    "href": "share.html#public-access",
    "title": "8  Data sharing",
    "section": "8.4 Public access",
    "text": "8.4 Public access\nCurrently, the only publicly available SOS data are the counts of samples across the project, counties, and crop types displayed in the ArcGIS Online Dashboard.\nA small, anonymized subset is included as example data in the {washi} and {soils} R packages for demonstration purposes.\nWhen the data are more mature and hosted in a proper database, we may publish an anonymized subset in a public repository such as:\n\nGitHub via an R package or Shiny app\nZenodo: integrates with GitHub and is citable with a DOI\nData.WA.gov: open data portal for the State of Washington\n\nMore inspiration for enhancing data discoverability and sharing across the agricultural and soil health communities include:\n\nUSDA LTAR data dashboards\nCAF LTAR metadata tool\n\nChapter 16 of Data Management in Large-Scale Education Research discusses more considerations for data sharing and choosing public repositories (Lewis 2023).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data sharing</span>"
    ]
  },
  {
    "objectID": "share.html#acknowledgments",
    "href": "share.html#acknowledgments",
    "title": "8  Data sharing",
    "section": "8.5 Acknowledgments",
    "text": "8.5 Acknowledgments\nAll research and data partially or completely funded by WaSHI must include acknowledgements to the State of Washington. The following text should be included in all publications resulting from this funding:\n\nData were in part provided by the Washington Soil Initiative, which is supported by the State of Washington and administered by the Washington State Department of Agriculture, Washington State Conservation Commission, and Washington State University.\n\nIf WaSHI staff make substantial scientific contributions to the manuscript, discuss the possibility of co-authorship credit.\n\n\n\n\nCzarnecki, Joby M. Prince, and Mary Ann Jones. 2022. “The Problem with Open Geospatial Data for on-Farm Research.” Agricultural & Environmental Letters 7 (1): e20062. https://doi.org/10.1002/ael2.20062.\n\n\nEuropean Commission. 2016. “H2020 Programme Guidelines on FAIR Data Management in Horizon 2020.” https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf.\n\n\nKorzekwa, Kaine. 2023. “Protecting Privacy While Making Data Open in Agricultural Research.” CSA News 68 (3): 6–10. https://doi.org/10.1002/csan.20979.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nWhyte, Angus, and Graham Pryor. 2011. “Open Science in Practice: Researcher Perspectives and Participation.” International Journal of Digital Curation 6 (March): 199–213. https://doi.org/10.2218/ijdc.v6i1.182.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data sharing</span>"
    ]
  },
  {
    "objectID": "code-guide.html",
    "href": "code-guide.html",
    "title": "9  Code style guide",
    "section": "",
    "text": "9.1 Projects\nKeep all files associated with a given project (input data, R scripts, analytical results, figures, reports) together in one directory. RStudio has built-in support for this through projects, which bundle all files in a portable, self-contained folder that can be moved around on your computer or on to other collaborators’ computers without breaking file paths.\nCreate a GitHub repository and commit the project folder for version control as discussed in Section 5.3. If not in a GitHub repository, the folder must be copied onto the shared drive.\nLearn more about projects in the Workflow: scripts and projects chapter of R4DS, in Jenny Bryan’s article Project-oriented workflow, and Shannon Pileggi’s workshop slides.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code style guide</span>"
    ]
  },
  {
    "objectID": "code-guide.html#projects",
    "href": "code-guide.html#projects",
    "title": "9  Code style guide",
    "section": "",
    "text": "Project folder structure\nA consistent and logical folder structure makes it easier for you (especially future you) and collaborators to make sense of the files and work you’ve done. Well documented projects also make it easier to resume a project after time away.\nThe below structure works most of the time and should be used as a starting point. However, different projects have different needs, so add and remove subfolders as needed.\n\nroot: top-level project folder containing the .Rproj file\ndata: contains raw and processed data files in subfolders. Raw data should be made read-only and not changed in any way. Review Section 5.2 for how to make a file read-only\noutput: outputs from R scripts such as figures or tables\nR: R scripts containing data processing or function definitions\nreports: Quarto or RMarkdown files with the resulting reports\nREADME: markdown file (can be generated from Quarto or RMarkdown) explaining the project\n\n\n\nSee an example project folder structure\n\n├── project-demo.Rproj\n├── data\n│   ├── processed\n│   │   └── data-clean.csv\n│   └── raw\n│       └── data-raw.xlsx\n├── output\n│   ├── fig-01.png\n│   ├── fig-02.png\n│   ├── tbl-01.png\n│   └── tbl-02.png\n├── R\n│   ├── 01_import.R\n│   ├── 02_tidy.R\n│   ├── 03_transform.R\n│   ├── 04_visualize.R\n│   └── custom-functions.R\n└── reports\n│   ├── soil-health-report.pdf\n│   └── soil-health-report.qmd\n│   └── images\n│       └── logo.png\n├── README.md\n└── README.qmd\n\nR packages, such as {washi} and {soils}, contain additional subfolders and files:\n\ninst: additional files to be included with package installation such as CITATION, fonts, and Quarto templates.\nman: .Rd (“R documentation”) files for each function generated from {roxygen2}.\nvignettes: long-form guides that go beyond function documentation and demonstrate a workflow to solve a particular problem\ntests: test files, usually using {testthat}\npkgdown and docs: files and output if using {pkgdown} to build a website for the package\nDESCRIPTION: file package metadata (authors, current version, dependencies)\nLICENSE: file describing the package usage agreement\nNAMESPACE: file generated by {roxygen2} listing functions imported from other packages and functions exported from your package\nNEWS.md: file documenting user-facing changes\n\nLearn more about other R package components in R Packages (2e).\n\n\nAbsolute vs relative paths\n\n\n\n\n\n\nNote\n\n\n\nDirectories and folders are used interchangeably here. If you’re interested in the technical differences, directories contain folders and files to organize data at different levels while folders hold subfolders and files in a single level.\n\n\n❌ Absolute paths start with the root directory and provide the full path to a specific file or folder like C:\\\\Users\\\\jryan\\\\Documents\\\\R\\\\projects\\\\project-demo\\\\data\\\\processed.1 Run getwd() to see where the current working directory is and setwd() to set it a specific folder. However, a working directory set to an absolute folder path will break the code if the folder is moved or renamed\n✅ Instead, always use relative paths, which are relative to the working directory (i.e. the project’s home) like data/processed/data-clean.csv. When working in an RStudio project, the default working directory is the root project directory (i.e., where the .Rproj file is).\n\n\n\nArtwork by Allison Horst\n\n\n\n\n{here} package\nIn combination with R projects, the {here} package builds relative file paths. This is especially important when rendering Quarto files because the default working directory is where the .qmd file lives. Using the above example project structure, running read.csv(\"data/processed/data-clean.csv\") in soil-health-report.qmd errors because it looks for a data subfolder in the reports folder. Instead, use here to build a relative path from the project root with read.csv(here::here(\"data\", \"processed\", \"data-clean.csv\")). This takes care of the backslashes or forward slashes so the relative path works with any operating system.\n\n\n\nArtwork by Allison Horst",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code style guide</span>"
    ]
  },
  {
    "objectID": "code-guide.html#naming-conventions",
    "href": "code-guide.html#naming-conventions",
    "title": "9  Code style guide",
    "section": "9.2 Naming conventions",
    "text": "9.2 Naming conventions\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\n— Phil Karlton\n\nBased on this quote, Indrajeet Patil developed a slide deck with detailed language-agnostic advice on naming things in computer science.\nR code specific naming conventions are listed below. Python and other programming languages have different conventions.\n\nProject folder, .RProj and GitHub repository\nName the project folder, .RProj file, and GitHub repository name the same. Be concise and descriptive. Use kebab-case.\nExample: washi-dmp and washi-dmp.RProj.\n\n\nFiles\nBe concise and descriptive. Avoid using special characters. Use kebab-case with underscores to separate different metadata groups (e.g., date_good-name).\nExamples: 2024_producer-report.qmd, tables.R, create-soils.R.\nIf files should be run in a particular order, prefix them with numbers. Left pad with zeros if there may be more than 10 files.\nExample:\n01_import.R\n02_tidy.R\n03_transform.R\n04_visualize.R\n\n\nVariables, objects, and functions\nVariables are column headers in spreadsheets (that become column names in R dataframes), objects are data structures in R and ArcGIS (vectors, lists, dataframes, fields, tables), and functions are self-contained modules of code that accomplish a specific task.\nVariable examples:\n# Good\nclay_percent\nmin_c_96hr_mg_c_kg_day\npmn_mg_kg\n\n# Bad\n\n# Uses special character\nclay_%\n\n# Less human readable, inconsistent with style guide, \n# starts with number and will error in R\n96hrminc_mgckgday\n\n# Hyphen will need to be escaped in R code to avoid error\npmn-mgkg\n\nObjects and functions\nObjects names should be nouns, while function names should be verbs (Wickham 2022). Use lowercase letters, numbers, and underscores. Do not put a number as the first character of the name. Do not use hyphens. Do not use names of common functions or variables.\nObject examples:\n# Good\nprimary_color\ndata_2023\n\n# Bad\n\n# Less human readable, inconsistent with style guide\nprimarycolor\n\n# Using a hyphen in an object name causes error\ndata-2023 &lt;- read.csv(\"2023_data-clean.csv\")\nError in data - 2023 &lt;- read.csv(\"2023_data-clean.csv\") : \ncould not find function \"-&lt;-\"\n  \n# Starting an object name with a number also causes error\n2023_data &lt;- read.csv(\"2023_data-clean.csv\")\nError: unexpected input in \"2023_\"\n\n# Overwrites R shortcut for TRUE\nT &lt;- FALSE\n\n# Overwrites c() R function\nc &lt;- 10\nFunction examples:\n# Good\nadd_row()\nassign_quality_codes()\n\n# Bad\n\n# Uses noun instead of verb\nrow_adder() \n\n# Inconsistent with style guide\nassignQualityCodes()\n\n# Overwrites common base R function\nmean()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code style guide</span>"
    ]
  },
  {
    "objectID": "code-guide.html#r-scripts",
    "href": "code-guide.html#r-scripts",
    "title": "9  Code style guide",
    "section": "9.3 R scripts",
    "text": "9.3 R scripts\n\nHeader template\nHeaders in R scripts standardize the metadata elements at the beginning of your code and document its purpose. The following template and instructions are adapted from Dr. Timothy S Farewell’s blog post (Farewell 2018).\n\nScript name: meaningful and concise\nPurpose: brief description of what the script aims to accomplish\nAuthor(s) and email: name and contact if there are any questions\nDate created: automatically filled in from the template\nNotes: space for thoughts or to-do’s\n\n## Header ======================================================================\n## Script name: check-crops.R\n##\n## Purpose: Cross reference sample requests, Field Maps forms, and management \n## surveys to get the correct crop planted at the time of sampling.\n##\n## Author: Jadey Ryan \n##\n## Email: jryan@agr.wa.gov\n##\n## Date created: 2024-01-02\n##\n## Notes:\n##   \n    \n# Attach packages ==============================================================\n\nlibrary(readxl)\nlibrary(writexl)\nlibrary(janitor)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load data ====================================================================\nAdd this template to RStudio using snippets:\n\nModify the below code with your name and preferred packages.\nIn RStudio, go to Tools &gt; Edit Code Snippets.\nScroll to the bottom of the R code snippets and paste your modified code (the indent and tabs are important!).\nClick Save and close the window.\nTry opening a new blank .R script, typing “header”, and then pressing Shift + Tab.\n\nsnippet header\n    ## Header ======================================================================\n    ##\n    ## Script name: \n    ##\n    ## Purpose: \n    ##\n    ## Author: Jadey Ryan \n    ##\n    ## Email: jryan@agr.wa.gov\n    ##\n    ## Date created: `r paste(Sys.Date())`\n    ##\n    ## Notes:\n    ##   \n    \n    # Attach packages ==============================================================\n\n    library(readxl)\n    library(writexl)\n    library(janitor)\n    library(dplyr)\n    library(tidyr)\n    \n    # Load data ====================================================================\n\n\nSection template\nThe above header template also uses section breaks (e.g., commented lines with = that break up the script into easily readable chunks). Section breaks are a fantastic tool in RStudio because they allow you to easily show or hide blocks of code, see an outline of your script, and navigate through the source file. Read more about code folding and sections in this Posit article.\nThe snippet to create this section template that fills in the rest of the line with = was adapted from this stack overflow answer.\nsnippet end\n    `r strrep(\"=\", 84 - rstudioapi::primary_selection(rstudioapi::getActiveDocumentContext())$range$start[2])`\nAfter adding the above code to your snippets, try creating a new section by typing “# Tidy data end” then pressing Shift + Tab.\n# Tidy data end&lt;Shift+Tab&gt; results in:\n# Tidy data ====================================================================",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code style guide</span>"
    ]
  },
  {
    "objectID": "code-guide.html#code-styling",
    "href": "code-guide.html#code-styling",
    "title": "9  Code style guide",
    "section": "9.4 Code styling",
    "text": "9.4 Code styling\nReview the Syntax chapter of the Tidyverse Style Guide for details on spacing, function calls, long lines, semicolons, assignments, comments, and more. For the opinionated “most important parts of the Tidyverse Style Guide,” skim through Chapter 4 Workflow: code style in R4DS. Instead of including each detail in this style guide and memorizing the content, use the {styler} package (as advised in R4DS Chapter 4).\n{styler} includes an RStudio addin that automatically formats code, making the style consistent across projects. We deviate slightly from the Tidyverse Style Guide and instead use {grkstyle}, an extension package developed by Garrick Aden-Buie, that handles line breaks inside function calls and indentation of function arguments differently. See the readme for examples.\n\nSet up {styler} and {grkstyle}\nInstall {styler} and {grkstyle} with:\ninstall.packages(\"styler\")\n\noptions(repos = c(\n    gadenbuie = \"https://gadenbuie.r-universe.dev\",\n    getOption(\"repos\")\n))\n\n# Download and install grkstyle in R\ninstall.packages(\"grkstyle\")\nSet grkstyle as the default in {styler} functions and addins with:\n# Set default code style for {styler} functions\ngrkstyle::use_grk_style()\nor add the following to your ~/.Rprofile:\noptions(styler.addins_style_transformer = \"grkstyle::grk_style_transformer()\")\nAccess your .Rprofile with usethis::edit_r_profile() to open the file in RStudio for editing. You may need to install the {usethis} package.\n\n\nUse {styler} and {grkstyle}\nOnce installed, apply the style to .R, .qmd, and .Rmd files using the command palette, keyboard shortcut, or addins menu.\n\nCommand palette\nUse RStudio’s command palette to quickly and easily access any RStudio command and keyboard shortcuts. Open the command palette with Cmd/Ctrl + Shift + P then type “styler” to see its available commands and shortcuts.\n\n\nKeyboard shortcuts\nUse Cmd/Ctrl + Shift + A to style the entire active file. We recommend styling the entire active file after finishing each code block or section. To style just a selection, use Cmd/Ctrl + Alt + Shift + A.\n\n\nAddins menu\nUse the addins menu in RStudio to style code by clicking a button to run the command.\n\n\n\n\nArtwork by Allison Horst\nArtwork by Allison Horst\n\n\n\nFarewell, Dr Timothy S. 2018. “My Easy r Script Header Template  Tim Farewell.” https://timfarewell.co.uk/my-r-script-header-template/.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code style guide</span>"
    ]
  },
  {
    "objectID": "code-guide.html#footnotes",
    "href": "code-guide.html#footnotes",
    "title": "9  Code style guide",
    "section": "",
    "text": "Note the two backslashes. Windows paths use backslashes, which mean something specific in R. To make Windows paths with backsplashes work, replace them with two backslashes or one forward slash.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code style guide</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk\nabout Version Control?” The American Statistician 72\n(1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\nCarlson, Bryan. 2021. “Data Management Plan for the\nR.J. Cook Agronomy Farm Long-Term Agroecological Research\nSite.”\n\n\nCzarnecki, Joby M. Prince, and Mary Ann Jones. 2022. “The Problem\nwith Open Geospatial Data for on-Farm Research.” Agricultural\n& Environmental Letters 7 (1): e20062. https://doi.org/10.1002/ael2.20062.\n\n\nEuropean Commission. 2016. “H2020 Programme Guidelines on FAIR\nData Management in Horizon 2020.” https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf.\n\n\nFarewell, Dr Timothy S. 2018. “My Easy r Script Header Template\n Tim Farewell.” https://timfarewell.co.uk/my-r-script-header-template/.\n\n\nHarvard Medical School. 2023. “Data Management Plans.” https://datamanagement.hms.harvard.edu/plan-design/data-management-plans.\n\n\nKorzekwa, Kaine. 2023. “Protecting Privacy While Making Data Open\nin Agricultural Research.” CSA News 68 (3): 6–10. https://doi.org/10.1002/csan.20979.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education\nResearch [in Preparation]. https://datamgmtinedresearch.com/.\n\n\nU.S. Fish & Wildlife Service. 2023. “Data Management Life\nCycle.” https://www.fws.gov/data/life-cycle.\n\n\nWhyte, Angus, and Graham Pryor. 2011. “Open Science in Practice:\nResearcher Perspectives and Participation.” International\nJournal of Digital Curation 6 (March): 199–213. https://doi.org/10.2218/ijdc.v6i1.182.\n\n\nWickham, Hadley. 2022. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific Data Management\nand Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "References"
    ]
  }
]